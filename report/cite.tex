\section{Tài liệu tham khảo}

% \begin{itemize}
%     \item {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding} \cite{devlin2018bert}
%     \item {Attention is All You Need} \cite{vaswani2017attention}
%     \item {RoBERTa: A Robustly Optimized BERT Pretraining Approach} \cite{liu2019roberta}
%     \item {Transformers: State-of-the-Art NLP} \cite{wolf2020transformers}
%     \item {Hugging Face Transformers} \cite{huggingface}
%     \item {Computer Organization and Design RISC-V Edition} \cite{patterson2017cod}
%     \item {Computer Organization and Architecture: Designing for Performance} \cite{stallings2016coa}
%     \item {Stanford CS224n: Natural Language Processing with Deep Learning} 
%     \item {mml-book} 
% \end{itemize}