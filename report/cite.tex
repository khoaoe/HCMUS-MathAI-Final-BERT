\section{Tài liệu tham khảo}

\begin{itemize}
    \item {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding} \cite{devlin2018bert}
    \item {Attention is All you Need} \cite{vaswani2017attention}
    \item {Distributed Representations of Words and Phrases and their Compositionality} \cite{mikolov2013distributed}
    \item {GloVe: Global Vectors for Word Representation} \cite{pennington2014glove}
    \item {Deep Contextualized Word Representations} \cite{peters2018deep}
    \item {Improving Language Understanding by Generative Pre-Training} \cite{radford2018improving}
    \item {Layer Normalization} \cite{ba2016layer}
    \item {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation} \cite{wu2016google}
    \item {RoBERTa: A Robustly Optimized BERT Pretraining Approach} \cite{liu2019roberta}
    \item {Learning Word Vectors for Sentiment Analysis} \cite{maas2011learning}
    \item {PhoBERT: Pre-trained Language Models for Vietnamese} \cite{nguyen2020phobert}
    \item {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding} \cite{wang2018glue}
    \item {Longformer: The Long-Document Transformer} \cite{beltagy2020longformer}
    \item {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension} \cite{lewis2019bart}
    \item {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer} \cite{raffel2020exploring}
    \item {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations} \cite{lan2019albert}
    \item {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators} \cite{clark2020electra}
    \item {SpanBERT: Improving Pre-training by Representing and Predicting Spans} \cite{joshi2020spanbert}
    \item {Unsupervised Cross-lingual Representation Learning at Scale} \cite{conneau2019unsupervised}
    \item {CamemBERT: a Tasty French Language Model} \cite{martin2019camembert}
    \item {ERNIE: Enhanced Representation through Knowledge Integration} \cite{sun2019ernie}
    \item {Knowledge Enhanced Contextual Word Representations} \cite{peters2019knowledge}
    \item {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter} \cite{sanh2019distilbert}
    \item {TinyBERT: Distilling BERT for Natural Language Understanding} \cite{jiao2019tinybert}
    \item {BioBERT: a Pre-trained Biomedical Language Representation Model for Biomedical Text Mining} \cite{lee2020biobert}
    \item {SciBERT: A Pretrained Language Model for Scientific Text} \cite{beltagy2019scibert}
\end{itemize}