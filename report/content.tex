% =========================================================================
%% CHƯƠNG 1: GIỚI THIỆU CHUNG VỀ BERT
%% =========================================================================
\section{Giới Thiệu Chung về BERT}
\label{sec:gioi_thieu_chung_bert}

\subsection{Bối cảnh và Tầm quan trọng của Mô hình Biểu diễn Ngôn ngữ}
\label{ssec:boi_canh_bieu_dien_ngon_ngu}
Trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP), việc xây dựng các biểu diễn ngôn ngữ (language representations) hiệu quả luôn là một mục tiêu trung tâm. Các biểu diễn này thường ở dạng vector, cố gắng nắm bắt các đặc tính ngữ nghĩa và cú pháp của từ, cụm từ hoặc câu, từ đó làm nền tảng cho nhiều tác vụ NLP phức tạp hơn. 

Trước khi BERT ra đời, các phương pháp biểu diễn ngôn ngữ phổ biến bao gồm các word embeddings tĩnh như Word2Vec \cite{mikolov2013distributed} và GloVe \cite{pennington2014glove}. Để hiểu rõ hạn chế của các phương pháp này, hãy xét một ví dụ cụ thể: từ "bank" trong câu "I went to the bank to deposit money" (ngân hàng) và "The river bank was flooded" (bờ sông) sẽ có cùng một vector biểu diễn, mặc dù ý nghĩa hoàn toàn khác nhau. Đây chính là hạn chế cố hữu của biểu diễn tĩnh (static) đó là không thể nắm bắt được ngữ cảnh.

Để khắc phục nhược điểm này, các mô hình biểu diễn ngôn ngữ theo ngữ cảnh (contextual language representation models) đã được phát triển. Các mô hình dựa trên kiến trúc tuần tự như Mạng Nơ-ron Hồi quy (RNN) và Bộ nhớ Dài-Ngắn Hạn (LSTM) bắt đầu cho thấy khả năng nắm bắt thông tin ngữ cảnh. Tuy nhiên, các mô hình này gặp phải vấn đề "information bottleneck" - thông tin phải được nén qua một vector trạng thái ẩn có kích thước cố định, dẫn đến mất mát thông tin khi xử lý chuỗi dài.

ELMo (Embeddings from Language Models) \cite{peters2018deep} đã cố gắng giải quyết vấn đề này bằng cách kết hợp biểu diễn từ cả hai chiều. Cụ thể, ELMo huấn luyện hai LSTM độc lập: một đọc văn bản từ trái sang phải, một đọc từ phải sang trái, sau đó concatenate các biểu diễn. Tuy nhiên, đây chỉ là kết hợp "nông" (shallow), hai LSTM không thực sự tương tác với nhau trong quá trình học.

Sự ra đời của kiến trúc Transformer \cite{vaswani2017attention} với cơ chế tự chú ý (self-attention) đã mang lại một cuộc cách mạng. Khác với RNN phải xử lý tuần tự từng từ, Transformer có thể xử lý toàn bộ chuỗi song song, cho phép mô hình hóa các phụ thuộc tầm xa một cách trực tiếp. OpenAI GPT \cite{radford2018improving} đã tận dụng Transformer nhưng vẫn giữ hướng huấn luyện từ trái sang phải, giới hạn khả năng hiểu ngữ cảnh toàn diện.

\texttt{BERT (Bidirectional Encoder Representations from Transformers)} \cite{devlin2018bert}, được Google giới thiệu năm 2018, đã giải quyết triệt để vấn đề này. BERT sử dụng một chiến lược đột phá: che ngẫu nhiên một số từ trong câu và yêu cầu mô hình dự đoán chúng dựa trên ngữ cảnh hai chiều. Điều này cho phép BERT học được biểu diễn sâu sắc hai chiều (deeply bidirectional), mỗi từ được hiểu dựa trên toàn bộ ngữ cảnh xung quanh, không chỉ một phía.

\subsection{Mục tiêu và Đóng góp chính của BERT}
\label{ssec:muc_tieu_dong_gop_bert}
Mục tiêu chính của bài báo BERT \cite{devlin2018bert} là giới thiệu một phương pháp tiền huấn luyện (pre-training) các biểu diễn ngôn ngữ hai chiều, có khả năng nắm bắt thông tin ngữ nghĩa và cú pháp phong phú từ dữ liệu văn bản không gán nhãn. Đây là một bước đột phá quan trọng vì dữ liệu không gán nhãn rất dồi dào và dễ thu thập, trong khi dữ liệu có nhãn cho các tác vụ cụ thể thường khan hiếm và tốn kém hơn.

Những đóng góp chính của BERT bao gồm:
\begin{itemize}
    \item \textbf{Kiến trúc hai chiều sâu sắc (Deeply Bidirectional):} Đây là điểm khác biệt quan trọng nhất của BERT. Trong khi các mô hình trước đó như GPT chỉ nhìn ngữ cảnh một chiều, hoặc như ELMo kết hợp hai chiều một cách độc lập, BERT cho phép mỗi lớp của mô hình xem xét ngữ cảnh hai chiều đồng thời. Điều này có nghĩa là khi xử lý từ "bank" trong câu, BERT có thể đồng thời xem xét cả "went to the" (bên trái) và "to deposit money" (bên phải) để hiểu đây là "ngân hàng" chứ không phải "bờ sông".
    
    \item \textbf{Hai tác vụ tiền huấn luyện sáng tạo:}
    \begin{itemize}
        \item \textbf{Masked Language Model (MLM):} BERT che ngẫu nhiên 15\% các từ trong câu và yêu cầu mô hình dự đoán chúng. Ví dụ, từ câu "The cat sat on the mat", BERT có thể che từ "sat" thành "[MASK]" và học cách dự đoán từ đó dựa trên ngữ cảnh. Điều quan trọng là BERT sử dụng chiến lược 80-10-10: 80\% thời gian thay bằng [MASK], 10\% thay bằng từ ngẫu nhiên, 10\% giữ nguyên. Chiến lược này giúp giảm sự khác biệt giữa giai đoạn huấn luyện (có [MASK]) và sử dụng thực tế (không có [MASK]).
        
        \item \textbf{Next Sentence Prediction (NSP):} BERT học dự đoán xem hai câu có liên tiếp trong văn bản gốc hay không. Ví dụ, "The weather is nice today. Let's go for a walk" (IsNext) so với "The weather is nice today. Penguins live in Antarctica" (NotNext). Tác vụ này giúp BERT hiểu mối quan hệ giữa các câu, rất quan trọng cho các ứng dụng như hỏi đáp hay suy luận ngôn ngữ.
    \end{itemize}
    
    \item \textbf{Hiệu suất vượt trội:} BERT đã thiết lập kỷ lục mới trên 11 tác vụ NLP khác nhau. Trên GLUE benchmark, BERT\textsubscript{LARGE} đạt điểm trung bình 80.5, vượt xa mô hình tốt nhất trước đó (OpenAI GPT với 72.8). Trên SQuAD v1.1, BERT thậm chí vượt qua hiệu suất của con người (F1: 93.2 so với 91.2).
    
    \item \textbf{Tính linh hoạt và dễ áp dụng:} BERT được thiết kế với triết lý "pre-train once, fine-tune for everything". Sau khi tiền huấn luyện trên dữ liệu lớn (BookCorpus và Wikipedia), BERT có thể được tinh chỉnh cho bất kỳ tác vụ NLP nào chỉ bằng cách thêm một lớp đầu ra đơn giản. Quá trình fine-tune thường chỉ mất vài giờ trên GPU, so với hàng tuần cho việc huấn luyện từ đầu.
\end{itemize}

Sự thành công của BERT không chỉ nằm ở hiệu suất vượt trội mà còn ở việc nó mở ra một hướng nghiên cứu mới: sử dụng mô hình ngôn ngữ lớn được tiền huấn luyện làm nền tảng cho mọi tác vụ NLP. Điều này đã dẫn đến sự phát triển của cả một thế hệ mô hình mới như RoBERTa, ALBERT, và xa hơn là GPT-3.

%% =========================================================================
%% CHƯƠNG 2: KIẾN TRÚC BERT VÀ CÁC THÀNH PHẦN CỐT LÕI
%% =========================================================================
\section{Kiến trúc BERT và các Thành phần Cốt lõi}
\label{sec:kien_truc_bert}
Kiến trúc của BERT dựa trên kiến trúc Transformer \cite{vaswani2017attention}, cụ thể là phần bộ mã hóa (encoder). Sau đây nhóm sẽ phân tích từng thành phần cốt lõi và cách chúng kết hợp để tạo nên sức mạnh của mô hình.

\subsection{Tổng quan về Kiến trúc Transformer}
\label{ssec:tong_quan_transformer}
Transformer là một kiến trúc mạng nơ-ron được giới thiệu với mục tiêu ban đầu là cải thiện các mô hình dịch. Điểm đột phá của Transformer là loại bỏ hoàn toàn các cấu trúc tuần tự (RNN, LSTM) và thay thế bằng cơ chế attention, cho phép xử lý song song toàn bộ chuỗi.

Kiến trúc Transformer gồm hai phần chính:
\begin{itemize}
    \item \textbf{Encoder (Bộ mã hóa):} Xử lý chuỗi đầu vào và tạo ra biểu diễn ngữ cảnh
    \item \textbf{Decoder (Bộ giải mã):} Sinh ra chuỗi đầu ra dựa trên biểu diễn từ encoder
\end{itemize}

BERT chỉ sử dụng phần Encoder vì mục tiêu là tạo biểu diễn ngôn ngữ, không phải sinh văn bản. Mỗi khối encoder trong BERT bao gồm:
\begin{itemize}
    \item \textbf{Lớp Multi-Head Self-Attention:} Cho phép mô hình "chú ý" đến các vị trí khác nhau trong chuỗi khi xử lý mỗi từ. Ví dụ, khi xử lý từ "it" trong câu "The animal didn't cross the street because it was too tired", lớp attention giúp mô hình liên kết "it" với "animal" chứ không phải "street".
    
    \item \textbf{Lớp Feed-Forward Network:} Xử lý độc lập từng vị trí với cùng một mạng nơ-ron. Điều này giúp biến đổi phi tuyến các biểu diễn sau khi đã tổng hợp thông tin ngữ cảnh từ attention.
\end{itemize}

Mỗi lớp con đều có kết nối dư (residual connection) và chuẩn hóa lớp (layer normalization), giúp huấn luyện ổn định các mạng nơ-ron sâu (deep neural networks).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/transformer_arch.png}
    \caption{Kiến trúc tổng quan của mô hình Transformer. BERT chỉ sử dụng phần Encoder (bên trái). Mỗi khối encoder chứa lớp Multi-Head Attention và Feed-Forward, được kết nối bằng residual connections và layer normalization.}
    \label{fig:transformer_architecture}
\end{figure}

\subsection{Cơ chế Tự chú ý (Self-Attention) - Trái tim của BERT}
\label{ssec:self_attention}
Self-attention là cơ chế cho phép mỗi từ trong chuỗi "nhìn" và tương tác với mọi từ khác để xây dựng biểu diễn của mình. Điều này khác biệt hoàn toàn với RNN chỉ có thể xem thông tin một cách tuần tự.

\subsubsection{Trực quan về Attention}
Hãy tưởng tượng đang đọc câu và cần hiểu từ "bank". Não bộ sẽ tự động "chú ý" đến các từ xung quanh như "river", "money", "deposit" để xác định nghĩa. Self-attention mô phỏng chính quá trình này.

\subsubsection{Công thức toán học}
Với đầu vào là các vector biểu diễn của từ, self-attention (hay cụ thể hơn là Scaled Dot-Product Attention) tính toán ba ma trận:
\begin{itemize}
    \item \textbf{Query (Q):} "Tôi đang tìm kiếm thông tin gì?"
    \item \textbf{Key (K):} "Tôi có thông tin gì để cung cấp?"
    \item \textbf{Value (V):} "Thông tin thực sự tôi sẽ truyền đi là gì?"
\end{itemize}

Công thức Scaled Dot-product Attention:
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

Trong đó:
\begin{itemize}
    \item $QK^T$: Tính điểm tương đồng giữa queries và keys (ma trận attention scores)
    \item $\sqrt{d_k}$: Hệ số chuẩn hóa để tránh gradient vanishing khi $d_k$ lớn
    \item $\text{softmax}$: Chuyển điểm số thành xác suất (tổng = 1)
    \item Nhân với $V$: Tổng hợp thông tin dựa trên trọng số attention
\end{itemize}

\subsubsection{Ví dụ minh họa}
Xét câu: "The cat sat on the mat"
Khi tính attention cho từ "sat":
\begin{enumerate}
    \item Query của "sat" được so sánh với Key của mọi từ
    \item Điểm số cao với "cat" (chủ ngữ) và "mat" (địa điểm)
    \item Sau softmax, ta có trọng số attention (ví dụ: cat=0.3, sat=0.1, on=0.1, the=0.15, mat=0.35)
    \item Biểu diễn mới của "sat" = tổng có trọng số của các Value
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/scaled_dotproduct.png}
    \caption{Cơ chế Scaled Dot-Product Attention. MatMul thực hiện phép nhân ma trận $QK^T$, Scale chia cho $\sqrt{d_k}$, Mask (tùy chọn) che các vị trí không hợp lệ, Softmax chuẩn hóa thành xác suất, và MatMul cuối cùng tính tổng có trọng số của Values.}
    \label{fig:scaled_dot_product_attention}
\end{figure}

\subsection{Chú ý Đa đầu (Multi-Head Attention) - Sức mạnh từ sự đa dạng}
\label{ssec:multi_head_attention}
Một cơ chế attention duy nhất chỉ có thể nắm bắt một loại mối quan hệ. Multi-Head Attention giải quyết hạn chế này bằng cách chạy nhiều attention song song, mỗi cái tập trung vào khía cạnh khác nhau.

\subsubsection{Ý tưởng chính}
Thay vì có một attention lớn, ta chia thành $h$ attention nhỏ hơn (heads). Mỗi head học một "góc nhìn" khác:
\begin{itemize}
    \item Head 1 có thể học quan hệ cú pháp (chủ ngữ - vị ngữ)
    \item Head 2 có thể học quan hệ ngữ nghĩa (từ đồng nghĩa)
    \item Head 3 có thể học khoảng cách (từ gần nhau)
    \item ...
\end{itemize}

\subsubsection{Công thức toán học}
$$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O $$
$$ \text{với } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$

Trong đó:
\begin{itemize}
    \item $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{model} \times d_k}$: Ma trận chiếu cho head thứ $i$
    \item $W^O \in \mathbb{R}^{hd_v \times d_{model}}$: Ma trận chiếu đầu ra
    \item $d_k = d_v = d_{model}/h$: Kích thước mỗi head
\end{itemize}

BERT\textsubscript{BASE} sử dụng $h=12$ heads với $d_{model}=768$, nên mỗi head có kích thước $d_k=64$.

\subsubsection{Ví dụ thực tế}
Trong câu "The bank will not approve the loan", các head khác nhau có thể học:
\begin{itemize}
    \item Head 1: Liên kết "bank" với "loan" (quan hệ ngữ nghĩa tài chính)
    \item Head 2: Liên kết "will not" với "approve" (phủ định)
    \item Head 3: Liên kết "the" với danh từ theo sau (quan hệ ngữ pháp)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/multihead.png}
    \caption{Cơ chế Multi-Head Attention. Đầu vào được chiếu thành $h$ bộ Q, K, V độc lập. Mỗi head thực hiện attention riêng, sau đó các kết quả được nối lại và chiếu qua $W^O$ để tạo đầu ra cuối cùng.}
    \label{fig:multi_head_attention}
\end{figure}

\subsection{Position-wise Feed-Forward Networks - Biến đổi phi tuyến}
\label{ssec:feed_forward_networks}
Sau khi tổng hợp thông tin qua attention, mỗi vị trí được xử lý qua một mạng FFN giống nhau:

$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$

Đặc điểm quan trọng:
\begin{itemize}
    \item Cùng một FFN được áp dụng cho mọi vị trí (parameter sharing)
    \item Lớp ẩn có kích thước lớn hơn ($d_{ff}=3072$ so với $d_{model}=768$)
    \item hàm kích hoạt ReLU tạo tính phi tuyến
\end{itemize}

FFN đóng vai trò như bộ xử lý đặc trưng cục bộ sau khi đã có ngữ cảnh toàn cục từ attention.

\subsection{Residual Connections và Layer Normalization}
\label{ssec:residual_layer_norm}
Mỗi sublayer (attention hoặc FFN) được bọc bởi:
$$ \text{LayerNorm}(x + \text{Sublayer}(x)) $$

Lợi ích:
\begin{itemize}
    \item \textbf{Residual connection ($x +$):} Cho phép gradient flow trực tiếp, giúp huấn luyện mạng nơ-ron sâu
    \item \textbf{Layer normalization:} Chuẩn hóa theo chiều features, ổn định quá trình học
\end{itemize}

\subsection{Biểu diễn Đầu vào của BERT - Kết hợp thông tin đa chiều}
\label{ssec:input_representation_bert}
BERT xây dựng biểu diễn đầu vào bằng cách cộng ba loại embedding:

$$ \text{Input}_i = \text{TokenEmb}_i + \text{SegmentEmb}_i + \text{PositionEmb}_i $$

\subsubsection{Token Embeddings}
\begin{itemize}
    \item Sử dụng WordPiece tokenization với vocabulary $\approx$ 30,000 tokens
    \item Ví dụ: "playing" → ["play", "\#\#ing"]
    \item Giúp xử lý từ hiếm và từ mới (out-of-vocabulary)
\end{itemize}

\subsubsection{Segment Embeddings}  
\begin{itemize}
    \item Phân biệt câu A và câu B trong các tác vụ cặp câu
    \item Chỉ có 2 giá trị: $E_A$ cho câu đầu, $E_B$ cho câu thứ hai
    \item Ví dụ: "[CLS] How are you ? [SEP] I am fine [SEP]"
    \begin{itemize}
        \item Tokens của "How are you ?" nhận $E_A$
        \item Tokens của "I am fine" nhận $E_B$
    \end{itemize}
\end{itemize}

\subsubsection{Position Embeddings}
\begin{itemize}
    \item Transformer không có cấu trúc tuần tự nên cần thông tin vị trí
    \item BERT học position embeddings (khác với Transformer gốc dùng sinusoidal)
    \item Giới hạn 512 positions (có thể mở rộng khi cần)
\end{itemize}

\subsubsection{Special Tokens}
\begin{itemize}
    \item \textbf{[CLS]:} Token đầu tiên, biểu diễn cuối cùng dùng cho classification
    \item \textbf{[SEP]:} Phân tách câu và đánh dấu kết thúc
    \item \textbf{[MASK]:} Dùng trong pre-training MLM
    \item \textbf{[PAD]:} Padding để các chuỗi có cùng độ dài
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/input_representation.png}
    \caption{Biểu diễn đầu vào của BERT. Ví dụ cho cặp câu "He likes playing" và "My dog is cute". Ba loại embeddings được cộng theo từng vị trí để tạo input embedding cuối cùng. Token [CLS] ở đầu sẽ được dùng cho các tác vụ phân loại.}
    \label{fig:bert_input_representation}
\end{figure}

\subsection{Các phiên bản BERT}
\label{ssec:bert_versions}
Google phát hành hai phiên bản chính:

\begin{table}[H]
\centering
\caption{So sánh các phiên bản BERT}
\begin{tabular}{lccccc}
\toprule
\textbf{Phiên bản} & \textbf{Layers (L)} & \textbf{Hidden (H)} & \textbf{Heads (A)} & \textbf{Parameters} & \textbf{Training Time} \\
\midrule
BERT\textsubscript{BASE} & 12 & 768 & 12 & 110M & 4 days on 16 TPUs \\
BERT\textsubscript{LARGE} & 24 & 1024 & 16 & 340M & 4 days on 64 TPUs \\
\bottomrule
\end{tabular}
\end{table}

Sự khác biệt không chỉ về kích thước mà còn về khả năng học:
\begin{itemize}
    \item BERT\textsubscript{LARGE} có capacity lớn hơn, phù hợp cho tác vụ phức tạp
    \item BERT\textsubscript{BASE} hiệu quả hơn về tài nguyên, phù hợp cho deployment
    \item Cả hai đều vượt trội so với các mô hình trước đó
\end{itemize}


%% =========================================================================
%% CHƯƠNG 3: CÁC TÁC VỤ TIỀN HUẤN LUYỆN CỦA BERT
%% =========================================================================
\section{Các Tác vụ Tiền huấn luyện của BERT}
\label{sec:pre_training_tasks}
BERT được tiền huấn luyện đồng thời trên hai tác vụ tự giám sát (self-supervised): Masked Language Model (MLM) và Next Sentence Prediction (NSP). Các tác vụ này cho phép BERT học từ dữ liệu văn bản không cần gán nhãn thủ công.

\subsection{Masked Language Model (MLM) - Đột phá cho học hai chiều}
\label{ssec:mlm}

\subsubsection{Vấn đề cốt lõi}
Các mô hình ngôn ngữ truyền thống chỉ có thể học một chiều vì lý do đơn giản: nếu cho phép mô hình "nhìn" cả hai chiều khi dự đoán từ tiếp theo, nó sẽ "gian lận" bằng cách nhìn trực tiếp vào từ cần dự đoán. BERT giải quyết vấn đề này bằng cách: che ngẫu nhiên một số từ và yêu cầu mô hình dự đoán chúng từ ngữ cảnh.

\subsubsection{Chiến lược Masking 80-10-10}
BERT che 15\% tokens trong mỗi chuỗi, nhưng không phải lúc nào cũng thay bằng [MASK]:

\begin{itemize}
    \item \textbf{80\% trường hợp → [MASK]:} 
    \begin{itemize}
        \item Input: "The cat sat on the mat"
        \item Masked: "The cat [MASK] on the mat"
        \item Target: Dự đoán "sat"
    \end{itemize}
    
    \item \textbf{10\% trường hợp → Random token:}
    \begin{itemize}
        \item Input: "The cat sat on the mat"
        \item Masked: "The cat apple on the mat"
        \item Target: Dự đoán "sat" (không phải "apple")
    \end{itemize}
    
    \item \textbf{10\% trường hợp → Giữ nguyên:}
    \begin{itemize}
        \item Input: "The cat sat on the mat"
        \item Masked: "The cat sat on the mat"
        \item Target: Vẫn dự đoán "sat"
    \end{itemize}
\end{itemize}

\subsubsection{Tại sao cần chiến lược này?}
\begin{enumerate}
    \item \textbf{Vấn đề [MASK] token:} Trong quá trình fine-tuning và sử dụng thực tế, không có token [MASK]. Nếu chỉ train với [MASK], mô hình sẽ học cách xử lý [MASK] tốt nhưng kém với các từ thực.
    
    \item \textbf{Random replacement (10\%):} Buộc mô hình phải học biểu diễn tốt cho MỌI token, không chỉ [MASK]. Mô hình phải "nghi ngờ" mọi từ có thể bị thay thế.
    
    \item \textbf{Keep unchanged (10\%):} Đảm bảo mô hình cũng học cách biểu diễn các từ không bị thay đổi. Tạo cân bằng trong học tập.
\end{enumerate}

\subsubsection{Ví dụ huấn luyện MLM}
Câu gốc: "BERT revolutionized natural language processing in 2018"
\begin{enumerate}
    \item Chọn ngẫu nhiên 15\% tokens (giả sử chọn "revolutionized" và "2018")

    \item Áp dụng chiến lược:
    \begin{itemize}
        \item "revolutionized" → [MASK] (80\% chance)
        \item "2018" → "cat" (10\% chance - random)
    \end{itemize}
\end{enumerate}

=> Input cho BERT: "BERT [MASK] natural language processing in cat"

Mục tiêu: 
\begin{itemize}
    \item Tại vị trí [MASK]: dự đoán "revolutionized"
    \item Tại vị trí "cat": dự đoán "2018"
    \item Các vị trí khác: không tính loss
\end{itemize}

\subsubsection{Hàm loss cho MLM}
Loss chỉ được tính cho các vị trí bị mask:
$$ \mathcal{L}_{MLM} = -\sum_{i \in \text{masked}} \log P(w_i | \text{context}) $$

Trong đó $P(w_i | \text{context})$ là xác suất dự đoán đúng từ gốc tại vị trí $i$.

\subsection{Next Sentence Prediction (NSP) - Học quan hệ giữa câu}
\label{ssec:nsp}

\subsubsection{Động lực}
Nhiều tác vụ NLP quan trọng yêu cầu hiểu mối quan hệ giữa các câu:
\begin{itemize}
    \item \textbf{Question Answering:} Câu hỏi và đoạn văn có liên quan?
    \item \textbf{Natural Language Inference:} Câu A kéo theo câu B?
    \item \textbf{Paraphrasing:} Hai câu có cùng ý nghĩa?
\end{itemize}

MLM chỉ học quan hệ trong câu, NSP bổ sung khả năng học quan hệ giữa các câu.

\subsubsection{Cách thức hoạt động}
\begin{enumerate}
    \item \textbf{Tạo dữ liệu huấn luyện:}
    \begin{itemize}
        \item 50\% mẫu: Câu B thực sự theo sau câu A trong văn bản (label: IsNext)
        \item 50\% mẫu: Câu B là câu ngẫu nhiên từ corpus (label: NotNext)
    \end{itemize}
    
    \item \textbf{Input format:}
    \begin{verbatim}
    [CLS] Sentence A [SEP] Sentence B [SEP]
    \end{verbatim}
    
    \item \textbf{Prediction:} Sử dụng biểu diễn cuối cùng của [CLS] token qua một classifier:
    $$ P(\text{IsNext}) = \text{softmax}(W_{NSP} \cdot h_{[CLS]} + b_{NSP}) $$
\end{enumerate}

\subsubsection{Ví dụ cụ thể}
\textbf{Positive example (IsNext):}
\begin{itemize}
    \item A: "The weather forecast predicts heavy rain tomorrow."
    \item B: "I should bring an umbrella to work."
    \item Logic: B là hệ quả logic của A
\end{itemize}

\textbf{Negative example (NotNext):}
\begin{itemize}
    \item A: "The weather forecast predicts heavy rain tomorrow."
    \item B: "Python is a popular programming language."
    \item Logic: Không có quan hệ ngữ nghĩa
\end{itemize}

\subsubsection{Vai trò của [CLS] token}
[CLS] token đóng vai trò như "summary vector" của cả cặp câu:
\begin{itemize}
    \item Vị trí đầu tiên, có thể attend đến mọi token khác
    \item Qua các layer, tích lũy thông tin từ cả hai câu
    \item Biểu diễn cuối cùng chứa thông tin về mối quan hệ câu
    \item Được sử dụng cho mọi tác vụ classification sau này
\end{itemize}

\subsubsection{Combined Loss}
BERT được train với combined loss:
$$ \mathcal{L}_{total} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP} $$

Cả hai loss được tối ưu đồng thời, giúp BERT học cả biểu diễn từ (MLM) và quan hệ câu (NSP).

\subsubsection{Tranh cãi về NSP}
Các nghiên cứu sau (RoBERTa, ALBERT) cho thấy:
\begin{itemize}
    \item NSP có thể không cần thiết hoặc thậm chí có hại cho một số tác vụ
    \item Vấn đề: Negative samples (random sentences) quá dễ phân biệt
    \item Cải tiến: Sentence Order Prediction (SOP) trong ALBERT - dự đoán thứ tự câu
\end{itemize}

Tuy nhiên, trong thiết kế gốc, NSP đóng góp vào thành công của BERT trên nhiều benchmark.

%% =========================================================================
%% CHƯƠNG 4: TINH CHỈNH (FINE-TUNING) BERT
%% =========================================================================
\section{Fine-tuning BERT cho các tác vụ Cụ thể}
\label{sec:fine_tuning_bert}
Sau khi được tiền huấn luyện trên khối lượng dữ liệu khổng lồ (BookCorpus + Wikipedia $\approx$ 3.3 tỷ từ), BERT có thể được fine-tune nhanh chóng cho các tác vụ cụ thể. Đây là minh chứng cho sức mạnh của transfer learning trong NLP.

\subsection{Quy trình fine-tuning Chung}
\label{ssec:quy_trinh_tinh_chinh}

\subsubsection{Nguyên lý cơ bản}
Fine-tuning BERT tuân theo triết lý "minimal architectural changes":
\begin{itemize}
    \item Giữ nguyên toàn bộ kiến trúc BERT đã pre-train
    \item Chỉ thêm một task-specific head (thường là 1-2 layers)
    \item Train toàn bộ parameters (BERT + head) trên dữ liệu của tác vụ
\end{itemize}

\subsubsection{So sánh với Feature-based Approach}
\begin{table}[H]
\centering
\caption{So sánh Fine-tuning vs Feature-based}
\begin{tabular}{lll}
\toprule
\textbf{Tiêu chí} & \textbf{Fine-tuning (BERT)} & \textbf{Feature-based (ELMo)} \\
\midrule
Parameters updated & Toàn bộ & Chỉ task-specific layers \\
Flexibility & Cao (adapt cho từng task) & Thấp (fixed features) \\
Training time & Lâu hơn & Nhanh hơn \\
Performance & Tốt nhất & Tốt \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Lợi thế của Fine-tuning}
\begin{enumerate}
    \item \textbf{Task-specific adaptation:} Mô hình được điều chỉnh cho tác vụ cụ thể khác nhau
    \item \textbf{End-to-end optimization:} Gradient flow từ output đến input embeddings
    \item \textbf{Superior performance:} Thường đạt kết quả tốt hơn feature-based
\end{enumerate}

\subsection{Ví dụ fine-tuning cho các Tác vụ Phổ biến}
\label{ssec:vi_du_tinh_chinh}

\subsubsection{Sentence Pair Classification}
\textbf{Task:} Natural Language Inference, Paraphrase Detection, etc.

\textbf{Input format:}
\begin{verbatim}
[CLS] Premise text [SEP] Hypothesis text [SEP]
\end{verbatim}

\textbf{Architecture:}
\begin{itemize}
    \item Lấy final hidden state của [CLS]: $h_{[CLS]} \in \mathbb{R}^H$
    \item Classification layer: $W \in \mathbb{R}^{K \times H}$ (K = số classes)
    \item Output: $P(y|x) = \text{softmax}(h_{[CLS]} \cdot W^T)$
\end{itemize}

\textbf{Ví dụ - Natural Language Inference:}
\begin{itemize}
    \item Premise: "A man is playing guitar"
    \item Hypothesis: "A person is making music"
    \item Classes: Entailment, Contradiction, Neutral
    \item Expected: Entailment
\end{itemize}

\subsubsection{Single Sentence Classification}  
\textbf{Task:} Sentiment Analysis, Topic Classification, etc.

\textbf{Input format:}
\begin{verbatim}
[CLS] Single sentence text [SEP]
\end{verbatim}

\textbf{Architecture:} Tương tự sentence pair nhưng chỉ có một câu

\textbf{Ví dụ - Sentiment Analysis:}
\begin{itemize}
    \item Input: "This movie exceeded all my expectations!"
    \item Classes: Positive, Negative
    \item Expected: Positive
\end{itemize}

\subsubsection{Question Answering}
\textbf{Task:} Extract answer span from context (SQuAD style)

\textbf{Input format:}
\begin{verbatim}
[CLS] Question [SEP] Context paragraph [SEP]
\end{verbatim}

\textbf{Architecture đặc biệt:}
\begin{itemize}
    \item Học 2 vectors: Start vector $S$ và End vector $E$
    \item Với mỗi token $i$ trong context:
    \begin{itemize}
        \item $P_{start}(i) = \frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}$
        \item $P_{end}(i) = \frac{e^{E \cdot T_i}}{\sum_j e^{E \cdot T_j}}$
    \end{itemize}
    \item Answer = span từ $\arg\max_i P_{start}(i)$ đến $\arg\max_j P_{end}(j)$ với $j \geq i$
\end{itemize}

\textbf{Ví dụ:}
\begin{itemize}
    \item Question: "When was BERT published?"
    \item Context: "BERT was published by Google AI in October 2018..."
    \item Answer span: "October 2018"
\end{itemize}

\subsubsection{Token Classification (NER)}
\textbf{Task:} Gán nhãn cho từng token

\textbf{Architecture:}
\begin{itemize}
    \item Mỗi token output $T_i$ qua classification layer riêng
    \item $P(y_i|x) = \text{softmax}(T_i \cdot W^T)$
\end{itemize}

\textbf{Ví dụ - Named Entity Recognition:}
\begin{verbatim}
Input:  [CLS] John works at Google in London [SEP]
Labels:   O   B-PER  O    O  B-ORG  O  B-LOC   O
\end{verbatim}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/pretraining_finetuning.png}
    \caption{Fine-tuning BERT cho các tác vụ khác nhau. Lưu ý cách sử dụng [CLS] cho classification tasks và individual tokens cho sequence labeling. Architecture thay đổi tối thiểu - chủ yếu là output layer.}
    \label{fig:bert_finetuning_tasks}
\end{figure}

\subsubsection{Hyperparameters cho Fine-tuning}
Từ bài báo gốc, các hyperparameters được khuyến nghị:
\begin{itemize}
    \item \textbf{Batch size:} 16, 32
    \item \textbf{Learning rate:} 2e-5, 3e-5, 5e-5 
    \item \textbf{Epochs:} 2, 3, 4
    \item \textbf{Warmup:} 0.1 of total steps
    \item \textbf{Dropout:} 0.1 (giữ nguyên từ pre-training)
\end{itemize}

Lưu ý: Datasets nhỏ nhạy cảm hơn với hyperparameters. Nên thử nhiều random seeds.

%% =========================================================================
%% CHƯƠNG 5: THỰC NGHIỆM SENTIMENT ANALYSIS
%% =========================================================================
\section{Thực nghiệm với BERT: Phân loại Cảm xúc Văn bản}
\label{sec:thuc_nghiem_bert_rewrite}

\subsection{Giới thiệu Thực nghiệm}
\label{ssec:gioi_thieu_thuc_nghiem_rewrite}
Chương thực nghiệm này được thiết kế để đạt được hai mục tiêu chính. Thứ nhất, minh chứng sự hiểu biết sâu sắc về kiến trúc và các cơ chế hoạt động bên trong của mô hình BERT. Điều này được thực hiện thông qua việc tự xây dựng (build-from-scratch) và kiểm tra các thành phần cốt lõi của BERT, dựa trên mã nguồn trong tệp \texttt{bert.py} và các thử nghiệm ban đầu trong tệp Jupyter Notebook \texttt{bert\_application.ipynb}.[1] Thứ hai, áp dụng một mô hình BERT đã được tiền huấn luyện để giải quyết một bài toán thực tế trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) là phân loại cảm xúc văn bản. Thí nghiệm này sử dụng bộ dữ liệu IMDB Movie Reviews, đồng thời đánh giá hiệu quả của mô hình và so sánh với các phương pháp cơ sở.[1] Cách tiếp cận này hoàn toàn phù hợp với yêu cầu của Đề án cuối kỳ Hướng 2, đó là "Nghiên cứu và trình bày lại một bài báo học thuật nổi bật trong học máy" và "Thử nghiệm thuật toán hoặc mô hình trên một tập dữ liệu cụ thể".[1]

Phương pháp thực nghiệm được chia thành hai giai đoạn chính. Giai đoạn đầu tiên tập trung vào việc xây dựng và xác minh các khối kiến trúc cơ bản của mô hình BERT. Mục đích của giai đoạn này không phải là tạo ra một mô hình BERT hoàn chỉnh, có khả năng cạnh tranh từ đầu cho tác vụ phân loại cảm xúc, mà là để đào sâu kiến thức và hiểu rõ hơn về cơ chế hoạt động phức tạp bên trong của BERT, chẳng hạn như cơ chế tự chú ý (self-attention) hay mạng truyền thẳng (feed-forward network). Việc tự triển khai các thành phần này giúp củng cố sự hiểu biết lý thuyết đã được trình bày ở các chương trước. Giai đoạn thứ hai chuyển sang sử dụng một mô hình BERT tiêu chuẩn, đã được tiền huấn luyện sẵn, cụ thể là \texttt{bert-base-uncased} từ thư viện Hugging Face Transformers.[1] Mô hình này sau đó được tinh chỉnh (fine-tuning) cho bài toán phân loại cảm xúc. Lựa chọn này không chỉ đảm bảo tính mạnh mẽ và khả năng tái tạo của kết quả thực nghiệm mà còn cho phép so sánh một cách công bằng với các nghiên cứu và phương pháp khác trong lĩnh vực. Việc kết hợp giữa xây dựng để hiểu và sử dụng thư viện để ứng dụng phản ánh một quy trình làm việc khoa học và thực tiễn trong nghiên cứu và phát triển AI.

Bộ dữ liệu được sử dụng chính trong các thực nghiệm là IMDB Movie Reviews. Đây là một bộ dữ liệu tiêu chuẩn và phổ biến cho bài toán phân loại cảm xúc nhị phân, bao gồm 50,000 bài đánh giá phim của người dùng, được chia đều thành 25,000 mẫu cho tập huấn luyện và 25,000 mẫu cho tập kiểm tra.[1] Mỗi bài đánh giá được gán nhãn là "tích cực" (positive) hoặc "tiêu cực" (negative). Trong tệp \texttt{bert\_application.ipynb}, để thuận tiện cho việc kiểm tra nhanh và minh họa quy trình, các tập con nhỏ hơn của bộ dữ liệu IMDB đã được sử dụng cho một số thử nghiệm và quá trình huấn luyện ban đầu, ví dụ như 200 mẫu cho tập huấn luyện và 100 mẫu cho tập kiểm tra.[1] Các kết quả và phân tích chi tiết về hiệu suất của mô hình trên tác vụ phân loại cảm xúc sẽ được trình bày dựa trên các thử nghiệm được thực hiện với các tập dữ liệu này.

\subsection{Xây dựng các Thành phần Cốt lõi của Mô hình BERT (\texttt{bert.py})}
\label{ssec:xay_dung_thanh_phan_cot_loi_rewrite}
Tiểu mục này tập trung trình bày chi tiết quá trình tự xây dựng và kiểm tra các thành phần chính của kiến trúc BERT, dựa trên mã nguồn được phát triển trong tệp \texttt{bert.py} và các kịch bản kiểm thử được thực hiện trong Giai đoạn A của tệp \texttt{bert\_application.ipynb}.[1] Mục tiêu chính của phần này không phải là mô tả việc xây dựng một mô hình BERT được huấn luyện đầy đủ từ đầu để giải quyết bài toán phân loại cảm xúc, mà là để minh họa sự nắm vững về mặt kỹ thuật đối với các khối xây dựng cơ bản của BERT, qua đó làm sâu sắc thêm hiểu biết về lý do tại sao BERT lại hiệu quả.

\subsubsection{Kiến trúc BERT Tùy chỉnh và Cấu hình Thử nghiệm}
\label{sssec:kien_truc_tuy_chinh_cau_hinh_thu_nghiem_rewrite}
Mô hình BERT, như đã được giới thiệu trong Chương \ref{sec:kien_truc_bert}, dựa trên kiến trúc Transformer Encoder.[1] Kiến trúc này bao gồm nhiều lớp mã hóa (encoder layers) xếp chồng lên nhau. Mỗi lớp mã hóa lại chứa hai khối chính: cơ chế tự chú ý đa đầu (Multi-Head Self-Attention) và một mạng truyền thẳng theo vị trí (Position-wise Feed-Forward Network). Xung quanh mỗi khối này là các kết nối dư (residual connections) và chuẩn hóa lớp (layer normalization) để hỗ trợ quá trình huấn luyện các mạng sâu.

Để kiểm tra các thành phần tự xây dựng trong \texttt{bert.py}, một phiên bản thu nhỏ của mô hình BERT đã được định nghĩa và sử dụng. Cấu hình này, được gọi là \texttt{small\_config} trong \texttt{bert\_application.ipynb} [1], cho phép thực hiện các kiểm tra một cách nhanh chóng và hiệu quả về mặt tài nguyên tính toán, trong khi vẫn đảm bảo tính đại diện cho các chức năng cốt lõi. Việc sử dụng một cấu hình thu nhỏ cho thấy một sự cân nhắc thực tế trong việc thiết kế thí nghiệm để xác minh tính đúng đắn của các module phức tạp mà không cần đến chi phí tính toán lớn của việc tái tạo hoàn toàn các mô hình như \texttt{BERT-base}. Các tham số chính của cấu hình này được trình bày trong Bảng \ref{tab:small_bert_config_rewrite}.

\begin{table}[H]
\centering
\caption{Cấu hình Mô hình BERT Thu nhỏ cho Kiểm tra Thành phần (dựa trên \texttt{small\_config} từ [1])}
\label{tab:small_bert_config_rewrite}
\begin{tabular}{lc}
\toprule
\textbf{Tham số} & \textbf{Giá trị} \\
\midrule
Kích thước từ điển (\texttt{vocab\_size}) & 30522 \\
Kích thước ẩn (\texttt{hidden\_size}) & 128 \\
Số lớp ẩn (\texttt{num\_hidden\_layers}) & 2 \\
Số đầu chú ý (\texttt{num\_attention\_heads}) & 4 \\
Kích thước trung gian FFN (\texttt{intermediate\_size}) & 512 \\
Số vị trí tối đa (\texttt{max\_position\_embeddings}) & 128 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Triển khai các Thành phần Cốt lõi}
\label{sssec:trien_khai_thanh_phan_cot_loi_rewrite}
Dựa trên kiến trúc Transformer Encoder, các thành phần chính sau đã được triển khai trong tệp \texttt{bert.py}:

\begin{itemize}
    \item \textbf{Lớp Nhúng (Embedding Layer - \texttt{BERTEmbedding}):} Thành phần này chịu trách nhiệm chuyển đổi chuỗi token đầu vào thành các vector biểu diễn. Nó kết hợp ba loại nhúng riêng biệt thông qua phép cộng theo từng phần tử:
    \begin{itemize}
        \item \textit{Token Embeddings:} Mỗi token trong từ điển (ví dụ: WordPiece vocabulary với \texttt{vocab\_size} là 30522) được ánh xạ tới một vector dày đặc.
        \item \textit{Segment Embeddings:} Được sử dụng để phân biệt giữa hai câu trong các tác vụ đầu vào dạng cặp câu (ví dụ: câu A và câu B).
        \item \textit{Position Embeddings:} Cung cấp thông tin về vị trí của mỗi token trong chuỗi, điều này rất quan trọng vì kiến trúc Transformer không có tính tuần tự vốn có như RNN.
    \end{itemize}
    Lớp này cũng bao gồm chuẩn hóa lớp (Layer Normalization) và dropout.

    \item \textbf{Cơ chế Tự Chú ý Đa đầu (Multi-Head Self-Attention - \texttt{MultiHeadAttention}):} Đây là trái tim của Transformer, cho phép mô hình cân nhắc tầm quan trọng của các từ khác nhau trong chuỗi khi biểu diễn một từ cụ thể.
    \begin{itemize}
        \item Kích thước ẩn (\texttt{hidden\_size}) được chia đều cho số lượng đầu chú ý (\texttt{num\_attention\_heads}). Ví dụ, với \texttt{hidden\_size=128} và \texttt{num\_attention\_heads=4}, mỗi đầu sẽ có kích thước là 32.
        \item Đối với mỗi đầu, các ma trận Query (Q), Key (K), và Value (V) được tạo ra từ đầu vào thông qua các phép biến đổi tuyến tính riêng biệt.
        \item Cơ chế Scaled Dot-Product Attention được áp dụng song song trên mỗi đầu: $Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$.
        \item Kết quả từ tất cả các đầu chú ý sau đó được ghép lại (concatenated) và truyền qua một lớp tuyến tính cuối cùng để tạo ra biểu diễn đầu ra.
    \end{itemize}

    \item \textbf{Mạng Truyền thẳng theo Vị trí (Position-wise Feed-Forward Network - \texttt{PositionwiseFeedForward}):} Sau khối tự chú ý đa đầu, mỗi vị trí trong chuỗi được xử lý độc lập bởi một mạng truyền thẳng.
    \begin{itemize}
        \item Mạng này bao gồm hai lớp tuyến tính. Lớp đầu tiên mở rộng kích thước từ \texttt{hidden\_size} lên \texttt{intermediate\_size} (ví dụ: 128 lên 512) và sử dụng một hàm kích hoạt (thường là GELU trong BERT gốc, cần kiểm tra trong \texttt{bert.py} nếu có sự khác biệt). Lớp thứ hai chiếu ngược lại từ \texttt{intermediate\_size} về \texttt{hidden\_size}.
        \item Cùng một bộ trọng số FFN được áp dụng cho tất cả các vị trí.
    \end{itemize}

    \item \textbf{Lớp Mã hóa Transformer (Transformer Encoder Layer - \texttt{EncoderLayer}):} Mỗi lớp mã hóa bao gồm một khối \texttt{MultiHeadAttention} theo sau bởi một khối \texttt{PositionwiseFeedForward}.
    \begin{itemize}
        \item Kết nối dư (residual connection) được áp dụng xung quanh mỗi khối con, theo sau là chuẩn hóa lớp (layer normalization). Tức là, đầu ra của mỗi khối con là $\text{LayerNorm}(x + \text{Sublayer}(x))$, trong đó $x$ là đầu vào của khối con đó.
    \end{itemize}

    \item \textbf{Mô hình BERT cho Tiền huấn luyện (\texttt{BertForPreTraining}):} Đây là lớp bao bọc toàn bộ kiến trúc BERT, xếp chồng nhiều \texttt{EncoderLayer} lên nhau (số lượng được xác định bởi \texttt{num\_hidden\_layers}). Quan trọng hơn, việc triển khai lớp này, ngay cả khi không thực hiện quá trình tiền huấn luyện đầy đủ, cho thấy sự hiểu biết rằng sức mạnh của BERT không chỉ đến từ kiến trúc Transformer mà còn từ các tác vụ tiền huấn luyện cụ thể của nó.
    \begin{itemize}
        \item Lớp này thường bao gồm các "đầu" (heads) cụ thể cho hai tác vụ tiền huấn luyện chính của BERT là Masked Language Model (MLM) và Next Sentence Prediction (NSP), như đã được mô tả chi tiết trong Chương \ref{sec:pre_training_tasks}.[1]
        \item Đối với MLM, một lớp tuyến tính được thêm vào để dự đoán các token đã bị che (masked) trong đầu vào dựa trên biểu diễn đầu ra của BERT encoder tại các vị trí tương ứng.
        \item Đối với NSP, một lớp tuyến tính khác được sử dụng để phân loại mối quan hệ giữa hai câu đầu vào (ví dụ: câu thứ hai có phải là câu tiếp theo của câu thứ nhất trong văn bản gốc hay không), dựa trên biểu diễn của token \texttt{} đặc biệt.
    \end{itemize}
\end{itemize}
Việc tự tay triển khai các thành phần này, từ việc quản lý kích thước của các tensor, đảm bảo áp dụng đúng các phép toán ma trận, cho đến việc kết hợp các kết nối dư và chuẩn hóa lớp, mang lại những hiểu biết thực tế quý báu mà việc chỉ đọc lý thuyết không thể có được.

\subsubsection{Kiểm tra và Xác minh Chức năng Thành phần}
\label{sssec:kiem_tra_xac_minh_thanh_phan_rewrite}
Để đảm bảo tính đúng đắn của các thành phần tự xây dựng trong \texttt{bert.py}, các hàm kiểm tra đơn vị đã được thực hiện trong Giai đoạn A của tệp \texttt{bert\_application.ipynb}.[1] Quá trình này rất quan trọng để xác minh rằng mỗi module hoạt động như mong đợi trước khi tích hợp chúng vào một hệ thống lớn hơn.

\begin{itemize}
    \item \textbf{Kiểm tra Cơ chế Chú ý (\texttt{test\_attention\_mechanism}):} Một thử nghiệm cơ bản được thực hiện để xác minh lớp \texttt{MultiHeadAttention}. Dữ liệu đầu vào giả lập (ví dụ: tensor ngẫu nhiên với kích thước phù hợp) được truyền qua một thực thể của lớp \texttt{MultiHeadAttention}. Kết quả đầu ra sau đó được kiểm tra để đảm bảo rằng kích thước (shape) của nó khớp với kích thước kỳ vọng, thường là \texttt{[batch\_size, sequence\_length, hidden\_size]}.[1]

    \item \textbf{Kiểm tra Toàn bộ Mô hình BERT (phiên bản nhỏ - \texttt{test\_bert\_model}):} Một thử nghiệm tích hợp hơn được tiến hành bằng cách khởi tạo một thực thể của lớp \texttt{BertForPreTraining} sử dụng cấu hình thu nhỏ \texttt{small\_config} đã đề cập ở trên. Dữ liệu đầu vào giả lập, bao gồm \texttt{input\_ids}, \texttt{attention\_mask}, \texttt{token\_type\_ids}, \texttt{masked\_lm\_labels}, và \texttt{next\_sentence\_label}, được tạo ra và truyền qua mô hình. Các khía cạnh sau được kiểm tra [1]:
    \begin{itemize}
        \item \textbf{Giá trị Loss:} Các giá trị loss tính toán được cho cả Masked Language Model (MLM Loss) và Next Sentence Prediction (NSP Loss) được kiểm tra để đảm bảo chúng là các giá trị số hợp lệ (ví dụ: không phải NaN, có giá trị dương). Điều này cho thấy quá trình tính toán loss đang hoạt động.
        \item \textbf{Kích thước Logits Đầu ra:} Kích thước của các logits đầu ra từ đầu MLM (dự đoán từ bị che) và đầu NSP (dự đoán câu tiếp theo) được xác minh. Ví dụ, logits MLM phải có kích thước \texttt{[batch\_size, num\_masked\_tokens, vocab\_size]} (hoặc \texttt{[batch\_size, sequence\_length, vocab\_size]} nếu dự đoán cho mọi token) và logits NSP phải có kích thước \texttt{[batch\_size, 2]}.
    \end{itemize}
\end{itemize}
Bảng \ref{tab:component_verification_summary_rewrite} tóm tắt các kiểm tra chính và kết quả quan sát được, cung cấp bằng chứng cụ thể rằng các thành phần tự xây dựng đã được kiểm tra một cách có hệ thống. Điều này không chỉ xác nhận tính đúng đắn của mã nguồn mà còn củng cố sự tự tin vào hiểu biết về kiến trúc BERT.

\begin{table}[H]
\centering
\caption{Tóm tắt Kiểm tra Chức năng Thành phần BERT Tùy chỉnh (dựa trên Giai đoạn A[1])}
\label{tab:component_verification_summary_rewrite}
\begin{tabular}{ll}
\toprule
\textbf{Thành phần/Kiểm tra} & \textbf{Kết quả/Trạng thái Quan sát} \\
\midrule
Kích thước đầu ra \texttt{MultiHeadAttention} & Khớp với kỳ vọng (ví dụ: \texttt{torch.Size()}) \\
Kích thước logits MLM từ \texttt{BertForPreTraining} & Khớp với kỳ vọng (ví dụ: \texttt{torch.Size()}) \\
Kích thước logits NSP từ \texttt{BertForPreTraining} & Khớp với kỳ vọng (ví dụ: \texttt{torch.Size()}) \\
Giá trị MLM Loss & Giá trị số hợp lệ (ví dụ: dương, không NaN) \\
Giá trị NSP Loss & Giá trị số hợp lệ (ví dụ: dương, không NaN) \\
\bottomrule
\multicolumn{2}{l}{\footnotesize{B: Kích thước lô (batch size), S: Độ dài chuỗi (sequence length), H: Kích thước ẩn (hidden size), V: Kích thước từ điển (vocab size)}}
\end{tabular}
\end{table}

\subsection{Ứng dụng Phân loại Cảm xúc Văn bản (\texttt{bert\_application.ipynb})}
\label{ssec:ung_dung_phan_loai_cam_xuc_rewrite}
Sau khi đã khám phá và xác minh các thành phần kiến trúc bên trong của BERT thông qua việc tự xây dựng, tiểu mục này chuyển trọng tâm sang ứng dụng thực tế của mô hình. Để giải quyết bài toán phân loại cảm xúc văn bản, một mô hình BERT tiêu chuẩn, đã được tiền huấn luyện từ thư viện Hugging Face Transformers, được sử dụng. Quy trình này, bao gồm tiền xử lý dữ liệu, thiết lập mô hình, tinh chỉnh và đánh giá, được thực hiện chủ yếu trong Giai đoạn B của tệp Jupyter Notebook \texttt{bert\_application.ipynb}.[1]

\subsubsection{Lựa chọn Mô hình và Bộ dữ liệu cho Tinh chỉnh}
\label{sssec:lua_chon_mo_hinh_du_lieu_finetuning_rewrite}

\begin{itemize}
    \item \textbf{Lựa chọn Mô hình:}
    Mô hình được chọn để tinh chỉnh là \texttt{BertForSequenceClassification} được tải từ thư viện \texttt{transformers} với trọng số tiền huấn luyện của \texttt{"bert-base-uncased"} và cấu hình cho 2 nhãn đầu ra (tích cực/tiêu cực).[1] Cụ thể, lệnh gọi là:
    \begin{verbatim}
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-uncased", 
        num_labels=2
    )
    \end{verbatim}
    Lý do cho lựa chọn này bao gồm:
    \begin{itemize}
        \item \textbf{Tận dụng Trọng số Tiền huấn luyện:} \texttt{bert-base-uncased} đã được tiền huấn luyện trên một kho dữ liệu văn bản khổng lồ (BooksCorpus và English Wikipedia). Điều này cung cấp cho mô hình một nền tảng hiểu biết ngôn ngữ sâu sắc và khả năng khái quát hóa tốt, rất quan trọng khi làm việc với các bộ dữ liệu tác vụ cụ thể có kích thước hạn chế.
        \item \textbf{Mô hình Tiêu chuẩn:} \texttt{bert-base-uncased} là một trong những mô hình BERT cơ sở được sử dụng rộng rãi nhất, cho phép dễ dàng so sánh kết quả với các nghiên cứu khác.
        \item \textbf{Kiến trúc Phù hợp:} Lớp \texttt{BertForSequenceClassification} được thiết kế đặc biệt cho các tác vụ phân loại văn bản. Nó tự động thêm một lớp phân loại tuyến tính (classifier head) lên trên cùng của BERT encoder, giúp đơn giản hóa quá trình thiết lập mô hình cho tác vụ cụ thể.
    \end{itemize}

    \item \textbf{Bộ dữ liệu và Tiền xử lý:}
    Bộ dữ liệu IMDB Movie Reviews tiếp tục được sử dụng cho tác vụ này. Trong Giai đoạn B của \texttt{bert\_application.ipynb}, các bước tiền xử lý sau đã được áp dụng cho các tập con dữ liệu (200 mẫu huấn luyện, 100 mẫu kiểm tra) [1]:
    \begin{itemize}
        \item \textbf{Tokenization:} Một \texttt{BertTokenizer} tương ứng với \texttt{"bert-base-uncased"} được sử dụng. Quá trình token hóa bao gồm việc chuyển đổi văn bản thô thành các token, sau đó thành các ID số. Các tham số quan trọng được sử dụng trong hàm \texttt{preprocess\_function} bao gồm:
            \begin{itemize}
                \item \texttt{truncation=True}: Cắt bớt các chuỗi dài hơn độ dài tối đa.
                \item \texttt{padding="max\_length"}: Đệm các chuỗi ngắn hơn để đạt độ dài tối đa.
                \item \texttt{max\_length=128}: Thiết lập độ dài tối đa của chuỗi đầu vào là 128 token.
            \end{itemize}
            Các token đặc biệt như \texttt{} và \texttt{} cũng được tự động thêm vào.
        \item \textbf{Chuẩn bị đầu vào cho PyTorch:} Sau khi token hóa, cột 'label' trong bộ dữ liệu được đổi tên thành 'labels' để tương thích với API của \texttt{Trainer}. Dữ liệu được định dạng thành các tensor PyTorch và các cột cần thiết cho quá trình huấn luyện (\texttt{input\_ids}, \texttt{attention\_mask}, \texttt{labels}) được chọn lọc.
    \end{itemize}
    Các bước làm sạch văn bản như loại bỏ thẻ HTML hay URL, mặc dù được mô tả trong báo cáo gốc [1], không được thực hiện một cách tường minh trong phần tiền xử lý của Giai đoạn B trong notebook \texttt{bert\_application.ipynb} [1] cho các tập con này; thay vào đó, notebook tập trung vào việc token hóa và chuẩn bị dữ liệu cho \texttt{Trainer}.
\end{itemize}

\subsubsection{Thiết lập và Tinh chỉnh Mô hình BERT}
\label{sssec:thiet_lap_tinh_chinh_bert_rewrite}
Sau khi đã chọn mô hình và chuẩn bị dữ liệu, bước tiếp theo là thiết lập và thực hiện quá trình tinh chỉnh.

\begin{itemize}
    \item \textbf{Kiến trúc Đầu ra cho Phân loại:}
    Như đã đề cập, lớp \texttt{BertForSequenceClassification} tự động thêm một "đầu phân loại" (classification head) lên trên cùng của mô hình BERT cơ sở. Đối với \texttt{bert-base-uncased}, đầu ra của BERT encoder là một chuỗi các vector ẩn có kích thước 768 cho mỗi token. Biểu diễn của token \texttt{} đặc biệt (thường là vector ẩn đầu tiên ở lớp cuối cùng) được lấy làm đại diện cho toàn bộ chuỗi đầu vào. Đầu phân loại này thường bao gồm một lớp dropout để chống overfitting, theo sau bởi một lớp tuyến tính (fully connected layer) để ánh xạ vector biểu diễn 768 chiều của \texttt{} sang không gian 2 chiều (tương ứng với 2 nhãn cảm xúc: tích cực và tiêu cực).[1, 1] Các trọng số của lớp tuyến tính này được khởi tạo ngẫu nhiên và sẽ được học trong quá trình tinh chỉnh.

    \item \textbf{Quy trình Huấn luyện:}
    Quá trình huấn luyện được quản lý hiệu quả bằng cách sử dụng lớp \texttt{Trainer} từ thư viện \texttt{transformers}.[1] Lớp này trừu tượng hóa nhiều chi tiết cấp thấp của vòng lặp huấn luyện, chẳng hạn như quản lý lô dữ liệu, tính toán gradient, và cập nhật trọng số.
    \begin{itemize}
        \item \textbf{Tham số Huấn luyện (\texttt{TrainingArguments}):} Các siêu tham số và cấu hình cho quá trình huấn luyện được định nghĩa thông qua một đối tượng \texttt{TrainingArguments}. Các thiết lập chính được sử dụng trong \texttt{bert\_application.ipynb} [1] bao gồm:
            \begin{itemize}
                \item \texttt{output\_dir="./results"}: Thư mục lưu trữ kết quả.
                \item \texttt{num\_train\_epochs=2}: Số epoch huấn luyện (mô hình duyệt qua toàn bộ tập huấn luyện 2 lần).
                \item \texttt{per\_device\_train\_batch\_size=8}: Kích thước lô cho mỗi thiết bị trong quá trình huấn luyện.
                \item \texttt{per\_device\_eval\_batch\_size=8}: Kích thước lô cho mỗi thiết bị trong quá trình đánh giá.
                \item \texttt{eval\_strategy="epoch"}: Thực hiện đánh giá sau mỗi epoch.
                \item \texttt{logging\_dir="./logs"}: Thư mục lưu trữ log huấn luyện.
                \item \texttt{logging\_steps=10}: Ghi log sau mỗi 10 bước huấn luyện.
                \item \texttt{learning\_rate}: Mặc dù không được đặt tường minh trong đoạn mã khởi tạo \texttt{TrainingArguments} trong [1], \texttt{Trainer} thường sử dụng giá trị mặc định (ví dụ: $5 \times 10^{-5}$) hoặc một lịch trình tốc độ học (learning rate scheduler).
                \item \texttt{fp16=torch.cuda.is\_available()}: Cho phép huấn luyện với độ chính xác hỗn hợp (mixed-precision) nếu có GPU, giúp tăng tốc độ và giảm bộ nhớ sử dụng.
            \end{itemize}
        \item \textbf{Optimizer và Loss Function:} \texttt{Trainer} tự động cấu hình optimizer phù hợp (thường là AdamW cho các mô hình BERT) và hàm loss. Đối với \texttt{BertForSequenceClassification}, hàm loss CrossEntropyLoss được sử dụng, đây là lựa chọn tiêu chuẩn cho các bài toán phân loại đa lớp (trong trường hợp này là nhị phân).
        \item \textbf{Thực thi Huấn luyện:} Một thực thể của \texttt{Trainer} được tạo với mô hình, các tham số huấn luyện, các tập dữ liệu huấn luyện và đánh giá đã được tiền xử lý, tokenizer, và một hàm \texttt{compute\_metrics} để tính toán các chỉ số đánh giá. Sau đó, phương thức \texttt{trainer.train()} được gọi để bắt đầu quá trình tinh chỉnh.[1]
    \end{itemize}
\end{itemize}
Bảng \ref{tab:finetuning_hyperparams_rewrite} tóm tắt các siêu tham số chính được sử dụng trong quá trình tinh chỉnh mô hình BERT cho bài toán phân loại cảm xúc. Việc ghi lại các thông số này là rất quan trọng để đảm bảo tính tái lập của thực nghiệm.

\begin{table}[H]
\centering
\caption{Các Siêu tham số Tinh chỉnh cho Phân loại Cảm xúc (dựa trên Giai đoạn B[1])}
\label{tab:finetuning_hyperparams_rewrite}
\begin{tabular}{ll}
\toprule
\textbf{Siêu tham số} & \textbf{Giá trị} \\
\midrule
Mô hình cơ sở & \texttt{bert-base-uncased} \\
Số Epochs & 2 \\
Kích thước lô (Batch Size) - Huấn luyện & 8 \\
Kích thước lô (Batch Size) - Đánh giá & 8 \\
Tốc độ học (Learning Rate) & Mặc định của \texttt{Trainer} (ví dụ: $5 \times 10^{-5}$) \\
Optimizer & AdamW (mặc định của \texttt{Trainer}) \\
Hàm Loss & CrossEntropyLoss \\
Độ dài chuỗi tối đa (\texttt{max\_length}) & 128 tokens \\
Đánh giá mỗi & 1 epoch \\
Huấn luyện độ chính xác hỗn hợp (\texttt{fp16}) & Bật nếu có GPU \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Kết quả Thực nghiệm và Đánh giá}
\label{sssec:ket_qua_thuc_nghiem_danh_gia_rewrite}
Sau khi quá trình tinh chỉnh hoàn tất, hiệu suất của mô hình được đánh giá trên tập kiểm tra.

\begin{itemize}
    \item \textbf{Chỉ số Đánh giá:}
    Chỉ số chính được sử dụng để đánh giá hiệu suất của mô hình là độ chính xác (accuracy). Một hàm \texttt{compute\_metrics} đã được định nghĩa trong \texttt{bert\_application.ipynb} để tính toán accuracy bằng cách so sánh các nhãn dự đoán (lấy bằng cách áp dụng \texttt{np.argmax} lên logits đầu ra) với các nhãn thực tế từ tập kiểm tra.[1] Các chỉ số khác như Precision, Recall, và F1-score cũng có thể được tính toán để có cái nhìn chi tiết hơn về hiệu suất, đặc biệt là trong trường hợp dữ liệu mất cân bằng hoặc khi các loại lỗi khác nhau có chi phí khác nhau.

    \item \textbf{Kết quả trên Tập Kiểm tra:}
    Sau khi huấn luyện trên tập \texttt{small\_train\_dataset} (200 mẫu) và đánh giá trên tập \texttt{small\_test\_dataset} (100 mẫu), mô hình BERT tinh chỉnh đạt được độ chính xác (\texttt{eval\_accuracy}) là 0.81 (tức 81\%).[1]

    \item \textbf{Ma trận Nhầm lẫn (Confusion Matrix):}
    Để hiểu rõ hơn về cách mô hình phân loại các mẫu, một ma trận nhầm lẫn đã được tạo ra từ các dự đoán của mô hình trên \texttt{small\_test\_dataset}. Ma trận này được trực quan hóa bằng \texttt{seaborn.heatmap} trong \texttt{bert\_application.ipynb}.[1]
    Giả sử ma trận nhầm lẫn thu được từ [1] có dạng (ví dụ, dựa trên accuracy 81% và 100 mẫu test, có thể là 40 TP, 41 TN, 9 FP, 10 FN - cần số liệu chính xác từ notebook nếu có):
    \begin{figure}[H]
        \centering
        % \includegraphics[width=0.6\textwidth]{path_to_confusion_matrix_image_from_notebook.png} % Placeholder
        \texttt{[1]}
        \caption{Ma trận Nhầm lẫn cho Phân loại Cảm xúc trên Tập Kiểm tra IMDB (100 mẫu) sử dụng BERT Tinh chỉnh. Trục hoành là "Dự đoán", trục tung là "Thực tế", với các nhãn "Tiêu cực" và "Tích cực".}
        \label{fig:confusion_matrix_bert_finetuned_rewrite}
    \end{figure}
    Phân tích ma trận nhầm lẫn (ví dụ dựa trên Hình \ref{fig:confusion_matrix_bert_finetuned_rewrite}) giúp xác định số lượng True Positives (TP - dự đoán đúng tích cực), True Negatives (TN - dự đoán đúng tiêu cực), False Positives (FP - dự đoán sai tích cực, thực tế là tiêu cực), và False Negatives (FN - dự đoán sai tiêu cực, thực tế là tích cực). Điều này cho phép đánh giá xem mô hình có xu hướng thiên vị dự đoán một lớp nào đó nhiều hơn hay không, hoặc loại lỗi nào phổ biến hơn. Ví dụ, nếu số lượng FN cao, điều đó có nghĩa là mô hình bỏ sót nhiều trường hợp tích cực.

    Bảng \ref{tab:imdb_results_finetuned_bert_rewrite} trình bày chi tiết hơn các kết quả đánh giá cho mô hình BERT tinh chỉnh. (Lưu ý: Các giá trị Precision, Recall, F1-score cần được tính toán từ ma trận nhầm lẫn của [1] nếu chưa có sẵn).

    \begin{table}[H]
        \centering
        \caption{Kết quả Phân loại Cảm xúc trên Tập Kiểm tra IMDB (100 mẫu) - Mô hình BERT Tinh chỉnh (dựa trên [1])}
        \label{tab:imdb_results_finetuned_bert_rewrite}
        \begin{tabular}{lc}
            \toprule
            \textbf{Metric} & \textbf{Giá trị} \\
            \midrule
            Accuracy & 0.8100 \\
            Precision (Positive) & (Tính từ Confusion Matrix) \\
            Recall (Positive) & (Tính từ Confusion Matrix) \\
            F1-score (Positive) & (Tính từ Confusion Matrix) \\
            Precision (Negative) & (Tính từ Confusion Matrix) \\
            Recall (Negative) & (Tính từ Confusion Matrix) \\
            F1-score (Negative) & (Tính từ Confusion Matrix) \\
            \midrule
            Macro avg F1-score & (Tính từ Precision/Recall các lớp) \\
            Weighted avg F1-score & (Tính từ Precision/Recall các lớp) \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{itemize}

\subsubsection{So sánh với các Mô hình Cơ sở}
\label{sssec:so_sanh_voi_mo_hinh_co_so_rewrite}
Để đánh giá đúng hơn hiệu suất của mô hình BERT tinh chỉnh, kết quả của nó đã được so sánh với hai phương pháp cơ sở cũng được triển khai và đánh giá trong \texttt{bert\_application.ipynb} [1], sử dụng cùng một tập con 500 mẫu từ tập huấn luyện IMDB để huấn luyện các mô hình cơ sở này và đánh giá trên một tập kiểm tra tương ứng.

\begin{itemize}
    \item \textbf{TF-IDF + Logistic Regression:} Đây là một phương pháp truyền thống trong NLP. Đặc trưng TF-IDF (Term Frequency-Inverse Document Frequency) được trích xuất từ văn bản (với \texttt{max\_features=1000} và loại bỏ stop words tiếng Anh). Các vector TF-IDF này sau đó được sử dụng để huấn luyện một mô hình phân loại Logistic Regression. Phương pháp này đạt độ chính xác 0.7533 trên tập kiểm tra của nó.[1] Việc đưa vào mô hình cơ sở này giúp định vị hiệu suất của BERT trong bối cảnh các kỹ thuật không dựa trên học sâu.

    \item \textbf{BERT Embeddings (từ \texttt{BertModel}) + Logistic Regression:} Trong phương pháp này, một mô hình \texttt{BertModel.from\_pretrained("bert-base-uncased")} (chỉ bao gồm phần encoder của BERT, không có lớp phân loại) được sử dụng như một công cụ trích xuất đặc trưng tĩnh. Cụ thể, vector biểu diễn của token \texttt{} từ lớp cuối cùng của BERT encoder được lấy làm embedding đại diện cho mỗi đoạn văn bản. Các embedding này sau đó được dùng để huấn luyện một mô hình Logistic Regression. Cách tiếp cận này đạt độ chính xác 0.7600.[1]
\end{itemize}
Bảng \ref{tab:comparison_baseline_models_rewrite} tóm tắt so sánh hiệu suất giữa mô hình BERT tinh chỉnh và hai mô hình cơ sở này.

\begin{table}[H]
    \centering
    \caption{So sánh Hiệu suất Phân loại Cảm xúc với các Mô hình Cơ sở (dựa trên [1])}
    \label{tab:comparison_baseline_models_rewrite}
    \begin{tabular}{lc}
        \toprule
        \textbf{Phương pháp} & \textbf{Accuracy} \\
        \midrule
        TF-IDF + Logistic Regression & 0.7533 \\
        BERT Embeddings (tĩnh) + Logistic Regression & 0.7600 \\
        \textbf{BERT Tinh chỉnh (Fine-tuned BERT)} & \textbf{0.8100} \\
        \bottomrule
    \end{tabular}
\end{table}
Kết quả so sánh này rất có ý nghĩa. Nó cho thấy mô hình BERT được tinh chỉnh toàn bộ (accuracy 0.81) vượt trội đáng kể so với cả phương pháp TF-IDF truyền thống và phương pháp sử dụng BERT chỉ như một công cụ trích xuất đặc trưng tĩnh. Sự cải thiện từ 0.76 (BERT embeddings + LR) lên 0.81 (Fine-tuned BERT) đặc biệt làm nổi bật lợi ích của việc cho phép gradient lan truyền ngược qua toàn bộ các lớp của mô hình BERT trong quá trình tinh chỉnh. Điều này cho phép các biểu diễn ngôn ngữ mạnh mẽ đã được tiền huấn luyện tự điều chỉnh để phù hợp hơn với các đặc điểm cụ thể của tác vụ phân loại cảm xúc, thay vì chỉ sử dụng chúng một cách cố định.

\subsubsection{Thảo luận Kết quả và Phân tích Chú ý}
\label{sssec:thao_luan_ket_qua_phan_tich_chu_y_rewrite}

\begin{itemize}
    \item \textbf{Phân tích Ưu điểm của BERT Tinh chỉnh:}
    Như đã thấy từ Bảng \ref{tab:comparison_baseline_models_rewrite}, mô hình BERT được tinh chỉnh mang lại hiệu suất cao nhất. Điều này là do nhiều yếu tố. Thứ nhất, BERT được tiền huấn luyện trên một lượng lớn dữ liệu, cho phép nó học được các biểu diễn ngữ nghĩa và ngữ cảnh phong phú mà các phương pháp như TF-IDF không thể nắm bắt. Thứ hai, kiến trúc Transformer với cơ chế tự chú ý cho phép BERT hiểu được các mối quan hệ phức tạp giữa các từ trong một câu, ngay cả khi chúng ở xa nhau. Quan trọng nhất, quá trình tinh chỉnh cho phép toàn bộ các tham số của mô hình BERT (không chỉ lớp phân loại cuối cùng) được điều chỉnh để tối ưu hóa cho tác vụ phân loại cảm xúc cụ thể. Điều này khác với việc sử dụng BERT embeddings tĩnh, nơi các biểu diễn từ BERT được giữ cố định. Sự thích ứng toàn diện này giải thích tại sao Fine-tuned BERT (0.81 accuracy) lại tốt hơn đáng kể so với BERT Embeddings + LR (0.76 accuracy).[1]

    \item \textbf{Phân tích Chú ý (Attention Visualization):}
    Giai đoạn D trong \texttt{bert\_application.ipynb} khám phá việc trực quan hóa trọng số chú ý của mô hình BERT.[1] Mặc dù các mô hình học sâu thường bị coi là "hộp đen", cơ chế chú ý trong Transformer cung cấp một cửa sổ để hiểu được phần nào của đầu vào mà mô hình tập trung vào khi đưa ra dự đoán. Ví dụ, khi phân loại một câu như "This movie is absolutely terrible", việc phân tích trọng số chú ý từ token \texttt{} (được sử dụng cho phân loại) đến các token khác trong câu có thể cho thấy rằng mô hình gán trọng số chú ý cao hơn cho các từ mang tính cảm xúc mạnh như "terrible" hoặc các từ bổ nghĩa như "absolutely".
    \begin{figure}[H]
        \centering
        % \includegraphics[width=0.8\textwidth]{path_to_attention_visualization_example.png} % Placeholder
        \texttt{[1]}
        \caption{Ví dụ Minh họa Trực quan hóa Trọng số Chú ý trong BERT. Các từ có trọng số chú ý cao hơn (ví dụ, được tô màu đậm hơn) cho thấy mô hình tập trung vào chúng khi đưa ra quyết định.}
        \label{fig:attention_visualization_example_rewrite}
    \end{figure}
    Phân tích này, dù mang tính định tính, giúp xây dựng sự tin tưởng và hiểu biết sâu hơn về hành vi của mô hình, phù hợp với mục tiêu "hiểu sâu" của đề án Hướng 2.[1]

    \item \textbf{Thách thức và Hạn chế:}
    Trong quá trình thực nghiệm, một số thách thức và hạn chế đã được ghi nhận:
    \begin{itemize}
        \item \textbf{Yêu cầu Tài nguyên Tính toán:} Việc tinh chỉnh các mô hình BERT, ngay cả phiên bản \texttt{bert-base}, cũng đòi hỏi tài nguyên tính toán đáng kể, bao gồm GPU có bộ nhớ đủ lớn (VRAM) và thời gian huấn luyện tương đối dài, ngay cả với tập dữ liệu con.[1] Việc sử dụng huấn luyện với độ chính xác hỗn hợp (\texttt{fp16}) có thể giúp giảm bớt phần nào gánh nặng này.[1]
        \item \textbf{Sự nhạy cảm với Siêu tham số:} Hiệu suất của BERT có thể khá nhạy cảm với việc lựa chọn các siêu tham số như tốc độ học, kích thước lô, số epoch huấn luyện. Việc tìm ra bộ siêu tham số tối ưu thường đòi hỏi nhiều thử nghiệm.[1]
        \item \textbf{Giới hạn của Tập dữ liệu Con:} Các kết quả chính được báo cáo ở đây (accuracy 0.81) dựa trên việc huấn luyện và đánh giá trên các tập dữ liệu con (200 mẫu huấn luyện, 100 mẫu kiểm tra) từ bộ IMDB.[1] Mặc dù điều này đủ để minh họa quy trình và so sánh tương đối giữa các phương pháp, hiệu suất trên toàn bộ bộ dữ liệu IMDB có thể khác (và thường cao hơn, như trong báo cáo gốc [1] đạt 0.9426 trên tập lớn hơn). Kết quả trên tập con nên được hiểu trong bối cảnh đó.
    \end{itemize}

    \item \textbf{Hướng Cải thiện Tiềm năng:}
    Dựa trên các kết quả và hạn chế, một số hướng cải thiện tiềm năng có thể được xem xét trong tương lai:
    \begin{itemize}
        \item \textbf{Sử dụng Mô hình Lớn hơn hoặc Hiệu quả hơn:} Thử nghiệm với các phiên bản BERT lớn hơn (ví dụ: \texttt{BERT-large}) hoặc các biến thể hiệu quả hơn như RoBERTa, ALBERT, DistilBERT có thể mang lại hiệu suất tốt hơn hoặc thời gian huấn luyện nhanh hơn.[1]
        \item \textbf{Tinh chỉnh Siêu tham số Kỹ lưỡng hơn:} Áp dụng các kỹ thuật tìm kiếm siêu tham số tự động (ví dụ: grid search, random search, Bayesian optimization) để tìm ra bộ siêu tham số tối ưu cho tác vụ.
        \item \textbf{Tăng cường Dữ liệu (Data Augmentation):} Đối với các bộ dữ liệu nhỏ, các kỹ thuật tăng cường dữ liệu như back-translation hoặc tạo các câu diễn giải (paraphrasing) có thể giúp cải thiện khả năng khái quát hóa của mô hình.[1]
        \item \textbf{Huấn luyện và Đánh giá trên Toàn bộ Dữ liệu:} Thực hiện huấn luyện và đánh giá trên toàn bộ tập dữ liệu IMDB để có được bức tranh chính xác hơn về hiệu suất của mô hình.
    \end{itemize}
\end{itemize}

Tóm lại, chương thực nghiệm này đã trình bày một cách tiếp cận toàn diện, từ việc tìm hiểu sâu kiến trúc BERT thông qua tự triển khai các thành phần cốt lõi, đến việc ứng dụng một mô hình BERT tiền huấn luyện mạnh mẽ để giải quyết bài toán phân loại cảm xúc. Các kết quả thu được không chỉ minh chứng hiệu quả của BERT mà còn cung cấp những hiểu biết quý báu về các khía cạnh thực tế của việc làm việc với các mô hình ngôn ngữ lớn.

%% =========================================================================
%% CHƯƠNG 6: KẾT QUẢ VÀ SO SÁNH
%% =========================================================================
\section{Kết quả và So sánh}
\label{sec:ket_qua_so_sanh}
Phần này trình bày kết quả của BERT trên các benchmark chuẩn từ bài báo gốc, cung cấp bối cảnh để hiểu tầm quan trọng của BERT trong lịch sử NLP.

\subsection{Kết quả trên GLUE Benchmark}
\label{ssec:ket_qua_glue}
GLUE (General Language Understanding Evaluation) là bộ benchmark gồm 9 tác vụ đa dạng, được thiết kế để đánh giá khả năng hiểu ngôn ngữ tổng quát của mô hình.

\subsubsection{Mô tả các tác vụ GLUE}
\begin{itemize}
    \item \textbf{MNLI:} Multi-Genre Natural Language Inference (393K mẫu)
    \item \textbf{QQP:} Quora Question Pairs - Xác định câu hỏi trùng lặp (364K mẫu)
    \item \textbf{QNLI:} Question NLI - Đoạn văn có chứa câu trả lời? (105K mẫu)
    \item \textbf{SST-2:} Stanford Sentiment Treebank - Phân loại cảm xúc (67K mẫu)
    \item \textbf{CoLA:} Corpus of Linguistic Acceptability (8.5K mẫu)
    \item \textbf{STS-B:} Semantic Textual Similarity (7K mẫu)
    \item \textbf{MRPC:} Microsoft Research Paraphrase Corpus (3.7K mẫu)
    \item \textbf{RTE:} Recognizing Textual Entailment (2.5K mẫu)
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Kết quả BERT trên GLUE test set. Điểm số cao nhất được in đậm.}
    \label{tab:glue_results_detailed}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccccc}
        \toprule
        \textbf{Model} & \textbf{MNLI-m/mm} & \textbf{QQP} & \textbf{QNLI} & \textbf{SST-2} & \textbf{CoLA} & \textbf{STS-B} & \textbf{MRPC} & \textbf{RTE} & \textbf{WNLI} & \textbf{Avg} \\
        & (Acc) & (F1/Acc) & (Acc) & (Acc) & (Mcc) & (Corr) & (F1/Acc) & (Acc) & (Acc) & \\
        \midrule
        Previous SOTA & 80.6/80.1 & 66.1/- & 82.3 & 93.2 & 35.0 & 81.0 & 86.0/- & 61.7 & 65.1 & 74.0 \\
        OpenAI GPT & 82.1/81.4 & 70.3/88.5 & 87.4 & 91.3 & 45.4 & 80.0 & 82.3/75.7 & 56.0 & 65.1 & 72.8 \\
        \midrule
        BERT\textsubscript{BASE} & 84.6/83.4 & 71.2/89.2 & 90.5 & 93.5 & 52.1 & 85.8 & 88.9/84.8 & 66.4 & 65.1 & 78.3 \\
        BERT\textsubscript{LARGE} & \textbf{86.7/85.9} & \textbf{72.1/89.3} & \textbf{92.7} & \textbf{94.9} & \textbf{60.5} & \textbf{86.5} & \textbf{89.3/85.4} & \textbf{70.1} & 65.1 & \textbf{80.5} \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsubsection{Phân tích kết quả GLUE}
\begin{itemize}
    \item BERT\textsubscript{LARGE} đạt 80.5 điểm trung bình, tăng 7.7 điểm tuyệt đối so với GPT
    \item Cải thiện lớn nhất: CoLA (+15.1), RTE (+14.1) - các tác vụ có ít dữ liệu
    \item Điều này cho thấy BERT học được representations tổng quát, transfer tốt cho small datasets
\end{itemize}

\subsection{Kết quả trên SQuAD}
\label{ssec:ket_qua_squad}

\subsubsection{SQuAD v1.1}
SQuAD v1.1 yêu cầu trích xuất câu trả lời từ đoạn văn cho câu hỏi.

\begin{table}[H]
    \centering
    \caption{Kết quả trên SQuAD v1.1 test set}
    \label{tab:squad_v1_results}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{EM} & \textbf{F1} \\
        \midrule
        Human Performance & 82.3 & 91.2 \\
        Previous best (ensemble) & 84.4 & 90.9 \\
        \midrule
        BERT\textsubscript{BASE} (single) & 80.8 & 88.5 \\
        BERT\textsubscript{LARGE} (single) & 84.1 & 90.9 \\
        BERT\textsubscript{LARGE} (ensemble) & \textbf{87.4} & \textbf{93.2} \\
        \bottomrule
    \end{tabular}
\end{table}

Điểm đáng chú ý: BERT ensemble vượt qua human performance!

\subsubsection{SQuAD v2.0}
SQuAD v2.0 khó hơn vì có thêm câu hỏi không có câu trả lời trong đoạn văn.

\begin{table}[H]
    \centering
    \caption{Kết quả trên SQuAD v2.0 test set}
    \label{tab:squad_v2_results}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{EM} & \textbf{F1} \\
        \midrule
        Human Performance & 86.8 & 89.5 \\
        Previous best & 72.3 & 74.8 \\
        \midrule
        BERT\textsubscript{LARGE} & \textbf{78.7} & \textbf{81.9} \\
        \bottomrule
    \end{tabular}
\end{table}

BERT cải thiện 7.1 F1 points - một bước nhảy lớn cho tác vụ khó này.

\subsection{Ablation Studies - Hiểu sâu về BERT}
\label{ssec:ablation_study}

\subsubsection{Tầm quan trọng của các thành phần}
\begin{table}[H]
    \centering
    \caption{Ablation study: Loại bỏ từng thành phần của BERT}
    \label{tab:ablation_components}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model} & \textbf{MNLI-m} & \textbf{QNLI} & \textbf{MRPC} & \textbf{SST-2} & \textbf{SQuAD} \\
        & (Acc) & (Acc) & (Acc) & (Acc) & (F1) \\
        \midrule
        BERT\textsubscript{BASE} & 84.4 & 88.4 & 86.7 & 92.7 & 88.5 \\
        \midrule
        No NSP & 83.9 & 84.9 & 86.5 & 92.6 & 87.9 \\
        No MLM (LTR only) & 82.1 & 84.3 & 77.5 & 92.1 & 77.8 \\
        \bottomrule
    \end{tabular}
\end{table}

Insights:
\begin{itemize}
    \item Bỏ NSP: Giảm nhẹ performance, đặc biệt trên QNLI (-3.5\%)
    \item Bỏ MLM (chỉ train left-to-right): Giảm mạnh, đặc biệt SQuAD (-10.7 F1)
    \item → Bidirectional training (MLM) quan trọng hơn NSP
\end{itemize}

\subsubsection{Effect của Model Size}
\begin{table}[H]
    \centering
    \caption{Ảnh hưởng của kích thước mô hình}
    \label{tab:model_size_effect}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Model} & \textbf{\#Layers} & \textbf{\#Heads} & \textbf{Hidden} & \textbf{Params} & \textbf{MNLI} & \textbf{MRPC} \\
        \midrule
        BERT-Small & 4 & 8 & 512 & 29M & 77.6 & 79.8 \\
        BERT-Medium & 8 & 8 & 512 & 42M & 80.8 & 82.7 \\
        BERT-Base & 12 & 12 & 768 & 110M & 84.4 & 86.7 \\
        BERT-Large & 24 & 16 & 1024 & 340M & 86.6 & 87.8 \\
        \bottomrule
    \end{tabular}
\end{table}

Larger models consistently better, even on small datasets (MRPC: 3.7K samples).

%% =========================================================================
%% CHƯƠNG 7: HẠN CHẾ VÀ HƯỚNG PHÁT TRIỂN
%% =========================================================================
\section{Hạn chế và Hướng Phát triển}
\label{sec:han_che_huong_phat_trien}

\subsection{Hạn chế của BERT}
\label{ssec:han_che_bert}
Mặc dù BERT đã tạo ra bước đột phá, mô hình vẫn có những hạn chế cần được nhận diện và giải quyết.

\subsubsection{Chi phí tính toán khổng lồ}
\begin{itemize}
    \item \textbf{Pre-training cost:}
    \begin{itemize}
        \item BERT\textsubscript{BASE}: 4 ngày trên 16 TPU chips
        \item BERT\textsubscript{LARGE}: 4 ngày trên 64 TPU chips
        \item Chi phí ước tính: \$7,000 - \$50,000 tùy cấu hình
    \end{itemize}
    \item \textbf{Environmental impact:} Carbon footprint tương đương 5 chuyến bay xuyên Mỹ
    \item \textbf{Inference cost:} BERT\textsubscript{LARGE} cần ~1GB memory, chậm cho real-time apps
\end{itemize}

\subsubsection{Masked Token Discrepancy}
\begin{itemize}
    \item [MASK] token chỉ xuất hiện trong pre-training, không có trong fine-tuning/inference
    \item Mặc dù có chiến lược 80-10-10, vẫn tồn tại domain shift
    \item Các mô hình sau (ELECTRA) giải quyết bằng cách không dùng [MASK]
\end{itemize}

\subsubsection{Fixed Length Limitation}
\begin{itemize}
    \item Maximum 512 tokens - không đủ cho văn bản dài (legal documents, books)
    \item Truncation làm mất thông tin quan trọng
    \item Solutions: Longformer (4096 tokens), BigBird (sparse attention)
\end{itemize}

\subsubsection{Unidirectional Fine-tuning for Generation}
\begin{itemize}
    \item BERT là encoder-only, không tự nhiên cho text generation
    \item Các hack (mask prediction iteratively) kém hiệu quả
    \item Dẫn đến phát triển encoder-decoder models (T5, BART)
\end{itemize}

\subsubsection{Static Embeddings}
\begin{itemize}
    \item Position embeddings cố định 512 vị trí
    \item Không thể extrapolate cho sequences dài hơn
    \item Relative position embeddings (T5) linh hoạt hơn
\end{itemize}

\subsubsection{NSP Task Controversy}
\begin{itemize}
    \item Nhiều nghiên cứu cho thấy NSP không cần thiết hoặc có hại
    \item Random sentences quá dễ phân biệt → task không học gì có ích
    \item RoBERTa bỏ NSP và đạt kết quả tốt hơn
\end{itemize}

\subsection{Các Hướng Phát triển Kế thừa BERT}
\label{ssec:huong_phat_trien_ke_thua}
BERT đã mở ra "Cambrian explosion" của các mô hình NLP. Các hướng phát triển chính:

\subsubsection{Cải tiến Training Efficiency}

\textbf{1. RoBERTa (Robustly Optimized BERT):}
\begin{itemize}
    \item Loại bỏ NSP, train lâu hơn (500K steps vs 100K)
    \item Dynamic masking thay vì static
    \item Larger batches (8K vs 256)
    \item Kết quả: +2-3\% trên hầu hết tasks
\end{itemize}

\textbf{2. ELECTRA (Efficiently Learning an Encoder):}
\begin{itemize}
    \item Replaced token detection thay vì MLM
    \item Train discriminator phân biệt real/fake tokens
    \item Hiệu quả hơn 4x về compute
    \item Small ELECTRA $\approx$ BERT\textsubscript{BASE} với 1/4 compute
\end{itemize}

\subsubsection{Model Compression}

\textbf{1. DistilBERT:}
\begin{itemize}
    \item Knowledge distillation từ BERT\textsubscript{BASE}
    \item 40\% smaller, 60\% faster, giữ 97\% performance
    \item Key: Distill during pre-training, not just fine-tuning
\end{itemize}

\textbf{2. ALBERT (A Lite BERT):}
\begin{itemize}
    \item Factorized embeddings: V × E → V × E' × H (E' << H)
    \item Cross-layer parameter sharing
    \item 18x fewer parameters với comparable performance
\end{itemize}

\textbf{3. Quantization \& Pruning:}
\begin{itemize}
    \item 8-bit quantization: 4x compression, <1\% performance drop
    \item Structured pruning: Remove attention heads/layers
    \item TensorRT optimization cho deployment
\end{itemize}

\subsubsection{Architectural Innovations}

\textbf{1. Handling Long Documents:}
\begin{itemize}
    \item \textbf{Longformer:} Sliding window + global attention
    \item \textbf{BigBird:} Sparse attention (random + window + global)
    \item \textbf{Linformer:} Linear complexity attention
\end{itemize}

\textbf{2. Unified Models:}
\begin{itemize}
    \item \textbf{T5:} "Text-to-Text" framework - mọi task là seq2seq
    \item \textbf{BART:} Denoising autoencoder cho generation
    \item \textbf{UniLM:} Unified pre-training cho cả understanding và generation
\end{itemize}

\subsubsection{Multilingual và Cross-lingual}

\textbf{1. mBERT → XLM → XLM-R:}
\begin{itemize}
    \item Progressive improvements trong multilingual understanding
    \item XLM-R: 100 languages, 2.5TB text
    \item Zero-shot cross-lingual transfer
\end{itemize}

\textbf{2. Language-Specific Models:}
\begin{itemize}
    \item PhoBERT (Vietnamese), CamemBERT (French), BERTje (Dutch)
    \item Often outperform mBERT on monolingual tasks
\end{itemize}

\subsubsection{Domain Adaptation}

\textbf{1. Continued Pre-training:}
\begin{itemize}
    \item BioBERT: PubMed papers → biomedical NER/RE
    \item SciBERT: Scientific papers → citation intent classification
    \item FinBERT: Financial texts → sentiment analysis
\end{itemize}

\textbf{2. Adapter Modules:}
\begin{itemize}
    \item Thêm small trainable modules, freeze BERT
    \item Efficient multi-domain adaptation
    \item Parameter efficient: ~3\% additional parameters per task
\end{itemize}

\subsubsection{Toward Foundation Models}
BERT's success paved the way for:
\begin{itemize}
    \item GPT-3: Scaling to 175B parameters
    \item PaLM: 540B parameters với breakthrough capabilities
    \item ChatGPT/GPT-4: Instruction following và alignment
\end{itemize}

%% =========================================================================
%% KẾT LUẬN
%% =========================================================================
\section{Kết luận}
\label{sec:ket_luan}
BERT - Bidirectional Encoder Representations from Transformers - không chỉ là một mô hình mà còn là một cột mốc quan trọng đánh dấu sự chuyển mình của lĩnh vực Xử lý Ngôn ngữ Tự nhiên. Bằng cách giới thiệu phương pháp học biểu diễn hai chiều sâu sắc thông qua Masked Language Modeling và Next Sentence Prediction, BERT đã chứng minh rằng việc pre-training trên dữ liệu không nhãn có thể tạo ra những biểu diễn ngôn ngữ mạnh mẽ, có khả năng transfer xuất sắc cho nhiều tác vụ downstream.

\textbf{Những đóng góp then chốt của BERT:}
\begin{itemize}
    \item \textbf{Deeply bidirectional architecture:} Lần đầu tiên cho phép mỗi từ được hiểu trong ngữ cảnh đầy đủ của cả câu, không chỉ một phía như các mô hình trước đó.
    
    \item \textbf{Pre-training objectives sáng tạo:} MLM và NSP tuy đơn giản nhưng hiệu quả, cho phép học từ dữ liệu không nhãn - nguồn tài nguyên dồi dào và dễ thu thập.
    
    \item \textbf{Transfer learning paradigm:} Thiết lập mô hình "pre-train once, fine-tune for everything" đã trở thành chuẩn mực cho ngành.
    
    \item \textbf{Empirical breakthroughs:} Không chỉ cải thiện incremental mà tạo ra những bước nhảy lớn trên nhiều benchmarks, thậm chí vượt qua human performance.
\end{itemize}

\textbf{Tác động sâu rộng:}

BERT đã mở ra kỷ nguyên mới của "Foundation Models" - những mô hình nền tảng được pre-train trên quy mô lớn rồi adapted cho các ứng dụng cụ thể. Từ BERT, chúng ta đã chứng kiến sự phát triển bùng nổ của:
\begin{itemize}
    \item Các biến thể hiệu quả hơn (RoBERTa, ELECTRA, ALBERT)
    \item Mô hình đa ngôn ngữ (XLM-R, mT5)
    \item Scaling lên quy mô khổng lồ (GPT-3, PaLM, ChatGPT)
    \item Ứng dụng thực tế trong mọi ngành (search, translation, chatbots, content generation)
\end{itemize}

\textbf{Bài học cho tương lai:}

Thành công của BERT dạy chúng ta rằng:
\begin{enumerate}
    \item Simple ideas executed well can be revolutionary (MLM là ý tưởng đơn giản nhưng hiệu quả)
    \item Scale matters - cả về dữ liệu và model capacity
    \item Transfer learning là chìa khóa cho AI dân chủ hóa
    \item Bidirectionality và context là cốt lõi của language understanding
\end{enumerate}

\textbf{Nhìn về phía trước:}

Trong khi BERT đã đặt nền móng vững chắc, vẫn còn nhiều thách thức:
\begin{itemize}
    \item Computational efficiency cho deployment rộng rãi
    \item Handling longer contexts và multimodal inputs
    \item Better alignment với human values và intentions
    \item Multilingual và low-resource language support
\end{itemize}

BERT sẽ được ghi nhớ không chỉ vì những con số ấn tượng trên benchmarks, mà vì đã thay đổi cách chúng ta nghĩ về language understanding và machine learning. Từ một bài báo học thuật, BERT đã trở thành nền tảng cho vô số ứng dụng AI đang thay đổi thế giới hàng ngày. Đó chính là di sản thực sự của một nghiên cứu đột phá.