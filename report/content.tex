% =========================================================================
%% ABSTRACT
%% =========================================================================
\begin{abstract}
    \hyperref[acro:bert]{\textbf{BERT}} (Bidirectional Encoder Representations from Transformers) đã tạo ra một cuộc cách mạng trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên bằng cách giới thiệu phương pháp học biểu diễn ngôn ngữ hai chiều sâu sắc. Báo cáo này phân tích chi tiết kiến trúc, cơ chế hoạt động và các đóng góp then chốt của \hyperref[acro:bert]{\textbf{BERT}}. Chúng tôi trình bày cơ sở toán học của các thành phần cốt lõi như cơ chế attention, kiến trúc Transformer, và hai tác vụ tiền huấn luyện đặc trưng: Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}}) và Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}}). Thông qua phân tích lý thuyết và thực nghiệm trên bài toán phân loại cảm xúc, báo cáo làm rõ lý do \hyperref[acro:bert]{\textbf{BERT}} đạt được hiệu suất vượt trội, thiết lập kỷ lục mới trên 11 tác vụ \hyperref[acro:nlp]{\texttt{NLP}} và mở ra kỷ nguyên mới cho các mô hình ngôn ngữ lớn.
    \end{abstract}
    
    % =========================================================================
    %% CHƯƠNG 1: GIỚI THIỆU CHUNG VỀ BERT
    %% =========================================================================
    \section{Giới Thiệu Chung về \hyperref[acro:bert]{\textbf{BERT}}}
    \label{sec:gioi_thieu_chung_bert}
    
    \subsection{Bối cảnh và Tầm quan trọng của Mô hình Biểu diễn Ngôn ngữ}
    \label{ssec:boi_canh_bieu_dien_ngon_ngu}
    
    Trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (\hyperref[acro:nlp]{\texttt{NLP}}), việc xây dựng các biểu diễn ngôn ngữ (language representations) hiệu quả luôn là thách thức trung tâm. Các biểu diễn này, thường ở dạng vector trong không gian nhiều chiều, cố gắng nắm bắt các đặc tính ngữ nghĩa và cú pháp phức tạp của ngôn ngữ tự nhiên.
    
    \subsubsection{Hạn chế của các phương pháp truyền thống}
    Trước khi \hyperref[acro:bert]{\textbf{BERT}} ra đời, các phương pháp biểu diễn ngôn ngữ phổ biến bao gồm các word embeddings tĩnh như Word2Vec \cite{mikolov2013distributed} và GloVe \cite{pennington2014glove}. Tuy nhiên, các phương pháp này gặp phải hạn chế cơ bản: \textbf{không thể nắm bắt ngữ cảnh động}.
    
    Để minh họa rõ hơn, xét ví dụ về từ đa nghĩa "bank":
    \begin{itemize}
        \item "I went to the \textbf{bank} to deposit money" (ngân hàng)
        \item "The river \textbf{bank} was flooded" (bờ sông)
    \end{itemize}
    
    Trong các mô hình embedding tĩnh, từ "bank" sẽ có cùng một vector biểu diễn trong cả hai câu, mặc dù ý nghĩa hoàn toàn khác nhau. Đây chính là hạn chế cố hữu của biểu diễn tĩnh - không thể điều chỉnh biểu diễn dựa trên ngữ cảnh cụ thể.
    
    \subsubsection{Sự phát triển của mô hình ngữ cảnh}
    Để khắc phục hạn chế này, các mô hình biểu diễn ngôn ngữ theo ngữ cảnh (contextual language representation models) đã được phát triển. Các mô hình dựa trên kiến trúc tuần tự như RNN và LSTM bắt đầu cho thấy khả năng nắm bắt thông tin ngữ cảnh. Tuy nhiên, chúng gặp phải vấn đề \textbf{information bottleneck} - thông tin phải được nén qua một vector trạng thái ẩn có kích thước cố định, dẫn đến mất mát thông tin nghiêm trọng khi xử lý chuỗi dài.
    
    \hyperref[acro:elmo]{\texttt{ELMo}} (Embeddings from Language Models) \cite{peters2018deep} đã cố gắng giải quyết vấn đề này bằng cách kết hợp biểu diễn từ cả hai chiều (forward và backward). Cụ thể, \hyperref[acro:elmo]{\texttt{ELMo}} huấn luyện hai LSTM độc lập và concatenate các biểu diễn. Tuy nhiên, đây chỉ là kết hợp "nông" (shallow fusion) - hai LSTM không thực sự tương tác với nhau trong quá trình học.
    
    \subsubsection{Cuộc cách mạng Transformer}
    Sự ra đời của kiến trúc Transformer \cite{vaswani2017attention} với cơ chế self-attention đã mang lại một bước đột phá quan trọng. Khác với RNN phải xử lý tuần tự từng token với độ phức tạp $O(n)$ cho mỗi bước, Transformer có thể xử lý toàn bộ chuỗi song song với độ phức tạp $O(n^2)$ cho toàn bộ chuỗi, cho phép mô hình hóa các phụ thuộc tầm xa một cách trực tiếp và hiệu quả hơn.
    
    OpenAI \hyperref[acro:gpt]{\texttt{GPT}} \cite{radford2018improving} đã tận dụng Transformer nhưng vẫn giữ hướng huấn luyện từ trái sang phải (left-to-right), giới hạn khả năng hiểu ngữ cảnh toàn diện. Đây chính là khoảng trống mà \hyperref[acro:bert]{\textbf{BERT}} hướng đến giải quyết.
    
    \subsection{Đột phá của \hyperref[acro:bert]{\textbf{BERT}} và Đóng góp chính}
    \label{ssec:muc_tieu_dong_gop_bert}
    
    \textbf{\hyperref[acro:bert]{\textbf{BERT}} (Bidirectional Encoder Representations from Transformers)} \cite{devlin2018bert}, được Google AI Language giới thiệu năm 2018, đã giải quyết triệt để vấn đề học một chiều của các mô hình trước đó. 
    
    \subsubsection{Đổi mới then chốt: Học hai chiều sâu sắc}
    \hyperref[acro:bert]{\textbf{BERT}} sử dụng một chiến lược đột phá: che ngẫu nhiên một số từ trong câu (masked language modeling) và yêu cầu mô hình dự đoán chúng dựa trên ngữ cảnh hai chiều. Điều này cho phép \hyperref[acro:bert]{\textbf{BERT}} học được biểu diễn \textbf{deeply bidirectional} - mỗi từ được hiểu dựa trên toàn bộ ngữ cảnh xung quanh, không chỉ một phía như các mô hình trước.
    
    \subsubsection{Các đóng góp chính của \hyperref[acro:bert]{\textbf{BERT}}}
    \begin{enumerate}
        \item \textbf{Kiến trúc hai chiều sâu sắc (Deeply Bidirectional):} Đây là điểm khác biệt quan trọng nhất. Trong khi \hyperref[acro:gpt]{\texttt{GPT}} chỉ nhìn ngữ cảnh một chiều và \hyperref[acro:elmo]{\texttt{ELMo}} kết hợp hai chiều một cách độc lập, \hyperref[acro:bert]{\textbf{BERT}} cho phép mỗi lớp của mô hình xem xét ngữ cảnh hai chiều đồng thời.
        
        \item \textbf{Hai tác vụ tiền huấn luyện sáng tạo:}
        \begin{itemize}
            \item \textbf{Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}}):} Che ngẫu nhiên 15\% các từ và dự đoán chúng từ ngữ cảnh
            \item \textbf{Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}}):} Dự đoán xem hai câu có liên tiếp trong văn bản gốc
        \end{itemize}
        
        \item \textbf{Hiệu suất vượt trội:} \hyperref[acro:bert]{\textbf{BERT}} đã thiết lập kỷ lục mới trên 11 tác vụ \hyperref[acro:nlp]{\texttt{NLP}} khác nhau. Trên \hyperref[acro:glue]{\texttt{GLUE}} benchmark, BERT\textsubscript{LARGE} đạt điểm trung bình 80.5\%, tăng 7.7 điểm tuyệt đối so với OpenAI \hyperref[acro:gpt]{\texttt{GPT}} (72.8\%) \cite{devlin2018bert}.
        
        \item \textbf{Paradigm "Pre-train once, fine-tune for everything":} \hyperref[acro:bert]{\textbf{BERT}} thiết lập mô hình transfer learning hiệu quả cho \hyperref[acro:nlp]{\texttt{NLP}}, cho phép fine-tune nhanh chóng cho các tác vụ downstream với chi phí tính toán thấp.
    \end{enumerate}
    
    Sự thành công của \hyperref[acro:bert]{\textbf{BERT}} không chỉ nằm ở hiệu suất vượt trội mà còn ở việc nó mở ra một hướng nghiên cứu mới: sử dụng mô hình ngôn ngữ lớn được tiền huấn luyện làm nền tảng cho mọi tác vụ \hyperref[acro:nlp]{\texttt{NLP}}.
    
    %% =========================================================================
    %% CHƯƠNG 2: KIẾN TRÚC BERT VÀ CÁC THÀNH PHẦN CỐT LÕI
    %% =========================================================================
    \section{Kiến trúc \hyperref[acro:bert]{\textbf{BERT}} và các Thành phần Cốt lõi}
    \label{sec:kien_truc_bert}
    
    Kiến trúc của \hyperref[acro:bert]{\textbf{BERT}} dựa trên phần encoder của kiến trúc Transformer \cite{vaswani2017attention}. Trong phần này, chúng tôi sẽ phân tích chi tiết từng thành phần cốt lõi và cơ sở toán học của chúng.
    
    \subsection{Tổng quan về Kiến trúc Transformer}
    \label{ssec:tong_quan_transformer}
    
    Transformer là một kiến trúc mạng nơ-ron được giới thiệu bởi Vaswani et al. \cite{vaswani2017attention} với mục tiêu ban đầu là cải thiện các mô hình dịch máy. Điểm đột phá của Transformer là loại bỏ hoàn toàn các cấu trúc tuần tự (RNN, LSTM) và thay thế bằng cơ chế attention, cho phép xử lý song song toàn bộ chuỗi.
    
    \subsubsection{Cấu trúc tổng thể}
    Kiến trúc Transformer gồm hai phần chính:
    \begin{itemize}
        \item \textbf{Encoder:} Xử lý chuỗi đầu vào và tạo ra biểu diễn ngữ cảnh
        \item \textbf{Decoder:} Sinh ra chuỗi đầu ra dựa trên biểu diễn từ encoder
    \end{itemize}
    
    \hyperref[acro:bert]{\textbf{BERT}} chỉ sử dụng phần Encoder vì mục tiêu là tạo biểu diễn ngôn ngữ, không phải sinh văn bản. Mỗi khối encoder trong \hyperref[acro:bert]{\textbf{BERT}} bao gồm hai thành phần chính:
    
    \begin{enumerate}
        \item \textbf{Multi-Head Self-Attention Layer:} Cho phép mô hình "chú ý" đến các vị trí khác nhau trong chuỗi khi xử lý mỗi từ
        \item \textbf{Position-wise Feed-Forward Network:} Xử lý độc lập từng vị trí với cùng một mạng nơ-ron
    \end{enumerate}
    
    Mỗi lớp con đều có kết nối dư (residual connection) và chuẩn hóa lớp (layer normalization) \cite{ba2016layer}, giúp huấn luyện ổn định các mạng nơ-ron sâu.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{img/transformer_arch.png}
        \caption{Kiến trúc tổng quan của mô hình Transformer \cite{vaswani2017attention}. \hyperref[acro:bert]{\textbf{BERT}} chỉ sử dụng phần Encoder (bên trái). Mỗi khối encoder chứa lớp Multi-Head Attention và Feed-Forward, được kết nối bằng residual connections và layer normalization.}
        \label{fig:transformer_architecture}
    \end{figure}
    
    \subsection{Cơ chế Self-Attention: Nền tảng toán học}
    \label{ssec:self_attention}
    
    Self-attention là cơ chế cho phép mỗi từ trong chuỗi "nhìn" và tương tác với mọi từ khác để xây dựng biểu diễn của mình. Đây là sự khác biệt cơ bản so với RNN chỉ có thể xem thông tin một cách tuần tự.
    
    \subsubsection{Công thức toán học của Scaled Dot-Product Attention}
    Với đầu vào là các vector biểu diễn, self-attention tính toán ba ma trận chiếu:
    \begin{itemize}
        \item \textbf{Query (Q):} Biểu diễn "câu hỏi" - thông tin mà token hiện tại đang tìm kiếm
        \item \textbf{Key (K):} Biểu diễn "khóa" - thông tin mà mỗi token có thể cung cấp
        \item \textbf{Value (V):} Biểu diễn "giá trị" - thông tin thực sự được truyền đi
    \end{itemize}
    
    Công thức Scaled Dot-Product Attention \cite{vaswani2017attention}:
    \begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \label{eq:scaled_attention}
    \end{equation}
    
    Trong đó:
    \begin{itemize}
        \item $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{m \times d_k}$, $V \in \mathbb{R}^{m \times d_v}$
        \item $n$ là số lượng queries, $m$ là số lượng keys/values
        \item $QK^T \in \mathbb{R}^{n \times m}$: Ma trận attention scores, tính điểm tương đồng giữa queries và keys
        \item $\sqrt{d_k}$: Hệ số chuẩn hóa để tránh gradient vanishing khi $d_k$ lớn. Khi $d_k$ lớn, phép nhân $QK^T$ có thể cho giá trị rất lớn, khiến softmax bão hòa
        \item $\text{softmax}$: Chuyển điểm số thành xác suất (tổng theo chiều keys = 1)
    \end{itemize}
    
    \subsubsection{Ý nghĩa và lợi ích}
    Cơ chế attention có độ phức tạp $O(n^2 \cdot d)$ so với RNN có độ phức tạp $O(n \cdot d^2)$. Khi $d > n$ (thường xảy ra trong NLP), attention hiệu quả hơn về mặt tính toán. Quan trọng hơn, attention cho phép mô hình hóa quan hệ giữa mọi cặp token trực tiếp, không phụ thuộc vào khoảng cách trong chuỗi.
    
    \subsection{Multi-Head Attention: Sức mạnh từ sự đa dạng}
    \label{ssec:multi_head_attention}
    
    Một cơ chế attention duy nhất chỉ có thể nắm bắt một loại mối quan hệ. Multi-Head Attention giải quyết hạn chế này bằng cách chạy nhiều attention song song, mỗi cái tập trung vào khía cạnh khác nhau.
    
    \subsubsection{Công thức toán học}
    \begin{equation}
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
    \label{eq:multihead}
    \end{equation}
    
    Trong đó:
    \begin{equation}
    \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
    \label{eq:head_i}
    \end{equation}
    
    Với:
    \begin{itemize}
        \item $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$: Ma trận chiếu cho head thứ $i$
        \item $W^O \in \mathbb{R}^{hd_v \times d_{model}}$: Ma trận chiếu đầu ra
        \item Thông thường: $d_k = d_v = d_{model}/h$
    \end{itemize}
    
    BERT\textsubscript{BASE} sử dụng $h=12$ heads với $d_{model}=768$, nên mỗi head có kích thước $d_k=64$.
    
    \subsubsection{Ý nghĩa của Multi-Head}
    Mỗi head có thể học một "góc nhìn" khác về dữ liệu:
    \begin{itemize}
        \item Head 1: Quan hệ cú pháp (chủ ngữ - vị ngữ)
        \item Head 2: Quan hệ ngữ nghĩa (từ đồng nghĩa)
        \item Head 3: Quan hệ vị trí (từ gần nhau)
        \item ...
    \end{itemize}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{img/multihead.png}
        \caption{Cơ chế Multi-Head Attention \cite{vaswani2017attention}. Đầu vào được chiếu thành $h$ bộ Q, K, V độc lập. Mỗi head thực hiện attention riêng, sau đó các kết quả được nối lại và chiếu qua $W^O$ để tạo đầu ra cuối cùng.}
        \label{fig:multi_head_attention}
    \end{figure}
    
    \subsection{Position-wise Feed-Forward Networks}
    \label{ssec:feed_forward_networks}
    
    Sau khi tổng hợp thông tin qua attention, mỗi vị trí được xử lý qua một mạng \hyperref[acro:ffn]{\texttt{FFN}} giống nhau:
    
    \begin{equation}
    \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
    \label{eq:ffn}
    \end{equation}
    
    Trong đó:
    \begin{itemize}
        \item $W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$, $b_1 \in \mathbb{R}^{d_{ff}}$
        \item $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$, $b_2 \in \mathbb{R}^{d_{model}}$
        \item $d_{ff}$ thường lớn hơn $d_{model}$ (trong BERT-base: $d_{ff}=3072$, $d_{model}=768$)
    \end{itemize}
    
    \hyperref[acro:ffn]{\texttt{FFN}} đóng vai trò như bộ xử lý đặc trưng cục bộ sau khi đã có ngữ cảnh toàn cục từ attention. Việc mở rộng sang không gian $d_{ff}$ lớn hơn cho phép mô hình học các biến đổi phi tuyến phức tạp hơn.
    
    \subsection{Residual Connections và Layer Normalization}
    \label{ssec:residual_layer_norm}
    
    Mỗi sublayer (attention hoặc \hyperref[acro:ffn]{\texttt{FFN}}) được bọc bởi:
    
    \begin{equation}
    \text{LayerNorm}(x + \text{Sublayer}(x))
    \label{eq:residual_layernorm}
    \end{equation}
    
    Layer Normalization được tính như sau:
    \begin{equation}
    \text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta
    \label{eq:layernorm}
    \end{equation}
    
    Trong đó $\mu$ và $\sigma$ là mean và standard deviation tính theo chiều features, $\gamma$ và $\beta$ là các tham số học được.
    
    Lợi ích:
    \begin{itemize}
        \item \textbf{Residual connection ($x +$):} Cho phép gradient flow trực tiếp qua mạng sâu, giảm thiểu vanishing gradient
        \item \textbf{Layer normalization:} Ổn định quá trình học bằng cách chuẩn hóa activations
    \end{itemize}
    
    \subsection{Biểu diễn Đầu vào của \hyperref[acro:bert]{\textbf{BERT}}}
    \label{ssec:input_representation_bert}
    
    \hyperref[acro:bert]{\textbf{BERT}} xây dựng biểu diễn đầu vào bằng cách cộng ba loại embedding:
    
    \begin{equation}
    \text{Input}_i = \text{TokenEmb}_i + \text{SegmentEmb}_i + \text{PositionEmb}_i
    \label{eq:input_embedding}
    \end{equation}
    
    \subsubsection{Token Embeddings}
    \begin{itemize}
        \item Sử dụng WordPiece tokenization \cite{wu2016google} với vocabulary $\approx$ 30,000 tokens
        \item Ví dụ: "playing" → ["play", "\#\#ing"]
        \item Giúp xử lý từ hiếm và từ ngoài từ vựng (OOV)
    \end{itemize}
    
    \subsubsection{Segment Embeddings}  
    \begin{itemize}
        \item Phân biệt câu A và câu B trong các tác vụ cặp câu
        \item Chỉ có 2 giá trị: $E_A$ cho câu đầu, $E_B$ cho câu thứ hai
    \end{itemize}
    
    \subsubsection{Position Embeddings}
    \begin{itemize}
        \item Transformer không có cấu trúc tuần tự nên cần thông tin vị trí explicit
        \item \hyperref[acro:bert]{\textbf{BERT}} học position embeddings (khác với Transformer gốc dùng sinusoidal encoding)
        \item Giới hạn 512 positions (có thể mở rộng khi cần)
    \end{itemize}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{img/input_representation.png}
        \caption{Biểu diễn đầu vào của \hyperref[acro:bert]{\textbf{BERT}} \cite{devlin2018bert}. Ba loại embeddings được cộng theo từng vị trí để tạo input embedding cuối cùng. Token [CLS] ở đầu sẽ được dùng cho các tác vụ phân loại cấp câu.}
        \label{fig:bert_input_representation}
    \end{figure}
    
    \subsection{Các phiên bản \hyperref[acro:bert]{\textbf{BERT}}}
    \label{ssec:bert_versions}
    
    Google phát hành hai phiên bản chính với sự đánh đổi giữa hiệu suất và chi phí tính toán:
    
    \begin{table}[H]
    \centering
    \caption{So sánh các phiên bản \hyperref[acro:bert]{\textbf{BERT}}}
    \label{tab:bert_versions}
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Phiên bản} & \textbf{Layers (L)} & \textbf{Hidden (H)} & \textbf{Heads (A)} & \textbf{Parameters} & \textbf{Training Time} \\
    \midrule
    BERT\textsubscript{BASE} & 12 & 768 & 12 & 110M & 4 days on 16 \hyperref[acro:tpu]{\texttt{TPU}}s \\
    BERT\textsubscript{LARGE} & 24 & 1024 & 16 & 340M & 4 days on 64 \hyperref[acro:tpu]{\texttt{TPU}}s \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    Sự khác biệt không chỉ về kích thước:
    \begin{itemize}
        \item BERT\textsubscript{LARGE} có capacity lớn hơn, phù hợp cho tác vụ phức tạp
        \item BERT\textsubscript{BASE} hiệu quả hơn về tài nguyên, phù hợp cho deployment
        \item Cả hai đều vượt trội so với các mô hình trước đó trên hầu hết các benchmark
    \end{itemize}
    
    %% =========================================================================
    %% CHƯƠNG 3: CÁC TÁC VỤ TIỀN HUẤN LUYỆN CỦA BERT
    %% =========================================================================
    \section{Các Tác vụ Tiền huấn luyện của \hyperref[acro:bert]{\textbf{BERT}}}
    \label{sec:pre_training_tasks}
    
    \hyperref[acro:bert]{\textbf{BERT}} được tiền huấn luyện đồng thời trên hai tác vụ tự giám sát (self-supervised): Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}}) và Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}}). Các tác vụ này cho phép \hyperref[acro:bert]{\textbf{BERT}} học từ dữ liệu văn bản không cần gán nhãn thủ công, tận dụng nguồn dữ liệu khổng lồ có sẵn.
    
    \subsection{Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}}): Đột phá cho học hai chiều}
    \label{ssec:mlm}
    
    \subsubsection{Vấn đề cốt lõi và giải pháp}
    Các mô hình ngôn ngữ truyền thống chỉ có thể học một chiều vì một lý do đơn giản nhưng cơ bản: nếu cho phép mô hình "nhìn" cả hai chiều khi dự đoán từ tiếp theo, nó sẽ "gian lận" bằng cách nhìn trực tiếp vào từ cần dự đoán. \hyperref[acro:bert]{\textbf{BERT}} giải quyết vấn đề này một cách thông minh: che ngẫu nhiên một số từ và yêu cầu mô hình dự đoán chúng từ ngữ cảnh hai chiều \cite{devlin2018bert}.
    
    \subsubsection{Chiến lược Masking 80-10-10}
    \hyperref[acro:bert]{\textbf{BERT}} che 15\% tokens trong mỗi chuỗi với chiến lược phức tạp:
    
    \begin{itemize}
        \item \textbf{80\% trường hợp → [MASK]:} Token bị thay thế bằng token đặc biệt [MASK]
        \item \textbf{10\% trường hợp → Random token:} Token bị thay thế bằng một token ngẫu nhiên khác
        \item \textbf{10\% trường hợp → Giữ nguyên:} Token không bị thay đổi
    \end{itemize}
    
    \subsubsection{Lý do của chiến lược 80-10-10}
    Chiến lược này được thiết kế cẩn thận để giải quyết vấn đề mismatch giữa pre-training và fine-tuning:
    
    \begin{enumerate}
        \item \textbf{Vấn đề [MASK] token:} Trong fine-tuning và inference, không có token [MASK]. Nếu chỉ train với [MASK], mô hình sẽ overfit vào việc xử lý [MASK].
        
        \item \textbf{Random replacement (10\%):} Buộc mô hình phải học biểu diễn tốt cho MỌI token, không chỉ [MASK]. Mô hình phải "nghi ngờ" mọi từ có thể bị thay thế.
        
        \item \textbf{Keep unchanged (10\%):} Đảm bảo mô hình cũng học cách biểu diễn các từ không bị thay đổi, tạo cân bằng trong học tập.
    \end{enumerate}
    
    \subsubsection{Hàm loss cho \hyperref[acro:mlm]{\texttt{MLM}}}
    Loss chỉ được tính cho các vị trí bị mask:
    
    \begin{equation}
    \mathcal{L}_{MLM} = -\sum_{i \in \text{masked}} \log P(w_i | \text{context})
    \label{eq:mlm_loss}
    \end{equation}
    
    Trong đó $P(w_i | \text{context})$ là xác suất dự đoán đúng từ gốc tại vị trí $i$ dựa trên ngữ cảnh hai chiều.
    
    \subsection{Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}}): Học quan hệ giữa câu}
    \label{ssec:nsp}
    
    \subsubsection{Động lực}
    Nhiều tác vụ \hyperref[acro:nlp]{\texttt{NLP}} quan trọng yêu cầu hiểu mối quan hệ giữa các câu:
    \begin{itemize}
        \item \textbf{Question Answering:} Câu hỏi và đoạn văn có liên quan?
        \item \textbf{Natural Language Inference:} Câu A kéo theo câu B?
        \item \textbf{Paraphrasing:} Hai câu có cùng ý nghĩa?
    \end{itemize}
    
    \hyperref[acro:mlm]{\texttt{MLM}} chỉ học quan hệ trong câu, \hyperref[acro:nsp]{\texttt{NSP}} bổ sung khả năng học quan hệ giữa các câu.
    
    \subsubsection{Cách thức hoạt động}
    \begin{enumerate}
        \item \textbf{Tạo dữ liệu huấn luyện:}
        \begin{itemize}
            \item 50\% mẫu: Câu B thực sự theo sau câu A trong văn bản (label: IsNext)
            \item 50\% mẫu: Câu B là câu ngẫu nhiên từ corpus (label: NotNext)
        \end{itemize}
        
        \item \textbf{Prediction:} Sử dụng biểu diễn cuối cùng của [CLS] token:
        \begin{equation}
        P(\text{IsNext}) = \text{softmax}(W_{NSP} \cdot h_{[CLS]} + b_{NSP})
        \label{eq:nsp_prediction}
        \end{equation}
    \end{enumerate}
    
    \subsubsection{Combined Loss}
    \hyperref[acro:bert]{\textbf{BERT}} được train với combined loss:
    \begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP}
    \label{eq:combined_loss}
    \end{equation}
    
    Cả hai loss được tối ưu đồng thời trong quá trình pre-training.
    
    \subsubsection{Tranh cãi về \hyperref[acro:nsp]{\texttt{NSP}}}
    Các nghiên cứu sau như \hyperref[acro:roberta]{\texttt{RoBERTa}} \cite{liu2019roberta} cho thấy \hyperref[acro:nsp]{\texttt{NSP}} có thể không cần thiết. Vấn đề chính là negative samples (random sentences) quá dễ phân biệt. \hyperref[acro:albert]{\texttt{ALBERT}} \cite{lan2019albert} đề xuất Sentence Order Prediction (\hyperref[acro:sop]{\texttt{SOP}}) như một cải tiến. Tuy nhiên, trong thiết kế gốc, \hyperref[acro:nsp]{\texttt{NSP}} vẫn đóng góp vào thành công của \hyperref[acro:bert]{\textbf{BERT}} trên nhiều benchmark.
    
    %% =========================================================================
    %% CHƯƠNG 4: TINH CHỈNH (FINE-TUNING) BERT
    %% =========================================================================
    \section{Fine-tuning \hyperref[acro:bert]{\textbf{BERT}} cho các tác vụ Cụ thể}
    \label{sec:fine_tuning_bert}
    
    Sau khi được tiền huấn luyện trên khối lượng dữ liệu khổng lồ (BookCorpus + Wikipedia $\approx$ 3.3 tỷ từ), \hyperref[acro:bert]{\textbf{BERT}} có thể được fine-tune nhanh chóng cho các tác vụ cụ thể. Đây là minh chứng cho sức mạnh của transfer learning trong \hyperref[acro:nlp]{\texttt{NLP}}.
    
    \subsection{Quy trình Fine-tuning Chung}
    \label{ssec:quy_trinh_tinh_chinh}
    
    \subsubsection{Nguyên lý cơ bản}
    Fine-tuning \hyperref[acro:bert]{\textbf{BERT}} tuân theo triết lý "minimal architectural changes" \cite{devlin2018bert}:
    \begin{itemize}
        \item Giữ nguyên toàn bộ kiến trúc \hyperref[acro:bert]{\textbf{BERT}} đã pre-train
        \item Chỉ thêm một task-specific head (thường là 1-2 layers)
        \item Train toàn bộ parameters (\hyperref[acro:bert]{\textbf{BERT}} + head) trên dữ liệu của tác vụ
    \end{itemize}
    
    \subsubsection{So sánh Fine-tuning vs Feature-based Approach}
    \begin{table}[H]
    \centering
    \caption{So sánh hai phương pháp sử dụng pre-trained models}
    \label{tab:finetune_vs_feature}
    \begin{tabular}{lll}
    \toprule
    \textbf{Tiêu chí} & \textbf{Fine-tuning (\hyperref[acro:bert]{\textbf{BERT}})} & \textbf{Feature-based (\hyperref[acro:elmo]{\texttt{ELMo}})} \\
    \midrule
    Parameters updated & Toàn bộ & Chỉ task-specific layers \\
    Flexibility & Cao (adapt cho từng task) & Thấp (fixed features) \\
    Training time & Lâu hơn & Nhanh hơn \\
    Performance & Tốt nhất & Tốt \\
    Memory requirement & Cao & Thấp hơn \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    \subsubsection{Lợi thế của Fine-tuning}
    \begin{enumerate}
        \item \textbf{Task-specific adaptation:} Toàn bộ mô hình được điều chỉnh cho tác vụ cụ thể
        \item \textbf{End-to-end optimization:} Gradient flow từ output layer đến input embeddings
        \item \textbf{Superior performance:} Thường đạt kết quả tốt hơn đáng kể so với feature-based
    \end{enumerate}
    
    \subsection{Fine-tuning cho các Tác vụ Phổ biến}
    \label{ssec:vi_du_tinh_chinh}
    
    \subsubsection{Sentence Pair Classification}
    \textbf{Ứng dụng:} Natural Language Inference, Paraphrase Detection, Semantic Similarity
    
    \textbf{Input format:}
    \begin{verbatim}
    [CLS] Sentence A [SEP] Sentence B [SEP]
    \end{verbatim}
    
    \textbf{Architecture:}
    \begin{itemize}
        \item Lấy final hidden state của [CLS]: $h_{[CLS]} \in \mathbb{R}^H$
        \item Classification layer: $W \in \mathbb{R}^{K \times H}$ (K = số classes)
        \item Output: $P(y|x) = \text{softmax}(h_{[CLS]} \cdot W^T)$
    \end{itemize}
    
    \subsubsection{Single Sentence Classification}  
    \textbf{Ứng dụng:} Sentiment Analysis, Topic Classification, Spam Detection
    
    \textbf{Architecture:} Tương tự sentence pair nhưng chỉ có một câu đầu vào
    
    \subsubsection{Question Answering}
    \textbf{Ứng dụng:} Extract answer span from context (SQuAD style)
    
    \textbf{Architecture đặc biệt:}
    \begin{itemize}
        \item Học 2 vectors: Start vector $S \in \mathbb{R}^H$ và End vector $E \in \mathbb{R}^H$
        \item Với mỗi token $i$ trong context:
        \begin{equation}
        P_{start}(i) = \frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}, \quad P_{end}(i) = \frac{e^{E \cdot T_i}}{\sum_j e^{E \cdot T_j}}
        \label{eq:qa_probability}
        \end{equation}
        \item Answer = span từ $\arg\max_i P_{start}(i)$ đến $\arg\max_j P_{end}(j)$ với $j \geq i$
    \end{itemize}
    
    \subsubsection{Token Classification (NER)}
    \textbf{Ứng dụng:} Named Entity Recognition, Part-of-Speech Tagging
    
    \textbf{Architecture:}
    \begin{itemize}
        \item Mỗi token output $T_i$ qua classification layer riêng
        \item $P(y_i|x) = \text{softmax}(T_i \cdot W^T)$
    \end{itemize}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{img/pretraining_finetuning.png}
        \caption{Fine-tuning \hyperref[acro:bert]{\textbf{BERT}} cho các tác vụ khác nhau \cite{devlin2018bert}. Architecture thay đổi tối thiểu - chủ yếu là output layer phù hợp với từng loại tác vụ.}
        \label{fig:bert_finetuning_tasks}
    \end{figure}
    
    \subsubsection{Hyperparameters được khuyến nghị}
    Từ bài báo gốc \cite{devlin2018bert}, các hyperparameters được khuyến nghị:
    \begin{itemize}
        \item \textbf{Batch size:} 16, 32
        \item \textbf{Learning rate:} 2e-5, 3e-5, 5e-5 
        \item \textbf{Epochs:} 2, 3, 4
        \item \textbf{Warmup:} 10\% of total steps
        \item \textbf{Dropout:} 0.1 (giữ nguyên từ pre-training)
    \end{itemize}
    
    Lưu ý: Datasets nhỏ nhạy cảm hơn với hyperparameters. Nên thử nhiều random seeds và chọn model tốt nhất trên validation set.
    
    %% =========================================================================
    %% CHƯƠNG 5: THỰC NGHIỆM VỚI BERT
    %% =========================================================================
    \section{Thực nghiệm \hyperref[acro:bert]{\textbf{BERT}}}
    \label{sec:thuc_nghiem_bert}
    
    \subsection{Giới thiệu}
    \label{ssec:gioi_thieu_thuc_nghiem}
    
    \subsubsection{Mục tiêu thực nghiệm}
    Trình bày một chuỗi thực nghiệm toàn diện về mô hình \hyperref[acro:bert]{\textbf{BERT}}, được thiết kế để kết nối giữa lý thuyết và ứng dụng. Mục tiêu được thực hiện thông qua hai phương pháp tiếp cận bổ sung cho nhau:
    
    \begin{enumerate}
        \item \textbf{Xây dựng từ nền tảng (From-scratch Implementation):} 
        \begin{itemize}
            \item Triển khai các thành phần cốt lõi của \hyperref[acro:bert]{\textbf{BERT}} trong file \texttt{bert.py} để hiểu sâu về kiến trúc và cơ chế hoạt động nội tại của mô hình.
            \item Minh họa và xác minh tính đúng đắn của cài đặt thông qua các kiểm thử trên hai tác vụ tiền huấn luyện là Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}}) và Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}}).
        \end{itemize}
        \item \textbf{Ứng dụng (sử dụng pre-trained models):} 
        \begin{itemize}
            \item Sử dụng mô hình \hyperref[acro:bert]{\textbf{BERT}} pre-trained từ thư viện Hugging Face để giải quyết bài toán thực tế: phân loại cảm xúc trên bộ dữ liệu IMDb Movie Reviews \cite{maas2011learning}.
            \item Xây dựng một pipeline hoàn chỉnh từ tiền xử lý, huấn luyện đến đánh giá, so sánh hiệu năng của \hyperref[acro:bert]{\textbf{BERT}} với một phương pháp baseline truyền thống (\hyperref[acro:tfidf]{\texttt{TF-IDF}} + \hyperref[acro:lr]{\texttt{LR}}).
            \item Phân tích sâu hơn về kết quả, trực quan hóa biểu diễn và cơ chế attention để làm rõ điểm mạnh của mô hình.
            \item Toàn bộ quá trình được trình bày trong file \texttt{bert\_application.ipynb}.
        \end{itemize}
    \end{enumerate}
    
    \subsubsection{Thiết kế Thực nghiệm}
    Thực nghiệm được thiết kế theo nguyên tắc \textit{progressive complexity} - bắt đầu từ việc kiểm tra các khối xây dựng cơ bản, sau đó tiến tới ứng dụng và phân tích một mô hình hoàn chỉnh trên bài toán phức tạp hơn.
    
    \begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2.2cm,
        block/.style={rectangle, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=3.5em},
        line/.style={draw, -latex'}]  
        
        \node [block] (impl) {Triển khai \& Xác minh\\(\texttt{bert.py})};
        \node [block, right of=impl, node distance=6.5cm] (app) {Ứng dụng\\Pre-trained};
        \node [block, below of=app, node distance=3.5cm] (finetune) {Fine-tuning\\trên IMDb};
        \node [block, left of=finetune, node distance=6.5cm] (evaluate) {Đánh giá \&\\So sánh};
        
        \path [line] (impl) -- node[above] {Hiểu lý thuyết} (app);
        \path [line] (app) -- (finetune);
        \path [line] (finetune) -- (evaluate);
        \path [line, dashed] (evaluate) -- node[left] {Phân tích sâu} (impl);
    \end{tikzpicture}
    \caption{Quy trình thực nghiệm từ triển khai, xác minh đến ứng dụng và phân tích sâu}
    \label{fig:experiment_workflow}
    \end{figure}
    
    \subsection{Kiểm thử các thành phần cốt lõi của \hyperref[acro:bert]{\textbf{BERT}} (From-scratch)}
    \label{ssec:kiem_thu_cot_loi}
    Để xác minh việc cài đặt lại kiến trúc \hyperref[acro:bert]{\textbf{BERT}} trong file \texttt{bert.py} là chính xác, chúng tôi đã thực hiện các kiểm thử nhỏ cho hai tác vụ tiền huấn luyện. Một cấu hình \hyperref[acro:bert]{\textbf{BERT}} thu nhỏ đã được sử dụng để đảm bảo tốc độ tính toán, với các tham số chính: \texttt{hidden\_size=128}, \texttt{num\_hidden\_layers=2}, \texttt{num\_attention\_heads=2}.
    
    \subsubsection{Kiểm tra Tác vụ Masked Language Model (\hyperref[acro:mlm]{\texttt{MLM}})}
    Mô hình được huấn luyện trên một tập dữ liệu nhỏ và sau đó được yêu cầu dự đoán token bị che trong câu: \texttt{"BERT is a [MASK]."}
    Kết quả cho thấy mô hình đã học được các mối quan hệ ngữ nghĩa cơ bản, với các dự đoán hợp lý như "model" hay "kelvin".
    
    \begin{verbatim}
    Top 5 dự đoán cho [MASK]:
    --------------------------------------------------
    Token           | Score (Logit)   | Độ tin cậy tương đối
    --------------------------------------------------
    model           | 0.7175          | 21.29%
    kelvin          | 0.6742          | 20.38%
    .               | 0.6318          | 19.54%
    screaming       | 0.6266          | 19.44%
    is              | 0.6225          | 19.36%
    \end{verbatim}
    
    \subsubsection{Kiểm tra Tác vụ Next Sentence Prediction (\hyperref[acro:nsp]{\texttt{NSP}})}
    Mô hình được kiểm tra với hai trường hợp: một cặp câu liên quan và một cặp câu không liên quan.
    \begin{itemize}
        \item \textbf{Cặp câu liên quan:} \texttt{A: 'She reads a book.'} - \texttt{B: 'The book is about dragons.'}
        \begin{itemize}
            \item[] \textbf{Dự đoán:} 'Là câu tiếp theo (IsNext)' (Độ tin cậy: 51.21\%)
        \end{itemize}
        \item \textbf{Cặp câu không liên quan:} \texttt{A: 'The dog is cute.'} - \texttt{B: 'The sky is blue.'}
        \begin{itemize}
            \item[] \textbf{Dự đoán:} 'Không phải câu tiếp theo (NotNext)' (Độ tin cậy: 52.74\%)
        \end{itemize}
    \end{itemize}
    Mặc dù độ tin cậy không cao do tập huấn luyện nhỏ, mô hình đã đưa ra dự đoán đúng cho cả hai trường hợp, xác minh rằng logic của tác vụ \hyperref[acro:nsp]{\texttt{NSP}} đã được cài đặt chính xác.
    
    \subsection{Ứng dụng Phân loại Cảm xúc với \hyperref[acro:bert]{\textbf{BERT}} Pre-trained}
    \label{ssec:phan_loai_cam_xuc}
    Trong phần này, chúng tôi thực hiện fine-tuning một mô hình \hyperref[acro:bert]{\textbf{BERT}} đã được tiền huấn luyện (\texttt{bert-base-uncased}) cho bài toán phân loại cảm xúc trên tập dữ liệu IMDb.
    
    \subsubsection{Thiết lập Thực nghiệm}
    
    \paragraph{Dataset Description}
    Bộ dữ liệu IMDb Movie Reviews \cite{maas2011learning} được sử dụng. Để đảm bảo tính hiệu quả trong thực nghiệm, một tập con được chọn ngẫu nhiên:
    \begin{itemize}
        \item \textbf{Kích thước:} 5,000 reviews (4,000 cho huấn luyện, 1,000 cho validation)
        \item \textbf{Phân bố nhãn:} Cân bằng (50\% positive, 50\% negative)  
        \item \textbf{Độ dài tối đa:} Các review được cắt hoặc đệm đến 256 tokens.
    \end{itemize}
    
    \paragraph{Baseline Model}
    Để có cơ sở so sánh, một mô hình phân loại truyền thống được xây dựng, sử dụng \textbf{\hyperref[acro:tfidf]{\texttt{TF-IDF}} (với 5000 features)} để trích xuất đặc trưng và \textbf{Logistic Regression} để phân loại.
    
    \subsubsection{Quy trình Fine-tuning}
    Quá trình fine-tuning được thực hiện với các siêu tham số được lựa chọn cẩn thận, dựa trên khuyến nghị từ bài báo gốc và điều chỉnh cho phù hợp với tác vụ.
    
    \begin{table}[H]
    \centering
    \caption{Hyperparameters chính cho Fine-tuning \hyperref[acro:bert]{\textbf{BERT}}}
    \label{tab:finetuning_params_app}
    \begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameter} & \textbf{Giá trị} \\
    \midrule
    Pre-trained Model & \texttt{bert-base-uncased} \\
    Learning Rate & $2 \times 10^{-5}$ \\
    Batch Size & 16 (8 với gradient accumulation 2) \\
    Number of Epochs & 3 \\
    Warmup Steps & 500 \\
    Weight Decay & 0.01 \\
    Optimizer & AdamW \\
    Mixed Precision (fp16) & Enabled (nếu có \hyperref[acro:gpu]{\texttt{GPU}}) \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    \subsection{Kết quả và Phân tích}
    \label{ssec:ket_qua_phan_tich_app}
    
    \subsubsection{Kết quả Định lượng}
    \hyperref[acro:bert]{\textbf{BERT}} fine-tuned cho thấy sự vượt trội rõ rệt so với mô hình baseline, cải thiện độ chính xác khoảng 3.9 điểm phần trăm tuyệt đối trên tập validation.
    
    \begin{table}[H]
    \centering
    \caption{Kết quả phân loại cảm xúc trên tập validation (1000 mẫu)}
    \label{tab:results_summary_app}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Phương pháp} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
    \midrule
    \textbf{\hyperref[acro:bert]{\textbf{BERT}} Fine-tuned} & \textbf{0.894} & \textbf{0.89} & \textbf{0.90} & \textbf{0.89} \\
    \hyperref[acro:tfidf]{\texttt{TF-IDF}} + \hyperref[acro:lr]{\texttt{LR}} & 0.855 & 0.86 & 0.86 & 0.85 \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    \subsubsection{Phân tích Lỗi và Ma trận Nhầm lẫn}
    Ma trận nhầm lẫn (Hình \ref{fig:confusion_matrix_app}) cho thấy mô hình \hyperref[acro:bert]{\textbf{BERT}} có khả năng phân biệt tốt giữa hai lớp, với số lượng dự đoán sai (False Positives và False Negatives) tương đối thấp và cân bằng.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{img/bert_confusion_matrix.png}
        \caption{Ma trận nhầm lẫn cho \hyperref[acro:bert]{\textbf{BERT}} fine-tuned trên tập validation của IMDb.}
        \label{fig:confusion_matrix_app}
    \end{figure}
    
    Phân tích các trường hợp dự đoán sai cho thấy mô hình thường gặp khó khăn với các câu có cấu trúc phức tạp, chứa sự mỉa mai (sarcasm) hoặc các câu phủ định tinh vi. Ví dụ, một câu review tích cực về một bộ phim hạng B bị dự đoán là tiêu cực do chứa các từ ngữ thường liên quan đến đánh giá thấp.
    
    \subsubsection{Trực quan hóa Biểu diễn và Attention}
    \paragraph{t-SNE Embedding Visualization}
    Để hiểu cách \hyperref[acro:bert]{\textbf{BERT}} "nhìn" dữ liệu, chúng tôi đã trích xuất các vector embedding của token [CLS] từ tầng cuối cùng và giảm chiều xuống 2D bằng thuật toán t-SNE. Hình \ref{fig:tsne_comparison_app} cho thấy các điểm dữ liệu được biểu diễn bởi \hyperref[acro:bert]{\textbf{BERT}} (trái) tạo thành hai cụm riêng biệt rõ ràng (tích cực và tiêu cực), trong khi các vector từ \hyperref[acro:tfidf]{\texttt{TF-IDF}} (phải) bị trộn lẫn nhiều hơn. Điều này minh họa khả năng vượt trội của \hyperref[acro:bert]{\textbf{BERT}} trong việc học các biểu diễn có khả năng phân tách cao.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.49\textwidth]{img/tsne_bert.png}
        \includegraphics[width=0.49\textwidth]{img/tsne_tfidf.png}
        \caption{So sánh trực quan hóa t-SNE của BERT Embeddings (trái) và TF-IDF Vectors (phải).}
        \label{fig:tsne_comparison_app}
    \end{figure}
    
    \paragraph{Attention Pattern Analysis}
    Sử dụng thư viện \texttt{bertviz}, chúng tôi phân tích các trọng số attention để xem mô hình tập trung vào những từ nào. Kết quả cho thấy, với một câu như \texttt{"The movie was not bad, actually it was quite good..."}, các attention head ở những lớp trên cùng tập trung vào mối quan hệ giữa "not" và "bad", giúp mô hình hiểu đúng ý nghĩa phủ định và đưa ra dự đoán chính xác.
    
    \subsection{Kết luận Chương Thực nghiệm}
    \label{ssec:ket_luan_chuong_thuc_nghiem}
    Thực nghiệm đã đạt được hai mục tiêu chính: (1) Việc xây dựng lại các thành phần cốt lõi đã giúp củng cố kiến thức về kiến trúc và cơ chế hoạt động bên trong của \hyperref[acro:bert]{\textbf{BERT}}. (2) Quá trình ứng dụng mô hình pre-trained vào bài toán phân loại cảm xúc đã chứng minh hiệu quả vượt trội của \hyperref[acro:bert]{\textbf{BERT}} so với phương pháp baseline, với độ chính xác đạt \textbf{89.4\%} trên tập validation. Các phân tích sâu hơn về ma trận nhầm lẫn, embedding và attention đã cung cấp những insight giá trị về điểm mạnh và điểm yếu của mô hình, khẳng định vai trò của học hai chiều và transfer learning trong \hyperref[acro:nlp]{\texttt{NLP}} hiện đại.
    
    %% =========================================================================
    %% CHƯƠNG 6: KẾT QUẢ VÀ SO SÁNH
    %% =========================================================================
    \section{Kết quả và So sánh}
    \label{sec:ket_qua_so_sanh}
    
    Phần này trình bày kết quả của \hyperref[acro:bert]{\textbf{BERT}} trên các benchmark chuẩn từ bài báo gốc \cite{devlin2018bert}, cung cấp bối cảnh để hiểu tầm quan trọng của \hyperref[acro:bert]{\textbf{BERT}} trong lịch sử \hyperref[acro:nlp]{\texttt{NLP}}.
    
    \subsection{Kết quả trên \hyperref[acro:glue]{\texttt{GLUE}} Benchmark}
    \label{ssec:ket_qua_glue}
    
    \hyperref[acro:glue]{\texttt{GLUE}} (General Language Understanding Evaluation) \cite{wang2018glue} là bộ benchmark gồm 9 tác vụ đa dạng, được thiết kế để đánh giá khả năng hiểu ngôn ngữ tổng quát của mô hình.
    
    \subsubsection{Mô tả các tác vụ \hyperref[acro:glue]{\texttt{GLUE}}}
    \begin{itemize}
        \item \textbf{\hyperref[acro:mnli]{\texttt{MNLI}}:} Multi-Genre Natural Language Inference (393K mẫu)
        \item \textbf{\hyperref[acro:qqp]{\texttt{QQP}}:} Quora Question Pairs - Xác định câu hỏi trùng lặp (364K mẫu)
        \item \textbf{\hyperref[acro:qnli]{\texttt{QNLI}}:} Question NLI - Đoạn văn có chứa câu trả lời? (105K mẫu)
        \item \textbf{\hyperref[acro:sst2]{\texttt{SST-2}}:} Stanford Sentiment Treebank - Phân loại cảm xúc (67K mẫu)
        \item \textbf{\hyperref[acro:cola]{\texttt{CoLA}}:} Corpus of Linguistic Acceptability (8.5K mẫu)
        \item \textbf{\hyperref[acro:stsb]{\texttt{STS-B}}:} Semantic Textual Similarity (7K mẫu)
        \item \textbf{\hyperref[acro:mrpc]{\texttt{MRPC}}:} Microsoft Research Paraphrase Corpus (3.7K mẫu)
        \item \textbf{\hyperref[acro:rte]{\texttt{RTE}}:} Recognizing Textual Entailment (2.5K mẫu)
    \end{itemize}
    
    \begin{table}[H]
        \centering
        \caption{Kết quả \hyperref[acro:bert]{\textbf{BERT}} trên \hyperref[acro:glue]{\texttt{GLUE}} test set. Điểm số cao nhất được in đậm \cite{devlin2018bert}.}
        \label{tab:glue_results_detailed}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{lcccccccccc}
            \toprule
            \textbf{Model} & \textbf{\hyperref[acro:mnli]{\texttt{MNLI}}-m/mm} & \textbf{\hyperref[acro:qqp]{\texttt{QQP}}} & \textbf{\hyperref[acro:qnli]{\texttt{QNLI}}} & \textbf{\hyperref[acro:sst2]{\texttt{SST-2}}} & \textbf{\hyperref[acro:cola]{\texttt{CoLA}}} & \textbf{\hyperref[acro:stsb]{\texttt{STS-B}}} & \textbf{\hyperref[acro:mrpc]{\texttt{MRPC}}} & \textbf{\hyperref[acro:rte]{\texttt{RTE}}} & \textbf{\hyperref[acro:wnli]{\texttt{WNLI}}} & \textbf{Avg} \\
            & (Acc) & (\hyperref[acro:f1]{\texttt{F1}}/Acc) & (Acc) & (Acc) & (Mcc) & (Corr) & (\hyperref[acro:f1]{\texttt{F1}}/Acc) & (Acc) & (Acc) & \\
            \midrule
            Previous SOTA & 80.6/80.1 & 66.1/- & 82.3 & 93.2 & 35.0 & 81.0 & 86.0/- & 61.7 & 65.1 & 74.0 \\
            OpenAI \hyperref[acro:gpt]{\texttt{GPT}} \cite{radford2018improving} & 82.1/81.4 & 70.3/88.5 & 87.4 & 91.3 & 45.4 & 80.0 & 82.3/75.7 & 56.0 & 65.1 & 72.8 \\
            \midrule
            BERT\textsubscript{BASE} & 84.6/83.4 & 71.2/89.2 & 90.5 & 93.5 & 52.1 & 85.8 & 88.9/84.8 & 66.4 & 65.1 & 78.3 \\
            BERT\textsubscript{LARGE} & \textbf{86.7/85.9} & \textbf{72.1/89.3} & \textbf{92.7} & \textbf{94.9} & \textbf{60.5} & \textbf{86.5} & \textbf{89.3/85.4} & \textbf{70.1} & 65.1 & \textbf{80.5} \\
            \bottomrule
        \end{tabular}%
        }
    \end{table}
    
    \subsubsection{Phân tích kết quả \hyperref[acro:glue]{\texttt{GLUE}}}
    \begin{itemize}
        \item BERT\textsubscript{LARGE} đạt 80.5 điểm trung bình, tăng 7.7 điểm tuyệt đối so với \hyperref[acro:gpt]{\texttt{GPT}} (72.8)
        \item Cải thiện lớn nhất: \hyperref[acro:cola]{\texttt{CoLA}} (+15.1), \hyperref[acro:rte]{\texttt{RTE}} (+14.1) - các tác vụ có ít dữ liệu
        \item Điều này cho thấy \hyperref[acro:bert]{\textbf{BERT}} học được representations tổng quát, transfer tốt cho small datasets
    \end{itemize}
    
    \subsection{Kết quả trên \hyperref[acro:squad]{\texttt{SQuAD}}}
    \label{ssec:ket_qua_squad}
    
    \subsubsection{\hyperref[acro:squad]{\texttt{SQuAD}} v1.1}
    \hyperref[acro:squad]{\texttt{SQuAD}} v1.1 yêu cầu trích xuất câu trả lời từ đoạn văn cho câu hỏi.
    
    \begin{table}[H]
        \centering
        \caption{Kết quả trên \hyperref[acro:squad]{\texttt{SQuAD}} v1.1 test set \cite{devlin2018bert}.}
        \label{tab:squad_v1_results}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Model} & \textbf{\hyperref[acro:em]{\texttt{EM}}} & \textbf{\hyperref[acro:f1]{\texttt{F1}}} \\
            \midrule
            Human Performance & 82.3 & 91.2 \\
            Previous best (ensemble) & 84.4 & 90.9 \\
            \midrule
            BERT\textsubscript{BASE} (single) & 80.8 & 88.5 \\
            BERT\textsubscript{LARGE} (single) & 84.1 & 90.9 \\
            BERT\textsubscript{LARGE} (ensemble) & \textbf{87.4} & \textbf{93.2} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    Điểm đáng chú ý: \hyperref[acro:bert]{\textbf{BERT}} ensemble vượt qua human performance với \hyperref[acro:f1]{\texttt{F1}} 93.2 so với 91.2!
    
    \subsubsection{\hyperref[acro:squad]{\texttt{SQuAD}} v2.0}
    \hyperref[acro:squad]{\texttt{SQuAD}} v2.0 khó hơn vì có thêm câu hỏi không có câu trả lời trong đoạn văn.
    
    \begin{table}[H]
        \centering
        \caption{Kết quả trên \hyperref[acro:squad]{\texttt{SQuAD}} v2.0 test set \cite{devlin2018bert}.}
        \label{tab:squad_v2_results}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Model} & \textbf{\hyperref[acro:em]{\texttt{EM}}} & \textbf{\hyperref[acro:f1]{\texttt{F1}}} \\
            \midrule
            Human Performance & 86.8 & 89.5 \\
            Previous best & 72.3 & 74.8 \\
            \midrule
            BERT\textsubscript{LARGE} & \textbf{78.7} & \textbf{81.9} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \hyperref[acro:bert]{\textbf{BERT}} cải thiện \textbf{7.1} \hyperref[acro:f1]{\texttt{F1}} points - một bước nhảy lớn cho tác vụ khó này.
    
    \subsection{Ablation Studies: Hiểu sâu về \hyperref[acro:bert]{\textbf{BERT}}}
    \label{ssec:ablation_study_bert}
    
    \subsubsection{Tầm quan trọng của các thành phần}
    \begin{table}[H]
        \centering
        \caption{Ablation study: Loại bỏ từng thành phần của \hyperref[acro:bert]{\textbf{BERT}} \cite{devlin2018bert}.}
        \label{tab:ablation_components}
        \begin{tabular}{lccccc}
            \toprule
            \textbf{Model} & \textbf{\hyperref[acro:mnli]{\texttt{MNLI}}-m} & \textbf{\hyperref[acro:qnli]{\texttt{QNLI}}} & \textbf{\hyperref[acro:mrpc]{\texttt{MRPC}}} & \textbf{\hyperref[acro:sst2]{\texttt{SST-2}}} & \textbf{\hyperref[acro:squad]{\texttt{SQuAD}}} \\
            & (Acc) & (Acc) & (Acc) & (Acc) & (\hyperref[acro:f1]{\texttt{F1}}) \\
            \midrule
            BERT\textsubscript{BASE} & 84.4 & 88.4 & 86.7 & 92.7 & 88.5 \\
            \midrule
            No \hyperref[acro:nsp]{\texttt{NSP}} & 83.9 & 84.9 & 86.5 & 92.6 & 87.9 \\
            No \hyperref[acro:mlm]{\texttt{MLM}} (LTR only) & 82.1 & 84.3 & 77.5 & 92.1 & 77.8 \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    Insights quan trọng:
    \begin{itemize}
        \item Bỏ \hyperref[acro:nsp]{\texttt{NSP}}: Giảm nhẹ performance, đặc biệt trên \hyperref[acro:qnli]{\texttt{QNLI}} (-3.5\%)
        \item Bỏ \hyperref[acro:mlm]{\texttt{MLM}} (chỉ train left-to-right): Giảm mạnh, đặc biệt \hyperref[acro:squad]{\texttt{SQuAD}} (-10.7 \hyperref[acro:f1]{\texttt{F1}})
        \item → Bidirectional training (\hyperref[acro:mlm]{\texttt{MLM}}) quan trọng hơn nhiều so với \hyperref[acro:nsp]{\texttt{NSP}}
    \end{itemize}
    
    \subsubsection{Ảnh hưởng của Model Size}
    \begin{table}[H]
        \centering
        \caption{Ảnh hưởng của kích thước mô hình \cite{devlin2018bert}.}
        \label{tab:model_size_effect}
        \begin{tabular}{lcccccc}
            \toprule
            \textbf{Model} & \textbf{\#Layers} & \textbf{\#Heads} & \textbf{Hidden} & \textbf{Params} & \textbf{\hyperref[acro:mnli]{\texttt{MNLI}}} & \textbf{\hyperref[acro:mrpc]{\texttt{MRPC}}} \\
            \midrule
            BERT-Small & 4 & 8 & 512 & 29M & 77.6 & 79.8 \\
            BERT-Medium & 8 & 8 & 512 & 42M & 80.8 & 82.7 \\
            BERT-Base & 12 & 12 & 768 & 110M & 84.4 & 86.7 \\
            BERT-Large & 24 & 16 & 1024 & 340M & 86.6 & 87.8 \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    Larger models consistently better, even on small datasets (\hyperref[acro:mrpc]{\texttt{MRPC}}: 3.7K samples). Điều này cho thấy pre-training hiệu quả giúp tránh overfitting.
    
    %% =========================================================================
    %% CHƯƠNG 7: HẠN CHẾ VÀ HƯỚNG PHÁT TRIỂN
    %% =========================================================================
    \section{Hạn chế và Hướng Phát triển}
    \label{sec:han_che_huong_phat_trien}
    
    \subsection{Hạn chế của \hyperref[acro:bert]{\textbf{BERT}}}
    \label{ssec:han_che_bert}
    
    Mặc dù \hyperref[acro:bert]{\textbf{BERT}} đã tạo ra bước đột phá, mô hình vẫn có những hạn chế cần được nhận diện và giải quyết.
    
    \subsubsection{Chi phí tính toán khổng lồ}
    \begin{itemize}
        \item \textbf{Pre-training cost:}
        \begin{itemize}
            \item BERT\textsubscript{BASE}: 4 ngày trên 16 \hyperref[acro:tpu]{\texttt{TPU}} chips \cite{devlin2018bert}
            \item BERT\textsubscript{LARGE}: 4 ngày trên 64 \hyperref[acro:tpu]{\texttt{TPU}} chips \cite{devlin2018bert}
            \item Chi phí ước tính: \$7,000 - \$50,000 tùy cấu hình
        \end{itemize}
        \item \textbf{Environmental impact:} Carbon footprint tương đương 5 chuyến bay xuyên Mỹ
        \item \textbf{Inference cost:} BERT\textsubscript{LARGE} cần ~1GB memory, chậm cho real-time apps
    \end{itemize}
    
    \subsubsection{Masked Token Discrepancy}
    \begin{itemize}
        \item [MASK] token chỉ xuất hiện trong pre-training, không có trong fine-tuning/inference
        \item Mặc dù có chiến lược 80-10-10, vẫn tồn tại domain shift
        \item Các mô hình sau (\hyperref[acro:electra]{\texttt{ELECTRA}} \cite{clark2020electra}) giải quyết bằng cách không dùng [MASK]
    \end{itemize}
    
    \subsubsection{Fixed Length Limitation}
    \begin{itemize}
        \item Maximum 512 tokens - không đủ cho văn bản dài (legal documents, books)
        \item Truncation làm mất thông tin quan trọng
        \item Solutions: Longformer \cite{beltagy2020longformer} (4096 tokens), BigBird (sparse attention)
    \end{itemize}
    
    \subsubsection{Unidirectional Fine-tuning for Generation}
    \begin{itemize}
        \item \hyperref[acro:bert]{\textbf{BERT}} là encoder-only, không tự nhiên cho text generation
        \item Các hack (mask prediction iteratively) kém hiệu quả
        \item Dẫn đến phát triển encoder-decoder models (\hyperref[acro:t5]{\texttt{T5}} \cite{raffel2020exploring}, \hyperref[acro:bart]{\texttt{BART}} \cite{lewis2019bart})
    \end{itemize}
    
    \subsubsection{Static Embeddings}
    \begin{itemize}
        \item Position embeddings cố định 512 vị trí
        \item Không thể extrapolate cho sequences dài hơn
        \item Relative position embeddings (\hyperref[acro:t5]{\texttt{T5}} \cite{raffel2020exploring}) linh hoạt hơn
    \end{itemize}
    
    \subsubsection{\hyperref[acro:nsp]{\texttt{NSP}} Task Controversy}
    \begin{itemize}
        \item Nhiều nghiên cứu cho thấy \hyperref[acro:nsp]{\texttt{NSP}} không cần thiết hoặc có hại \cite{liu2019roberta}
        \item Random sentences quá dễ phân biệt → task không học gì có ích
        \item \hyperref[acro:roberta]{\texttt{RoBERTa}} \cite{liu2019roberta} bỏ \hyperref[acro:nsp]{\texttt{NSP}} và đạt kết quả tốt hơn
    \end{itemize}
    
    \subsection{Các Hướng Phát triển Kế thừa \hyperref[acro:bert]{\textbf{BERT}}}
    \label{ssec:huong_phat_trien_ke_thua}
    
    \hyperref[acro:bert]{\textbf{BERT}} đã mở ra "Cambrian explosion" của các mô hình \hyperref[acro:nlp]{\texttt{NLP}}. Các hướng phát triển chính:
    
    \subsubsection{Cải tiến Training Efficiency}
    
    \textbf{1. \hyperref[acro:roberta]{\texttt{RoBERTa}} (Robustly Optimized \hyperref[acro:bert]{\textbf{BERT}}) \cite{liu2019roberta}:}
    \begin{itemize}
        \item Loại bỏ \hyperref[acro:nsp]{\texttt{NSP}}, train lâu hơn (500K steps vs 100K)
        \item Dynamic masking thay vì static
        \item Larger batches (8K vs 256)
        \item Kết quả: +2-3\% trên hầu hết tasks
    \end{itemize}
    
    \textbf{2. \hyperref[acro:electra]{\texttt{ELECTRA}} (Efficiently Learning an Encoder) \cite{clark2020electra}:}
    \begin{itemize}
        \item Replaced token detection thay vì \hyperref[acro:mlm]{\texttt{MLM}}
        \item Train discriminator phân biệt real/fake tokens
        \item Hiệu quả hơn 4x về compute
        \item Small \hyperref[acro:electra]{\texttt{ELECTRA}} $\approx$ BERT\textsubscript{BASE} với 1/4 compute
    \end{itemize}
    
    \subsubsection{Model Compression}
    
    \textbf{1. DistilBERT \cite{sanh2019distilbert}:}
    \begin{itemize}
        \item Knowledge distillation từ BERT\textsubscript{BASE}
        \item 40\% smaller, 60\% faster, giữ 97\% performance
        \item Key: Distill during pre-training, not just fine-tuning
    \end{itemize}
    
    \textbf{2. \hyperref[acro:albert]{\texttt{ALBERT}} (A Lite \hyperref[acro:bert]{\textbf{BERT}}) \cite{lan2019albert}:}
    \begin{itemize}
        \item Factorized embeddings: $V \times E \rightarrow V \times E' \times H$ (E' << H)
        \item Cross-layer parameter sharing
        \item 18x fewer parameters với comparable performance
    \end{itemize}
    
    \textbf{3. Quantization \& Pruning:}
    \begin{itemize}
        \item 8-bit quantization: 4x compression, <1\% performance drop
        \item Structured pruning: Remove attention heads/layers
        \item TensorRT optimization cho deployment
    \end{itemize}
    
    \subsubsection{Architectural Innovations}
    
    \textbf{1. Handling Long Documents:}
    \begin{itemize}
        \item \textbf{Longformer \cite{beltagy2020longformer}:} Sliding window + global attention
        \item \textbf{BigBird:} Sparse attention (random + window + global)
        \item \textbf{Linformer:} Linear complexity attention
    \end{itemize}
    
    \textbf{2. Unified Models:}
    \begin{itemize}
        \item \textbf{\hyperref[acro:t5]{\texttt{T5}} \cite{raffel2020exploring}:} "Text-to-Text" framework - mọi task là seq2seq
        \item \textbf{\hyperref[acro:bart]{\texttt{BART}} \cite{lewis2019bart}:} Denoising autoencoder cho generation
        \item \textbf{\hyperref[acro:unilm]{\texttt{UniLM}}:} Unified pre-training cho cả understanding và generation
    \end{itemize}
    
    \subsubsection{Multilingual và Cross-lingual}
    
    \textbf{1. mBERT → XLM \cite{conneau2019unsupervised} → \hyperref[acro:xlm-r]{\texttt{XLM-R}}:}
    \begin{itemize}
        \item Progressive improvements trong multilingual understanding
        \item \hyperref[acro:xlm-r]{\texttt{XLM-R}}: 100 languages, 2.5TB text
        \item Zero-shot cross-lingual transfer
    \end{itemize}
    
    \textbf{2. Language-Specific Models:}
    \begin{itemize}
        \item PhoBERT (Vietnamese) \cite{nguyen2020phobert}, CamemBERT (French) \cite{martin2019camembert}, BERTje (Dutch)
        \item Often outperform mBERT on monolingual tasks
    \end{itemize}
    
    \subsubsection{Domain Adaptation}
    
    \textbf{1. Continued Pre-training:}
    \begin{itemize}
        \item BioBERT \cite{lee2020biobert}: PubMed papers → biomedical \hyperref[acro:ner]{\texttt{NER}}/RE
        \item SciBERT \cite{beltagy2019scibert}: Scientific papers → citation intent classification
        \item FinBERT: Financial texts → sentiment analysis
    \end{itemize}
    
    \textbf{2. Adapter Modules:}
    \begin{itemize}
        \item Thêm small trainable modules, freeze \hyperref[acro:bert]{\textbf{BERT}}
        \item Efficient multi-domain adaptation
        \item Parameter efficient: ~3\% additional parameters per task
    \end{itemize}
    
    \subsubsection{Toward Foundation Models}
    \hyperref[acro:bert]{\textbf{BERT}}'s success paved the way for:
    \begin{itemize}
        \item GPT-3: Scaling to 175B parameters
        \item PaLM: 540B parameters với breakthrough capabilities
        \item ChatGPT/GPT-4: Instruction following và alignment
    \end{itemize}
    
    %% =========================================================================
    %% KẾT LUẬN
    %% =========================================================================
    \section{Kết luận}
    \label{sec:ket_luan}
    
    \hyperref[acro:bert]{\textbf{BERT}} - Bidirectional Encoder Representations from Transformers \cite{devlin2018bert} - không chỉ là một mô hình mà còn là một cột mốc quan trọng đánh dấu sự chuyển mình của lĩnh vực Xử lý Ngôn ngữ Tự nhiên. Bằng cách giới thiệu phương pháp học biểu diễn hai chiều sâu sắc thông qua Masked Language Modeling và Next Sentence Prediction, \hyperref[acro:bert]{\textbf{BERT}} đã chứng minh rằng việc pre-training trên dữ liệu không nhãn có thể tạo ra những biểu diễn ngôn ngữ mạnh mẽ, có khả năng transfer xuất sắc cho nhiều tác vụ downstream.
    
    \textbf{Những đóng góp then chốt của \hyperref[acro:bert]{\textbf{BERT}}:}
    \begin{itemize}
        \item \textbf{Deeply bidirectional architecture:} Lần đầu tiên cho phép mỗi từ được hiểu trong ngữ cảnh đầy đủ của cả câu, không chỉ một phía như các mô hình trước đó. Điều này được thực hiện thông qua cơ chế self-attention cho phép mô hình hóa quan hệ giữa mọi cặp token với độ phức tạp $O(n^2)$.
        
        \item \textbf{Pre-training objectives sáng tạo:} \hyperref[acro:mlm]{\texttt{MLM}} và \hyperref[acro:nsp]{\texttt{NSP}} tuy đơn giản nhưng hiệu quả, cho phép học từ dữ liệu không nhãn - nguồn tài nguyên dồi dào và dễ thu thập. Chiến lược masking 80-10-10 được thiết kế cẩn thận để giảm thiểu domain shift giữa pre-training và fine-tuning.
        
        \item \textbf{Transfer learning paradigm:} Thiết lập mô hình "pre-train once, fine-tune for everything" đã trở thành chuẩn mực cho ngành. Với chi phí fine-tuning thấp (vài giờ trên GPU), \hyperref[acro:bert]{\textbf{BERT}} democratize việc sử dụng các mô hình ngôn ngữ mạnh mẽ.
        
        \item \textbf{Empirical breakthroughs:} Không chỉ cải thiện incremental mà tạo ra những bước nhảy lớn trên nhiều benchmarks. Trên \hyperref[acro:glue]{\texttt{GLUE}}, \hyperref[acro:bert]{\textbf{BERT}} tăng 7.7 điểm tuyệt đối. Trên \hyperref[acro:squad]{\texttt{SQuAD}} v1.1, thậm chí vượt qua human performance.
    \end{itemize}
    
    \textbf{Tác động sâu rộng:}
    
    \hyperref[acro:bert]{\textbf{BERT}} đã mở ra kỷ nguyên mới của "Foundation Models" - những mô hình nền tảng được pre-train trên quy mô lớn rồi adapted cho các ứng dụng cụ thể. Từ \hyperref[acro:bert]{\textbf{BERT}}, chúng ta đã chứng kiến sự phát triển bùng nổ của:
    
    \begin{itemize}
        \item Các biến thể hiệu quả hơn (\hyperref[acro:roberta]{\texttt{RoBERTa}} \cite{liu2019roberta}, \hyperref[acro:electra]{\texttt{ELECTRA}} \cite{clark2020electra}, \hyperref[acro:albert]{\texttt{ALBERT}} \cite{lan2019albert})
        \item Mô hình đa ngôn ngữ (\hyperref[acro:xlm-r]{\texttt{XLM-R}} \cite{conneau2019unsupervised}, mT5)
        \item Scaling lên quy mô khổng lồ (GPT-3, PaLM, ChatGPT)
        \item Ứng dụng thực tế trong mọi ngành (search, translation, chatbots, content generation)
    \end{itemize}
    
    \textbf{Bài học cho tương lai:}
    
    Thành công của \hyperref[acro:bert]{\textbf{BERT}} cho thấy rằng:
    \begin{enumerate}
        \item \textbf{Simple ideas executed well can be revolutionary:} \hyperref[acro:mlm]{\texttt{MLM}} là ý tưởng đơn giản nhưng được thực hiện một cách tinh tế với attention mechanism và chiến lược masking phù hợp.
        
        \item \textbf{Scale matters:} Cả về dữ liệu (3.3B words) và model capacity (340M parameters cho BERT\textsubscript{LARGE}). Nhưng quan trọng hơn là cách sử dụng scale một cách hiệu quả.
        
        \item \textbf{Transfer learning là chìa khóa:} Cho phép tận dụng kiến thức từ dữ liệu không nhãn cho các tác vụ có ít dữ liệu labeled.
        
        \item \textbf{Bidirectionality và context là cốt lõi:} Hiểu ngôn ngữ đòi hỏi xem xét ngữ cảnh đầy đủ, không chỉ một chiều.
    \end{enumerate}
    
    \textbf{Nhìn về phía trước:}
    
    Trong khi \hyperref[acro:bert]{\textbf{BERT}} đã đặt nền móng vững chắc, vẫn còn nhiều thách thức:
    \begin{itemize}
        \item \textbf{Computational efficiency:} Cần giảm chi phí tính toán cho deployment rộng rãi
        \item \textbf{Longer contexts:} Xử lý văn bản dài hơn 512 tokens hiệu quả
        \item \textbf{Multimodal understanding:} Mở rộng sang hình ảnh, âm thanh, video
        \item \textbf{Better alignment:} Với human values và intentions
        \item \textbf{Low-resource languages:} Hỗ trợ các ngôn ngữ ít tài nguyên
    \end{itemize}
    
    \hyperref[acro:bert]{\textbf{BERT}} sẽ được ghi nhớ không chỉ vì những con số ấn tượng trên benchmarks, mà vì đã thay đổi cách chúng ta nghĩ về language understanding và machine learning. Từ một bài báo học thuật, \hyperref[acro:bert]{\textbf{BERT}} đã trở thành nền tảng cho vô số ứng dụng AI đang thay đổi thế giới hàng ngày. Đó chính là di sản thực sự của một nghiên cứu đột phá - không chỉ giải quyết các vấn đề kỹ thuật mà còn mở ra những khả năng mới cho toàn nhân loại.