{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\hcmus\\HK4\\MathAI\\final\\code\\.my-env\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\hcmus\\HK4\\MathAI\\final\\code\\.my-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\hcmus\\HK4\\MathAI\\final\\code\\.my-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT training...\n",
      "Epoch 0, Batch 0, Loss: 11.5031\n",
      "Average training loss: 11.5031\n",
      "BERT model created successfully!\n",
      "Total parameters: 133,547,324\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # Linear transformations\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.w_o(attention_output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.layer_norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_position_embeddings=512, \n",
    "                 type_vocab_size=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, d_model)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, \n",
    "                                      device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "            \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        return x\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # Pool the [CLS] token representation\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class BertForMaskedLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertForNextSentencePrediction(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, n_heads=12, \n",
    "                 d_ff=3072, max_position_embeddings=512, type_vocab_size=2, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            type_vocab_size=type_vocab_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.encoder = BertEncoder(\n",
    "            n_layers=n_layers,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.pooler = BertPooler(d_model)\n",
    "        \n",
    "        # Pre-training heads\n",
    "        self.cls = BertForMaskedLM(vocab_size, d_model)\n",
    "        self.nsp = BertForNextSentencePrediction(d_model)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "                masked_lm_labels=None, next_sentence_label=None):\n",
    "        \n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)\n",
    "        else:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # Embeddings\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(embedding_output, attention_mask)\n",
    "        \n",
    "        # Pooler for [CLS] token\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "        \n",
    "        # Pre-training outputs\n",
    "        prediction_scores = self.cls(encoder_output)\n",
    "        seq_relationship_score = self.nsp(pooled_output)\n",
    "        \n",
    "        outputs = {\n",
    "            'last_hidden_state': encoder_output,\n",
    "            'pooler_output': pooled_output,\n",
    "            'prediction_logits': prediction_scores,\n",
    "            'seq_relationship_logits': seq_relationship_score\n",
    "        }\n",
    "        \n",
    "        # Calculate losses if labels provided\n",
    "        total_loss = 0\n",
    "        if masked_lm_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            masked_lm_loss = loss_fct(\n",
    "                prediction_scores.view(-1, prediction_scores.size(-1)),\n",
    "                masked_lm_labels.view(-1)\n",
    "            )\n",
    "            total_loss += masked_lm_loss\n",
    "            outputs['masked_lm_loss'] = masked_lm_loss\n",
    "            \n",
    "        if next_sentence_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(\n",
    "                seq_relationship_score.view(-1, 2),\n",
    "                next_sentence_label.view(-1)\n",
    "            )\n",
    "            total_loss += next_sentence_loss\n",
    "            outputs['next_sentence_loss'] = next_sentence_loss\n",
    "            \n",
    "        if total_loss > 0:\n",
    "            outputs['loss'] = total_loss\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# Dataset class for BERT pre-training\n",
    "class BertPretrainingDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512, mlm_probability=0.15):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def create_masked_lm_predictions(self, tokens):\n",
    "        \"\"\"Create masked language model predictions\"\"\"\n",
    "        output_tokens = tokens.copy()\n",
    "        output_labels = [-100] * len(tokens)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if random.random() < self.mlm_probability:\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    # 80% replace with [MASK]\n",
    "                    output_tokens[i] = self.tokenizer.mask_token_id\n",
    "                elif prob < 0.9:\n",
    "                    # 10% replace with random token\n",
    "                    output_tokens[i] = random.randint(1, self.tokenizer.vocab_size - 1)\n",
    "                # 10% keep original\n",
    "                \n",
    "                output_labels[i] = tokens[i]\n",
    "                \n",
    "        return output_tokens, output_labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True, \n",
    "                                     max_length=self.max_length, \n",
    "                                     truncation=True, padding='max_length')\n",
    "        \n",
    "        # Create masked LM predictions\n",
    "        masked_tokens, mlm_labels = self.create_masked_lm_predictions(tokens)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(masked_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1 if t != 0 else 0 for t in tokens], \n",
    "                                         dtype=torch.long),\n",
    "            'masked_lm_labels': torch.tensor(mlm_labels, dtype=torch.long),\n",
    "            'next_sentence_label': torch.tensor(0, dtype=torch.long)  # Simplified\n",
    "        }\n",
    "\n",
    "# Training function\n",
    "def train_bert(model, dataloader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            masked_lm_labels = batch['masked_lm_labels'].to(device)\n",
    "            next_sentence_labels = batch['next_sentence_label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                masked_lm_labels=masked_lm_labels,\n",
    "                next_sentence_label=next_sentence_labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Example usage for fine-tuning on classification\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(bert_model.pooler.dense.out_features, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            return {'loss': loss, 'logits': logits}\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "# Initialize model\n",
    "def create_bert_model(vocab_size=30522):\n",
    "    model = BERT(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=768,\n",
    "        n_layers=12,\n",
    "        n_heads=12,\n",
    "        d_ff=3072,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Initialize tokenizer and model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokenizer = \u001b[43mBertTokenizer\u001b[49m.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m     model = create_bert_model(vocab_size=tokenizer.vocab_size)\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Example texts for training\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = create_bert_model(vocab_size=tokenizer.vocab_size)\n",
    "    \n",
    "    # Example texts for training\n",
    "    texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"BERT is a transformer-based machine learning technique.\",\n",
    "        \"Natural language processing is a subfield of AI.\",\n",
    "        # Add more texts for actual training\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BertPretrainingDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Setup training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting BERT training...\")\n",
    "    avg_loss = train_bert(model, dataloader, optimizer, device, epochs=1)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Example: Create classification model\n",
    "    classification_model = BertForSequenceClassification(model, num_labels=2)\n",
    "    print(\"BERT model created successfully!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
