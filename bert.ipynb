{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.32.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism with improved numerical stability\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Separate weights for Q, K, V projections\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Linear transformations and reshape for multi-head\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Expand mask for multi-head attention\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.w_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network with GELU activation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with pre-norm architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"BERT embeddings with token, position, and segment embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, max_position_embeddings: int = 512,\n",
    "                 type_vocab_size: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, d_model)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize position ids\n",
    "        self.register_buffer(\"position_ids\", torch.arange(max_position_embeddings).expand((1, -1)))\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, \n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "            \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, \n",
    "                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    \"\"\"Pooler for [CLS] token representation\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # Take the hidden state corresponding to the first token ([CLS])\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    \"\"\"Transform for MLM predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    \"\"\"Language Model prediction head\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(d_model)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    \"\"\"Pre-training heads for MLM and NSP\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(d_model, vocab_size)\n",
    "        self.seq_relationship = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, sequence_output: torch.Tensor, \n",
    "                pooled_output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    \"\"\"BERT model with improved architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            d_model=config['hidden_size'],\n",
    "            max_position_embeddings=config['max_position_embeddings'],\n",
    "            type_vocab_size=config['type_vocab_size'],\n",
    "            dropout=config['hidden_dropout_prob']\n",
    "        )\n",
    "        \n",
    "        self.encoder = BertEncoder(\n",
    "            n_layers=config['num_hidden_layers'],\n",
    "            d_model=config['hidden_size'],\n",
    "            n_heads=config['num_attention_heads'],\n",
    "            d_ff=config['intermediate_size'],\n",
    "            dropout=config['hidden_dropout_prob']\n",
    "        )\n",
    "        \n",
    "        self.pooler = BertPooler(config['hidden_size'])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config['initializer_range'])\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config['initializer_range'])\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "        \n",
    "        # Pass through encoder\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=embedding_output,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Pool the [CLS] token\n",
    "        pooled_output = self.pooler(encoder_outputs)\n",
    "        \n",
    "        return {\n",
    "            'last_hidden_state': encoder_outputs,\n",
    "            'pooler_output': pooled_output\n",
    "        }\n",
    "\n",
    "class BertForPreTraining(nn.Module):\n",
    "    \"\"\"BERT model with pre-training heads\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config['hidden_size'], config['vocab_size'])\n",
    "        \n",
    "        # Tie weights between input embeddings and output embeddings\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.weight\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                next_sentence_label: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        \n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, prediction_scores.size(-1)), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'prediction_logits': prediction_scores,\n",
    "            'seq_relationship_logits': seq_relationship_score,\n",
    "            'hidden_states': outputs['last_hidden_state'],\n",
    "            'pooler_output': outputs['pooler_output']\n",
    "        }\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT for sequence classification tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
    "        self.classifier = nn.Linear(config['hidden_size'], num_labels)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs['pooler_output']\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'hidden_states': outputs['last_hidden_state'],\n",
    "            'pooler_output': outputs['pooler_output']\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for BERT-base\n",
    "def get_bert_config(model_size='base'):\n",
    "    \"\"\"Get BERT configuration\"\"\"\n",
    "    if model_size == 'base':\n",
    "        return {\n",
    "            'vocab_size': 30522,\n",
    "            'hidden_size': 768,\n",
    "            'num_hidden_layers': 12,\n",
    "            'num_attention_heads': 12,\n",
    "            'intermediate_size': 3072,\n",
    "            'hidden_dropout_prob': 0.1,\n",
    "            'attention_probs_dropout_prob': 0.1,\n",
    "            'max_position_embeddings': 512,\n",
    "            'type_vocab_size': 2,\n",
    "            'initializer_range': 0.02,\n",
    "        }\n",
    "    elif model_size == 'large':\n",
    "        return {\n",
    "            'vocab_size': 30522,\n",
    "            'hidden_size': 1024,\n",
    "            'num_hidden_layers': 24,\n",
    "            'num_attention_heads': 16,\n",
    "            'intermediate_size': 4096,\n",
    "            'hidden_dropout_prob': 0.1,\n",
    "            'attention_probs_dropout_prob': 0.1,\n",
    "            'max_position_embeddings': 512,\n",
    "            'type_vocab_size': 2,\n",
    "            'initializer_range': 0.02,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model size: {model_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Total parameters: 110,078,780\n",
      "Output shapes:\n",
      "  - Prediction logits: torch.Size([2, 128, 30522])\n",
      "  - NSP logits: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create BERT model\n",
    "config = get_bert_config('base')\n",
    "model = BertForPreTraining(config)\n",
    "\n",
    "# Example input\n",
    "batch_size = 2\n",
    "seq_length = 128\n",
    "input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_length))\n",
    "attention_mask = torch.ones(batch_size, seq_length)\n",
    "token_type_ids = torch.zeros(batch_size, seq_length, dtype=torch.long)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Output shapes:\")\n",
    "print(f\"  - Prediction logits: {outputs['prediction_logits'].shape}\")\n",
    "print(f\"  - NSP logits: {outputs['seq_relationship_logits'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT pre-training with MLM and NSP tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 texts: List[str],\n",
    "                 tokenizer: BertTokenizer,\n",
    "                 max_length: int = 512,\n",
    "                 mlm_probability: float = 0.15,\n",
    "                 short_seq_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: List of text documents\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length: Maximum sequence length\n",
    "            mlm_probability: Probability of masking tokens for MLM\n",
    "            short_seq_prob: Probability of creating shorter sequences\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.short_seq_prob = short_seq_prob\n",
    "        \n",
    "        # Pre-process texts into sentences\n",
    "        self.documents = self._preprocess_texts()\n",
    "        \n",
    "    def _preprocess_texts(self) -> List[List[str]]:\n",
    "        \"\"\"Split texts into sentences\"\"\"\n",
    "        documents = []\n",
    "        for text in self.texts:\n",
    "            # Simple sentence splitting (can be improved with NLTK or spaCy)\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            if sentences:\n",
    "                documents.append(sentences)\n",
    "        return documents\n",
    "    \n",
    "    def _get_random_sentence(self, exclude_doc_idx: int) -> str:\n",
    "        \"\"\"Get a random sentence from a different document\"\"\"\n",
    "        doc_idx = random.choice([i for i in range(len(self.documents)) if i != exclude_doc_idx])\n",
    "        if self.documents[doc_idx]:\n",
    "            return random.choice(self.documents[doc_idx])\n",
    "        return \"\"\n",
    "    \n",
    "    def _create_training_instance(self, doc_idx: int) -> Tuple[str, str, int]:\n",
    "        \"\"\"Create a training instance with sentence A, sentence B, and NSP label\"\"\"\n",
    "        document = self.documents[doc_idx]\n",
    "        \n",
    "        # Get sentence A\n",
    "        sent_idx_a = random.randint(0, len(document) - 1)\n",
    "        sent_a = document[sent_idx_a]\n",
    "        \n",
    "        # Create sentence B and NSP label\n",
    "        if random.random() < 0.5 and sent_idx_a < len(document) - 1:\n",
    "            # Next sentence (positive example)\n",
    "            sent_b = document[sent_idx_a + 1]\n",
    "            is_next = 1\n",
    "        else:\n",
    "            # Random sentence (negative example)\n",
    "            sent_b = self._get_random_sentence(doc_idx)\n",
    "            is_next = 0\n",
    "            \n",
    "        return sent_a, sent_b, is_next\n",
    "    \n",
    "    def _truncate_seq_pair(self, tokens_a: List[int], tokens_b: List[int], max_length: int):\n",
    "        \"\"\"Truncate sequence pair to fit max_length\"\"\"\n",
    "        while len(tokens_a) + len(tokens_b) > max_length - 3:  # Account for [CLS], [SEP], [SEP]\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "    \n",
    "    def _create_masked_lm_predictions(self, \n",
    "                                     tokens: List[int],\n",
    "                                     mlm_probability: float) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"Create masked language model predictions\"\"\"\n",
    "        output_tokens = tokens.copy()\n",
    "        output_labels = [-100] * len(tokens)  # -100 is ignored by CrossEntropyLoss\n",
    "        \n",
    "        # Get candidates for masking (exclude [CLS], [SEP], [PAD])\n",
    "        candidate_indices = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token not in [self.tokenizer.cls_token_id, \n",
    "                           self.tokenizer.sep_token_id,\n",
    "                           self.tokenizer.pad_token_id]:\n",
    "                candidate_indices.append(i)\n",
    "        \n",
    "        # Sample indices to mask\n",
    "        random.shuffle(candidate_indices)\n",
    "        num_to_mask = max(1, int(len(candidate_indices) * mlm_probability))\n",
    "        mask_indices = candidate_indices[:num_to_mask]\n",
    "        \n",
    "        for idx in mask_indices:\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random.random() < 0.8:\n",
    "                output_tokens[idx] = self.tokenizer.mask_token_id\n",
    "            else:\n",
    "                # 10% of the time, replace with random token\n",
    "                if random.random() < 0.5:\n",
    "                    output_tokens[idx] = random.randint(0, self.tokenizer.vocab_size - 1)\n",
    "                # 10% of the time, keep original token\n",
    "                \n",
    "            output_labels[idx] = tokens[idx]\n",
    "            \n",
    "        return output_tokens, output_labels\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.documents) * 100  # Create multiple instances per document\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get document index\n",
    "        doc_idx = idx % len(self.documents)\n",
    "        \n",
    "        # Create training instance\n",
    "        sent_a, sent_b, is_next = self._create_training_instance(doc_idx)\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        tokens_a = self.tokenizer.tokenize(sent_a)\n",
    "        tokens_b = self.tokenizer.tokenize(sent_b) if sent_b else []\n",
    "        \n",
    "        # Truncate to fit max_length\n",
    "        self._truncate_seq_pair(tokens_a, tokens_b, self.max_length)\n",
    "        \n",
    "        # Build input sequence: [CLS] A [SEP] B [SEP]\n",
    "        tokens = [self.tokenizer.cls_token_id]\n",
    "        segment_ids = [0]\n",
    "        \n",
    "        for token in tokens_a:\n",
    "            tokens.append(self.tokenizer.convert_tokens_to_ids(token))\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        tokens.append(self.tokenizer.sep_token_id)\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(self.tokenizer.convert_tokens_to_ids(token))\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "            tokens.append(self.tokenizer.sep_token_id)\n",
    "            segment_ids.append(1)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(tokens)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        tokens.extend([self.tokenizer.pad_token_id] * padding_length)\n",
    "        segment_ids.extend([0] * padding_length)\n",
    "        attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        # Create MLM predictions\n",
    "        masked_tokens, mlm_labels = self._create_masked_lm_predictions(tokens, self.mlm_probability)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(masked_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(segment_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(mlm_labels, dtype=torch.long),\n",
    "            'next_sentence_label': torch.tensor(is_next, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BertTrainer:\n",
    "    \"\"\"Trainer class for BERT pre-training\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 val_dataloader: Optional[DataLoader] = None,\n",
    "                 learning_rate: float = 1e-4,\n",
    "                 warmup_steps: int = 10000,\n",
    "                 weight_decay: float = 0.01,\n",
    "                 max_grad_norm: float = 1.0,\n",
    "                 device: Optional[str] = None):\n",
    "        \n",
    "        # Auto-detect device if not specified\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.model = model.to(self.device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() \n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': weight_decay,\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() \n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        total_steps = len(train_dataloader) * 10  # Assuming 10 epochs\n",
    "        self.scheduler = self._get_linear_schedule_with_warmup(\n",
    "            self.optimizer, warmup_steps, total_steps\n",
    "        )\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def _get_linear_schedule_with_warmup(self, optimizer, num_warmup_steps, num_training_steps):\n",
    "        \"\"\"Create a schedule with a learning rate that decreases linearly after warmup\"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(\n",
    "                0.0, float(num_training_steps - current_step) / \n",
    "                float(max(1, num_training_steps - num_warmup_steps))\n",
    "            )\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    def train_epoch(self) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                token_type_ids=batch['token_type_ids'],\n",
    "                labels=batch['labels'],\n",
    "                next_sentence_label=batch['next_sentence_label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(self.train_dataloader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}, \"\n",
    "                      f\"LR: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_dataloader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self) -> float:\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        if self.val_dataloader is None:\n",
    "            return 0.0\n",
    "            \n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    token_type_ids=batch['token_type_ids'],\n",
    "                    labels=batch['labels'],\n",
    "                    next_sentence_label=batch['next_sentence_label']\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs['loss'].item()\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_dataloader)\n",
    "        self.val_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def train(self, num_epochs: int):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            print(f\"Average training loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Validate\n",
    "            if self.val_dataloader:\n",
    "                val_loss = self.validate()\n",
    "                print(f\"Average validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(epoch + 1)\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "        }\n",
    "        torch.save(checkpoint, f'bert_checkpoint_epoch_{epoch}.pt')\n",
    "        print(f\"Checkpoint saved: bert_checkpoint_epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_bert_training():\n",
    "    \"\"\"Demonstrate BERT training pipeline\"\"\"\n",
    "    \n",
    "    # Sample texts for training\n",
    "    texts = [\n",
    "        \"BERT is a transformer-based machine learning technique for natural language processing. \"\n",
    "        \"It was developed by Google and introduced in 2018. BERT stands for Bidirectional Encoder \"\n",
    "        \"Representations from Transformers.\",\n",
    "        \n",
    "        \"Natural language processing is a subfield of linguistics, computer science, and artificial \"\n",
    "        \"intelligence concerned with the interactions between computers and human language. \"\n",
    "        \"In particular, it focuses on programming computers to process and analyze large amounts of natural language data.\",\n",
    "        \n",
    "        \"The transformer architecture relies entirely on self-attention mechanisms to compute \"\n",
    "        \"representations of its input and output. Unlike recurrent neural networks, transformers \"\n",
    "        \"do not require sequential processing and can process all positions in parallel.\",\n",
    "    ]\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BertDataset(texts, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    config = get_bert_config('base')\n",
    "    model = BertForPreTraining(config)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = BertTrainer(\n",
    "        model=model,\n",
    "        train_dataloader=dataloader,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Train for 1 epoch (for demonstration)\n",
    "    trainer.train(num_epochs=1)\n",
    "    \n",
    "    print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_bert_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample texts\n",
    "# texts = [\n",
    "#     \"\"\"BERT is a transformer-based machine learning technique for natural language processing. \n",
    "#     It was developed by Google and introduced in 2018. BERT stands for Bidirectional Encoder \n",
    "#     Representations from Transformers. The model is designed to pre-train deep bidirectional \n",
    "#     representations from unlabeled text by jointly conditioning on both left and right context.\"\"\",\n",
    "    \n",
    "#     \"\"\"Natural language processing is a subfield of linguistics, computer science, and artificial \n",
    "#     intelligence. It focuses on the interactions between computers and human language. In particular, \n",
    "#     how to program computers to process and analyze large amounts of natural language data. \n",
    "#     The goal is a computer capable of understanding the contents of documents.\"\"\",\n",
    "    \n",
    "#     \"\"\"The transformer architecture relies entirely on self-attention mechanisms. It computes \n",
    "#     representations of its input and output without using sequence-aligned RNNs or convolution. \n",
    "#     Unlike recurrent neural networks, transformers do not require sequential processing. They can \n",
    "#     process all positions in parallel, making them much more efficient for training.\"\"\",\n",
    "    \n",
    "#     \"\"\"Machine learning is a method of data analysis that automates analytical model building. \n",
    "#     It is a branch of artificial intelligence based on the idea that systems can learn from data. \n",
    "#     They can identify patterns and make decisions with minimal human intervention. Machine learning \n",
    "#     algorithms build a model based on sample data, known as training data.\"\"\",\n",
    "    \n",
    "#     \"\"\"Deep learning is part of a broader family of machine learning methods. It is based on \n",
    "#     artificial neural networks with representation learning. Learning can be supervised, \n",
    "#     semi-supervised or unsupervised. Deep learning architectures have been applied to fields \n",
    "#     including computer vision, speech recognition, and natural language processing.\"\"\",\n",
    "    \n",
    "#     \"\"\"Attention mechanisms allow models to focus on specific parts of the input when producing output. \n",
    "#     In the context of neural networks, attention helps the model learn which parts of the input \n",
    "#     are most relevant for the current task. Self-attention, also known as intra-attention, relates \n",
    "#     different positions of a single sequence to compute a representation of the sequence.\"\"\",\n",
    "    \n",
    "#     \"\"\"Pre-training in deep learning refers to training a model on a large dataset before fine-tuning \n",
    "#     it on a smaller, task-specific dataset. This approach has been particularly successful in NLP. \n",
    "#     Models like BERT are pre-trained on massive text corpora using self-supervised objectives. \n",
    "#     They can then be fine-tuned on downstream tasks with relatively small amounts of labeled data.\"\"\",\n",
    "    \n",
    "#     \"\"\"The masked language model is a training technique where some tokens in the input are randomly \n",
    "#     masked. The model must predict the original tokens based on the context. This allows BERT to \n",
    "#     learn bidirectional representations, as it can use information from both left and right context. \n",
    "#     The masking strategy includes replacing tokens with [MASK], random tokens, or keeping them unchanged.\"\"\"\n",
    "# ]\n",
    "\n",
    "# # Initialize BERT tokenizer\n",
    "# # Using the pre-trained tokenizer from Hugging Face for consistency\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Tạo model\n",
    "# config = get_bert_config('base')\n",
    "# model = BertForPreTraining(config)\n",
    "\n",
    "# # 2. Chuẩn bị data\n",
    "# dataset = BertDataset(texts, tokenizer, max_length=128)\n",
    "# dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# # 3. Training\n",
    "# trainer = BertTrainer(model, dataloader, device='cpu')\n",
    "# trainer.train(num_epochs=10)\n",
    "\n",
    "# # 4. Fine-tuning cho classification\n",
    "# classifier = BertForSequenceClassification(config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_demo():\n",
    "    \"\"\"Quick demo that shows BERT working without full training\"\"\"\n",
    "    \n",
    "    # Minimal texts for demo\n",
    "    texts = [\n",
    "        \"BERT uses masked language modeling to learn bidirectional representations.\",\n",
    "        \"The transformer architecture enables parallel processing of sequences.\",\n",
    "        \"Self-attention mechanisms capture long-range dependencies in text.\"\n",
    "    ]\n",
    "    \n",
    "    # Setup\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Create small dataset - only 3 documents × 5 = 15 samples\n",
    "    dataset = BertDataset(texts, tokenizer, max_length=64)  # Shorter sequences\n",
    "    dataset.__len__ = lambda: 15  # Override to create fewer samples\n",
    "    \n",
    "    # Small batch size for CPU\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    config = get_bert_config('base')\n",
    "    # Smaller model for demo\n",
    "    config['num_hidden_layers'] = 2  # Only 2 layers instead of 12\n",
    "    config['num_attention_heads'] = 4  # Fewer attention heads\n",
    "    config['hidden_size'] = 256  # Smaller hidden size\n",
    "    config['intermediate_size'] = 1024  # Smaller FFN\n",
    "    \n",
    "    model = BertForPreTraining(config)\n",
    "    \n",
    "    print(f\"Demo model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"Dataset size: {len(dataset)} samples\")\n",
    "    print(f\"Batch size: 2\")\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Quick training demo\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    print(f\"\\nRunning 3 training steps on {device}...\")\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= 3:  # Only 3 steps for demo\n",
    "            break\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step_time = time.time() - start_time\n",
    "        print(f\"Step {i+1}: Loss = {loss.item():.4f}, Time = {step_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\nDemo completed!\")\n",
    "    print(\"\\nFor full training, consider:\")\n",
    "    print(\"- Using GPU (currently on CPU)\" if device == 'cpu' else \"- GPU detected ✓\")\n",
    "    print(\"- Using pre-trained weights instead of training from scratch\")\n",
    "    print(\"- Using HuggingFace's optimized implementation for production\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test_model_outputs(model, tokenizer):\n",
    "    \"\"\"Test the model with a simple example\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Create a simple masked input\n",
    "    text = \"The capital of France is [MASK].\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Add dummy labels for demo\n",
    "    inputs['labels'] = torch.full_like(inputs['input_ids'], -100)\n",
    "    inputs['next_sentence_label'] = torch.tensor([1])  # Dummy NSP label\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get predictions for [MASK] token\n",
    "    mask_token_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero()[0, 1]\n",
    "    mask_token_logits = outputs['prediction_logits'][0, mask_token_index]\n",
    "    \n",
    "    # Top 5 predictions\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5).indices\n",
    "    print(\"\\nTop 5 predictions for [MASK]:\")\n",
    "    for i, token_id in enumerate(top_5_tokens):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"{i+1}. {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo model created with 9,686,844 parameters\n",
      "Dataset size: 300 samples\n",
      "Batch size: 2\n",
      "Number of batches: 150\n",
      "\n",
      "Running 3 training steps on cpu...\n",
      "Step 1: Loss = 10.8948, Time = 0.18s\n",
      "Step 2: Loss = 11.1435, Time = 0.12s\n",
      "Step 3: Loss = 10.9053, Time = 0.18s\n",
      "\n",
      "Demo completed!\n",
      "\n",
      "For full training, consider:\n",
      "- Using GPU (currently on CPU)\n",
      "- Using pre-trained weights instead of training from scratch\n",
      "- Using HuggingFace's optimized implementation for production\n",
      "\n",
      "Top 5 predictions for [MASK]:\n",
      "1. rich\n",
      "2. treating\n",
      "3. crews\n",
      "4. ##rgh\n",
      "5. dissatisfied\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run quick demo\n",
    "    model = quick_demo()\n",
    "    \n",
    "    # Test the model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    test_model_outputs(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
