{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.32.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BertConfig:\n",
    "    \"\"\"BERT configuration with detailed documentation\"\"\"\n",
    "    vocab_size: int = 30522          # Size of vocabulary\n",
    "    hidden_size: int = 768           # Hidden size (d_model)\n",
    "    num_hidden_layers: int = 12      # Number of transformer blocks\n",
    "    num_attention_heads: int = 12    # Number of attention heads\n",
    "    intermediate_size: int = 3072    # FFN intermediate size\n",
    "    hidden_dropout_prob: float = 0.1 # Dropout for hidden layers\n",
    "    attention_probs_dropout_prob: float = 0.1  # Dropout for attention\n",
    "    max_position_embeddings: int = 512         # Maximum sequence length\n",
    "    type_vocab_size: int = 2         # Token type vocab size\n",
    "    initializer_range: float = 0.02  # Weight initialization std\n",
    "    layer_norm_eps: float = 1e-12    # Layer norm epsilon\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert self.hidden_size % self.num_attention_heads == 0, \\\n",
    "            f\"hidden_size ({self.hidden_size}) must be divisible by num_attention_heads ({self.num_attention_heads})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention with detailed mathematical explanation\n",
    "    \n",
    "    Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V\n",
    "    \n",
    "    This addresses the problem of dot products growing large in magnitude\n",
    "    for large d_k, pushing softmax into regions with extremely small gradients.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature: float = 1.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        query: torch.Tensor,     # [batch, n_heads, seq_len, d_k]\n",
    "        key: torch.Tensor,       # [batch, n_heads, seq_len, d_k]\n",
    "        value: torch.Tensor,     # [batch, n_heads, seq_len, d_v]\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        return_attention: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query tensor\n",
    "            key: Key tensor  \n",
    "            value: Value tensor\n",
    "            mask: Attention mask (1 for positions to attend, 0 for masked)\n",
    "            return_attention: Whether to return attention weights\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, seq_len, d_k = query.size()\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # Einstein notation for clarity: bhqd,bhkd->bhqk\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (math.sqrt(d_k) * self.temperature)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Expand mask for all heads\n",
    "            if mask.dim() == 2:  # [batch, seq_len]\n",
    "                mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            elif mask.dim() == 3:  # [batch, seq_len, seq_len]\n",
    "                mask = mask.unsqueeze(1)  # [batch, 1, seq_len, seq_len]\n",
    "            \n",
    "            # Fill masked positions with -inf so they become 0 after softmax\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention probabilities\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # Einstein notation: bhqk,bhkd->bhqd\n",
    "        output = torch.matmul(attn_weights, value)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention allows the model to jointly attend to information\n",
    "    from different representation subspaces at different positions.\n",
    "    \n",
    "    MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n",
    "    where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Xavier uniform initialization\n",
    "        for module in [self.w_q, self.w_k, self.w_v, self.w_o]:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0.)\n",
    "                \n",
    "    def forward(\n",
    "        self, \n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor, \n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        return_attention: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len] or [batch_size, seq_len, seq_len]\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights: [batch_size, n_heads, seq_len, seq_len] (if return_attention=True)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # 1. Linear projections in batch from d_model => h x d_k\n",
    "        # [batch, seq_len, d_model] -> [batch, seq_len, n_heads, d_k] -> [batch, n_heads, seq_len, d_k]\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_v).transpose(1, 2)\n",
    "        \n",
    "        # 2. Apply attention on all the projected vectors in batch\n",
    "        if return_attention:\n",
    "            attn_output, attn_weights = self.attention(Q, K, V, mask=mask, return_attention=True)\n",
    "        else:\n",
    "            attn_output = self.attention(Q, K, V, mask=mask, return_attention=False)\n",
    "            attn_weights = None\n",
    "        \n",
    "        # 3. \"Concat\" using a view and apply a final linear\n",
    "        # [batch, n_heads, seq_len, d_v] -> [batch, seq_len, n_heads, d_v] -> [batch, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 4. Final linear projection\n",
    "        output = self.w_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \n",
    "    BERT uses GELU activation instead of ReLU for smoother gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1, activation: str = 'gelu'):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = self._get_activation_fn(activation)\n",
    "        \n",
    "    def _get_activation_fn(self, activation: str):\n",
    "        \"\"\"Get activation function by name\"\"\"\n",
    "        if activation == \"relu\":\n",
    "            return F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            return F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Activation '{activation}' not supported\")\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "# ========================= Transformer Block =========================\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with pre-norm architecture (used in BERT)\n",
    "    \n",
    "    Each block contains:\n",
    "    1. Multi-head self-attention\n",
    "    2. Position-wise feed-forward network\n",
    "    Both with residual connections and layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sub-layers\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        return_attention: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len] or None\n",
    "            return_attention: Whether to return attention weights\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        if return_attention:\n",
    "            attn_output, attn_weights = self.attention(x, x, x, mask, return_attention=True)\n",
    "        else:\n",
    "            attn_output = self.attention(x, x, x, mask, return_attention=False)\n",
    "            attn_weights = None\n",
    "            \n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        if return_attention:\n",
    "            return x, attn_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT embeddings consist of:\n",
    "    1. Token embeddings (from vocabulary)\n",
    "    2. Position embeddings (learned, not sinusoidal)\n",
    "    3. Token type embeddings (for sentence A/B distinction)\n",
    "    \n",
    "    The final embedding is the sum of all three, followed by LayerNorm and dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        # Create position IDs buffer\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", \n",
    "            torch.arange(config.max_position_embeddings).expand((1, -1))\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            token_type_ids: [batch_size, seq_len] or None\n",
    "            position_ids: [batch_size, seq_len] or None\n",
    "        Returns:\n",
    "            embeddings: [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        # Get position IDs\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "            \n",
    "        # Get token type IDs (default to 0 if not provided)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        # Get embeddings\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # Sum all embeddings\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        \n",
    "        # Apply LayerNorm and dropout\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.hidden_size,\n",
    "                n_heads=config.num_attention_heads,\n",
    "                d_ff=config.intermediate_size,\n",
    "                dropout=config.hidden_dropout_prob\n",
    "            ) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "            attention_mask: [batch_size, seq_len] or None\n",
    "            output_attentions: Whether to output attention weights\n",
    "            output_hidden_states: Whether to output all hidden states\n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - last_hidden_state: [batch_size, seq_len, hidden_size]\n",
    "                - hidden_states: List of hidden states (if output_hidden_states=True)\n",
    "                - attentions: List of attention weights (if output_attentions=True)\n",
    "        \"\"\"\n",
    "        all_hidden_states = [] if output_hidden_states else None\n",
    "        all_attentions = [] if output_attentions else None\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states.append(hidden_states)\n",
    "                \n",
    "            if output_attentions:\n",
    "                hidden_states, attn_weights = layer(hidden_states, attention_mask, return_attention=True)\n",
    "                all_attentions.append(attn_weights)\n",
    "            else:\n",
    "                hidden_states = layer(hidden_states, attention_mask, return_attention=False)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states.append(hidden_states)\n",
    "            \n",
    "        outputs = {\n",
    "            'last_hidden_state': hidden_states\n",
    "        }\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            outputs['hidden_states'] = all_hidden_states\n",
    "        if output_attentions:\n",
    "            outputs['attentions'] = all_attentions\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Pool the [CLS] token representation for classification tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        Returns:\n",
    "            pooled_output: [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # Take the hidden state of the first token ([CLS])\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    \"\"\"Transform for MLM predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = F.gelu\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    \"\"\"Language Model prediction head for MLM\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "        \n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        \n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    \"\"\"Pre-training heads for MLM and NSP\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        sequence_output: torch.Tensor,\n",
    "        pooled_output: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_output: [batch_size, seq_len, hidden_size]\n",
    "            pooled_output: [batch_size, hidden_size]\n",
    "        Returns:\n",
    "            prediction_scores: [batch_size, seq_len, vocab_size]\n",
    "            seq_relationship_score: [batch_size, 2]\n",
    "        \"\"\"\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model with all components integrated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def get_extended_attention_mask(self, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert attention mask from [batch_size, seq_len] to [batch_size, 1, 1, seq_len]\n",
    "        for compatibility with multi-head attention\n",
    "        \"\"\"\n",
    "        if attention_mask.dim() == 2:\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "        elif attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong shape for attention_mask (shape {attention_mask.shape})\")\n",
    "            \n",
    "        # Convert to float and apply mask\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        return extended_attention_mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len] or None\n",
    "            token_type_ids: [batch_size, seq_len] or None\n",
    "            position_ids: [batch_size, seq_len] or None\n",
    "            output_attentions: Whether to output attention weights\n",
    "            output_hidden_states: Whether to output all hidden states\n",
    "        Returns:\n",
    "            Dictionary containing model outputs\n",
    "        \"\"\"\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "        \n",
    "        # Pass through encoder\n",
    "        encoder_outputs = self.encoder(\n",
    "            hidden_states=embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs['last_hidden_state']\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        \n",
    "        outputs = {\n",
    "            'last_hidden_state': sequence_output,\n",
    "            'pooler_output': pooled_output\n",
    "        }\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            outputs['hidden_states'] = encoder_outputs['hidden_states']\n",
    "        if output_attentions:\n",
    "            outputs['attentions'] = encoder_outputs['attentions']\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(nn.Module):\n",
    "    \"\"\"BERT model with pre-training heads\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        \n",
    "        # Tie weights between input embeddings and output embeddings\n",
    "        self.tie_weights()\n",
    "        \n",
    "    def tie_weights(self):\n",
    "        \"\"\"Tie the weights between input embeddings and output embeddings\"\"\"\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.weight\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        next_sentence_label: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for pre-training\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len] or None\n",
    "            token_type_ids: [batch_size, seq_len] or None\n",
    "            position_ids: [batch_size, seq_len] or None\n",
    "            labels: [batch_size, seq_len] - labels for MLM (-100 for non-masked tokens)\n",
    "            next_sentence_label: [batch_size] - labels for NSP (0 or 1)\n",
    "            output_attentions: Whether to output attention weights\n",
    "            output_hidden_states: Whether to output all hidden states\n",
    "        Returns:\n",
    "            Dictionary containing losses and predictions\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        \n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # MLM loss\n",
    "            masked_lm_loss = loss_fct(\n",
    "                prediction_scores.view(-1, prediction_scores.size(-1)),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # NSP loss\n",
    "            next_sentence_loss = loss_fct(\n",
    "                seq_relationship_score.view(-1, 2),\n",
    "                next_sentence_label.view(-1)\n",
    "            )\n",
    "            \n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'mlm_loss': masked_lm_loss if labels is not None else None,\n",
    "            'nsp_loss': next_sentence_loss if next_sentence_label is not None else None,\n",
    "            'prediction_logits': prediction_scores,\n",
    "            'seq_relationship_logits': seq_relationship_score,\n",
    "            'hidden_states': outputs.get('hidden_states'),\n",
    "            'attentions': outputs.get('attentions')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Classification ModeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT for sequence classification tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BertConfig, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for classification\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len] or None\n",
    "            token_type_ids: [batch_size, seq_len] or None\n",
    "            labels: [batch_size] - classification labels\n",
    "        Returns:\n",
    "            Dictionary containing loss and logits\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs['pooler_output']\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'hidden_states': outputs['last_hidden_state'],\n",
    "            'pooler_output': outputs['pooler_output']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for BERT pre-training with MLM and NSP tasks\n",
    "    \n",
    "    Implements the 80-10-10 masking strategy:\n",
    "    - 80% of the time: Replace with [MASK] token\n",
    "    - 10% of the time: Replace with random token\n",
    "    - 10% of the time: Keep original token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        tokenizer: BertTokenizer,\n",
    "        max_length: int = 512,\n",
    "        mlm_probability: float = 0.15,\n",
    "        short_seq_prob: float = 0.1\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.short_seq_prob = short_seq_prob\n",
    "        \n",
    "        # Pre-process texts into sentences\n",
    "        self.documents = self._preprocess_texts()\n",
    "        \n",
    "    def _preprocess_texts(self) -> List[List[str]]:\n",
    "        \"\"\"Split texts into sentences\"\"\"\n",
    "        documents = []\n",
    "        for text in self.texts:\n",
    "            # Simple sentence splitting\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            if sentences:\n",
    "                documents.append(sentences)\n",
    "        return documents\n",
    "    \n",
    "    def _get_random_sentence(self, exclude_doc_idx: int) -> str:\n",
    "        \"\"\"Get a random sentence from a different document\"\"\"\n",
    "        if len(self.documents) == 1:\n",
    "            return \"\"\n",
    "        \n",
    "        doc_idx = random.choice([i for i in range(len(self.documents)) if i != exclude_doc_idx])\n",
    "        if self.documents[doc_idx]:\n",
    "            return random.choice(self.documents[doc_idx])\n",
    "        return \"\"\n",
    "    \n",
    "    def _create_training_instance(self, doc_idx: int) -> Tuple[str, str, int]:\n",
    "        \"\"\"Create a training instance with sentence A, sentence B, and NSP label\"\"\"\n",
    "        document = self.documents[doc_idx]\n",
    "        \n",
    "        # Get sentence A\n",
    "        sent_idx_a = random.randint(0, len(document) - 1)\n",
    "        sent_a = document[sent_idx_a]\n",
    "        \n",
    "        # Create sentence B and NSP label\n",
    "        if random.random() < 0.5 and sent_idx_a < len(document) - 1:\n",
    "            # Next sentence (positive example)\n",
    "            sent_b = document[sent_idx_a + 1]\n",
    "            is_next = 1\n",
    "        else:\n",
    "            # Random sentence (negative example)\n",
    "            sent_b = self._get_random_sentence(doc_idx)\n",
    "            is_next = 0\n",
    "            \n",
    "        return sent_a, sent_b, is_next\n",
    "    \n",
    "    def _truncate_seq_pair(self, tokens_a: List[int], tokens_b: List[int], max_length: int):\n",
    "        \"\"\"Truncate sequence pair to fit max_length\"\"\"\n",
    "        while len(tokens_a) + len(tokens_b) > max_length - 3:  # Account for [CLS], [SEP], [SEP]\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "    \n",
    "    def _create_masked_lm_predictions(\n",
    "        self,\n",
    "        tokens: List[int],\n",
    "        mlm_probability: float\n",
    "    ) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Create masked language model predictions\n",
    "        \n",
    "        Returns:\n",
    "            output_tokens: Tokens with masking applied\n",
    "            output_labels: Original tokens at masked positions (-100 for non-masked)\n",
    "        \"\"\"\n",
    "        output_tokens = tokens.copy()\n",
    "        output_labels = [-100] * len(tokens)  # -100 is ignored by CrossEntropyLoss\n",
    "        \n",
    "        # Get candidates for masking (exclude [CLS], [SEP], [PAD])\n",
    "        candidate_indices = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token not in [self.tokenizer.cls_token_id,\n",
    "                           self.tokenizer.sep_token_id,\n",
    "                           self.tokenizer.pad_token_id]:\n",
    "                candidate_indices.append(i)\n",
    "        \n",
    "        # Sample indices to mask\n",
    "        random.shuffle(candidate_indices)\n",
    "        num_to_mask = max(1, int(len(candidate_indices) * mlm_probability))\n",
    "        mask_indices = candidate_indices[:num_to_mask]\n",
    "        \n",
    "        for idx in mask_indices:\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random.random() < 0.8:\n",
    "                output_tokens[idx] = self.tokenizer.mask_token_id\n",
    "            else:\n",
    "                # 10% of the time, replace with random token\n",
    "                if random.random() < 0.5:\n",
    "                    output_tokens[idx] = random.randint(0, self.tokenizer.vocab_size - 1)\n",
    "                # 10% of the time, keep original token\n",
    "                \n",
    "            output_labels[idx] = tokens[idx]\n",
    "            \n",
    "        return output_tokens, output_labels\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.documents) * 100  # Create multiple instances per document\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get document index\n",
    "        doc_idx = idx % len(self.documents)\n",
    "        \n",
    "        # Create training instance\n",
    "        sent_a, sent_b, is_next = self._create_training_instance(doc_idx)\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        tokens_a = self.tokenizer.tokenize(sent_a)\n",
    "        tokens_b = self.tokenizer.tokenize(sent_b) if sent_b else []\n",
    "        \n",
    "        # Convert to token IDs\n",
    "        tokens_a = self.tokenizer.convert_tokens_to_ids(tokens_a)\n",
    "        tokens_b = self.tokenizer.convert_tokens_to_ids(tokens_b)\n",
    "        \n",
    "        # Truncate to fit max_length\n",
    "        self._truncate_seq_pair(tokens_a, tokens_b, self.max_length)\n",
    "        \n",
    "        # Build input sequence: [CLS] A [SEP] B [SEP]\n",
    "        tokens = [self.tokenizer.cls_token_id]\n",
    "        segment_ids = [0]\n",
    "        \n",
    "        tokens.extend(tokens_a)\n",
    "        segment_ids.extend([0] * len(tokens_a))\n",
    "        \n",
    "        tokens.append(self.tokenizer.sep_token_id)\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            segment_ids.extend([1] * len(tokens_b))\n",
    "            \n",
    "            tokens.append(self.tokenizer.sep_token_id)\n",
    "            segment_ids.append(1)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(tokens)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padding_length = self.max_length - len(tokens)\n",
    "        tokens.extend([self.tokenizer.pad_token_id] * padding_length)\n",
    "        segment_ids.extend([0] * padding_length)\n",
    "        attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        # Create MLM predictions\n",
    "        masked_tokens, mlm_labels = self._create_masked_lm_predictions(tokens, self.mlm_probability)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(masked_tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(segment_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(mlm_labels, dtype=torch.long),\n",
    "            'next_sentence_label': torch.tensor(is_next, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer:\n",
    "    \"\"\"\n",
    "    Enhanced trainer class for BERT pre-training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: Optional[DataLoader] = None,\n",
    "        learning_rate: float = 1e-4,\n",
    "        warmup_steps: int = 10000,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        device: Optional[str] = None,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        mixed_precision: bool = False\n",
    "    ):\n",
    "        # Auto-detect device if not specified\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.model = model.to(self.device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.mixed_precision = mixed_precision\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': weight_decay,\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        total_steps = len(train_dataloader) * 10  # Assuming 10 epochs\n",
    "        self.scheduler = self._get_linear_schedule_with_warmup(\n",
    "            self.optimizer, warmup_steps, total_steps\n",
    "        )\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if self.mixed_precision and self.device == 'cuda':\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "    def _get_linear_schedule_with_warmup(self, optimizer, num_warmup_steps, num_training_steps):\n",
    "        \"\"\"Create a schedule with a learning rate that decreases linearly after warmup\"\"\"\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(\n",
    "                0.0, float(num_training_steps - current_step) /\n",
    "                float(max(1, num_training_steps - num_warmup_steps))\n",
    "            )\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_mlm_loss = 0\n",
    "        total_nsp_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if self.mixed_precision and self.device == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        labels=batch['labels'],\n",
    "                        next_sentence_label=batch['next_sentence_label']\n",
    "                    )\n",
    "                    loss = outputs['loss'] / self.gradient_accumulation_steps\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    token_type_ids=batch['token_type_ids'],\n",
    "                    labels=batch['labels'],\n",
    "                    next_sentence_label=batch['next_sentence_label']\n",
    "                )\n",
    "                loss = outputs['loss'] / self.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            if self.scaler is not None:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                if self.scaler is not None:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                # Optimizer step\n",
    "                if self.scaler is not None:\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            # Track losses\n",
    "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
    "            if outputs.get('mlm_loss') is not None:\n",
    "                total_mlm_loss += outputs['mlm_loss'].item()\n",
    "            if outputs.get('nsp_loss') is not None:\n",
    "                total_nsp_loss += outputs['nsp_loss'].item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item() * self.gradient_accumulation_steps:.4f}\",\n",
    "                'lr': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # Log learning rate\n",
    "            if batch_idx % 100 == 0:\n",
    "                self.learning_rates.append(current_lr)\n",
    "        \n",
    "        # Average losses\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mlm_loss = total_mlm_loss / num_batches if total_mlm_loss > 0 else 0\n",
    "        avg_nsp_loss = total_nsp_loss / num_batches if total_nsp_loss > 0 else 0\n",
    "        \n",
    "        self.train_losses.append(avg_loss)\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'mlm_loss': avg_mlm_loss,\n",
    "            'nsp_loss': avg_nsp_loss\n",
    "        }\n",
    "    \n",
    "    def validate(self) -> Dict[str, float]:\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        if self.val_dataloader is None:\n",
    "            return {}\n",
    "            \n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_mlm_loss = 0\n",
    "        total_nsp_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dataloader, desc=\"Validation\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    token_type_ids=batch['token_type_ids'],\n",
    "                    labels=batch['labels'],\n",
    "                    next_sentence_label=batch['next_sentence_label']\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs['loss'].item()\n",
    "                if outputs.get('mlm_loss') is not None:\n",
    "                    total_mlm_loss += outputs['mlm_loss'].item()\n",
    "                if outputs.get('nsp_loss') is not None:\n",
    "                    total_nsp_loss += outputs['nsp_loss'].item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mlm_loss = total_mlm_loss / num_batches if total_mlm_loss > 0 else 0\n",
    "        avg_nsp_loss = total_nsp_loss / num_batches if total_nsp_loss > 0 else 0\n",
    "        \n",
    "        self.val_losses.append(avg_loss)\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'mlm_loss': avg_mlm_loss,\n",
    "            'nsp_loss': avg_nsp_loss\n",
    "        }\n",
    "    \n",
    "    def train(self, num_epochs: int, save_dir: Optional[str] = None):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Mixed Precision: {self.mixed_precision}\")\n",
    "        print(f\"Gradient Accumulation Steps: {self.gradient_accumulation_steps}\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch()\n",
    "            print(f\"\\nTraining metrics:\")\n",
    "            print(f\"  Average loss: {train_metrics['loss']:.4f}\")\n",
    "            print(f\"  MLM loss: {train_metrics['mlm_loss']:.4f}\")\n",
    "            print(f\"  NSP loss: {train_metrics['nsp_loss']:.4f}\")\n",
    "            \n",
    "            # Validate\n",
    "            if self.val_dataloader:\n",
    "                val_metrics = self.validate()\n",
    "                print(f\"\\nValidation metrics:\")\n",
    "                print(f\"  Average loss: {val_metrics['loss']:.4f}\")\n",
    "                print(f\"  MLM loss: {val_metrics['mlm_loss']:.4f}\")\n",
    "                print(f\"  NSP loss: {val_metrics['nsp_loss']:.4f}\")\n",
    "                \n",
    "                # Save best model\n",
    "                if save_dir and val_metrics['loss'] < best_val_loss:\n",
    "                    best_val_loss = val_metrics['loss']\n",
    "                    self.save_checkpoint(save_dir, epoch + 1, is_best=True)\n",
    "            \n",
    "            # Save regular checkpoint\n",
    "            if save_dir and (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(save_dir, epoch + 1)\n",
    "    \n",
    "    def save_checkpoint(self, save_dir: str, epoch: int, is_best: bool = False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            path = os.path.join(save_dir, 'best_model.pt')\n",
    "        else:\n",
    "            path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "            \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved: {path}\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Plot losses\n",
    "        axes[0].plot(self.train_losses, label='Train Loss')\n",
    "        if self.val_losses:\n",
    "            axes[0].plot(self.val_losses, label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Plot learning rate\n",
    "        axes[1].plot(self.learning_rates)\n",
    "        axes[1].set_xlabel('Steps (x100)')\n",
    "        axes[1].set_ylabel('Learning Rate')\n",
    "        axes[1].set_title('Learning Rate Schedule')\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(\n",
    "    model: BertModel,\n",
    "    tokenizer: BertTokenizer,\n",
    "    text: str,\n",
    "    layer_idx: int = -1,\n",
    "    head_idx: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text\n",
    "    \n",
    "    Args:\n",
    "        model: BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        text: Input text\n",
    "        layer_idx: Which layer to visualize (-1 for last layer)\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Get model outputs with attention\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            output_attentions=True\n",
    "        )\n",
    "    \n",
    "    # Get attention weights\n",
    "    attentions = outputs['attentions']  # List of tensors, one per layer\n",
    "    attention = attentions[layer_idx]    # [batch, n_heads, seq_len, seq_len]\n",
    "    attention = attention[0, head_idx]   # [seq_len, seq_len]\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention.numpy(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='Blues',\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Keys')\n",
    "    plt.ylabel('Queries')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention_mechanism():\n",
    "    \"\"\"Test the attention mechanism with a simple example\"\"\"\n",
    "    print(\"Testing Attention Mechanism...\")\n",
    "    \n",
    "    # Create simple input\n",
    "    batch_size, seq_len, d_model = 2, 5, 64\n",
    "    n_heads = 8\n",
    "    \n",
    "    # Random input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create attention module\n",
    "    mha = MultiHeadAttention(d_model, n_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = mha(x, x, x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    assert output.shape == x.shape, \"Output shape mismatch!\"\n",
    "    print(\" Attention mechanism test passed!\")\n",
    "\n",
    "def test_bert_model():\n",
    "    \"\"\"Test the complete BERT model\"\"\"\n",
    "    print(\"\\nTesting BERT Model...\")\n",
    "    \n",
    "    # Create config\n",
    "    config = BertConfig(\n",
    "        vocab_size=1000,\n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=2,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=512\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = BertForPreTraining(config)\n",
    "    \n",
    "    # Create dummy input\n",
    "    batch_size = 4\n",
    "    seq_len = 20\n",
    "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    attention_mask = torch.ones(batch_size, seq_len)\n",
    "    token_type_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "    labels = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    labels[torch.rand(batch_size, seq_len) > 0.15] = -100  # Mask most positions\n",
    "    next_sentence_label = torch.randint(0, 2, (batch_size,))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=labels,\n",
    "        next_sentence_label=next_sentence_label\n",
    "    )\n",
    "    \n",
    "    print(f\"Loss: {outputs['loss'].item():.4f}\")\n",
    "    print(f\"MLM predictions shape: {outputs['prediction_logits'].shape}\")\n",
    "    print(f\"NSP predictions shape: {outputs['seq_relationship_logits'].shape}\")\n",
    "    print(\" BERT model test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Attention Mechanism...\n",
      "Input shape: torch.Size([2, 5, 64])\n",
      "Output shape: torch.Size([2, 5, 64])\n",
      " Attention mechanism test passed!\n",
      "\n",
      "Testing BERT Model...\n",
      "Loss: 7.6057\n",
      "MLM predictions shape: torch.Size([4, 20, 1000])\n",
      "NSP predictions shape: torch.Size([4, 2])\n",
      " BERT model test passed!\n",
      "\n",
      "==================================================\n",
      "All tests passed! The BERT implementation is working correctly.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    " # Run tests\n",
    "test_attention_mechanism()\n",
    "test_bert_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All tests passed! The BERT implementation is working correctly.\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
